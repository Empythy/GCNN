{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "import collections\n",
    "from tensorflow.contrib import rnn\n",
    "import h5py\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from utils import normalize_adj, StandardScaler, masked_mae_tf, masked_mae_tf_by_horizon\n",
    "from gcn import gcn, gcnn_ddgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"data/NYCBikeHourly272.pickle\"\n",
    "fileObject = open(file_Name,'rb') \n",
    "hourly_bike = pickle.load(fileObject) \n",
    "hourly_bike = pd.DataFrame(hourly_bike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_num = 272 # node number \n",
    "feature_in = 24 # number of features at each node, e.g., bike sharing demand from past 24 hours\n",
    "horizon = 1 # the length to predict, e.g., predict the future one hour bike sharing demand\n",
    "lstm_steps = 1 # number of cells in the LSTM part\n",
    "\n",
    "X_whole = []\n",
    "Y_whole = []\n",
    "\n",
    "x_offsets = np.sort(\n",
    "    np.concatenate((np.arange(-feature_in+1, 1, 1),))\n",
    ")\n",
    "# Predict the next one hour\n",
    "y_offsets = np.sort(np.arange(1, 1+ horizon, 1))\n",
    "\n",
    "min_t = abs(min(x_offsets))\n",
    "max_t = abs(hourly_bike.shape[0] - abs(max(y_offsets)))  # Exclusive\n",
    "for t in range(min_t, max_t):\n",
    "    x_t = hourly_bike.iloc[t + x_offsets, 0:sn].values.flatten('F')\n",
    "    y_t = hourly_bike.iloc[t + y_offsets, 0:sn].values.flatten('F')\n",
    "    X_whole.append(x_t)\n",
    "    Y_whole.append(y_t)\n",
    "\n",
    "X_whole = np.stack(X_whole, axis=0)\n",
    "Y_whole = np.stack(Y_whole, axis=0)\n",
    "\n",
    "X_whole = np.reshape(X_whole, [X_whole.shape[0], node_num, feature_in])\n",
    "\n",
    "i = lstm_steps\n",
    "X_whole_lstm = []\n",
    "Y_whole_lstm = []\n",
    "\n",
    "while i <= X_whole.shape[0]:\n",
    "    X_whole_lstm.append(X_whole[i-lstm_steps:i,:]) # features from i-lstm_steps + 1 to i\n",
    "    Y_whole_lstm.append(Y_whole[i-1])\n",
    "    i = i + 1\n",
    "\n",
    "X_whole_lstm = np.stack(X_whole_lstm, axis = 0) # (34239, 10, 207, 12)\n",
    "Y_whole_lstm = np.stack(Y_whole_lstm, axis = 0) # (34239, 2484)\n",
    "\n",
    "num_train = 20000 # Note here actually we use the first 20000 to train the model. The paper mentioned \"22304\" need to be corrected.\n",
    "num_val = 2000\n",
    "num_test = 2000\n",
    "\n",
    "X_training = X_whole_lstm[:num_train, :]\n",
    "Y_training = Y_whole_lstm[:num_train, :]\n",
    "\n",
    "# shuffle the training dataset\n",
    "perm = np.arange(X_training.shape[0])\n",
    "np.random.shuffle(perm)\n",
    "X_training = X_training[perm]\n",
    "Y_training = Y_training[perm]\n",
    "\n",
    "X_val = X_whole_lstm[num_train:num_train+num_val, :]\n",
    "Y_val = Y_whole_lstm[num_train:num_train+num_val, :]\n",
    "\n",
    "X_test = X_whole_lstm[num_train+num_val:num_train+num_val+num_test, :]\n",
    "Y_test = Y_whole_lstm[num_train+num_val:num_train+num_val+num_test, :]\n",
    "\n",
    "scaler = StandardScaler(mean=X_training.mean(), std=X_training.std())\n",
    "\n",
    "X_training = scaler.transform(X_training)\n",
    "Y_training = scaler.transform(Y_training)\n",
    "\n",
    "X_val = scaler.transform(X_val)\n",
    "Y_val = scaler.transform(Y_val)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "Y_test = scaler.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def gcn(x, weights, biases, batch_size, n_input, frequency,flag, n_output_vec):\n",
    "    # Hidden layer with RELU activation\n",
    "    \n",
    "   # output_list = tf.Variable(tf.zeros([n_output_vec,1]),dtype=tf.float32) #'Tensor' object does not support item assignment, cant build Ypre\n",
    "\n",
    "    #Xtem = tf.reshape(x[i,:], [n_input, frequency])\n",
    "    #Xtem = tf.transpose(Xtem)\n",
    "    #Atem = tf.convert_to_tensor(A_whole_final, dtype=np.float32)\n",
    "    #Atem1 = tf.diag(tf.ones([n_input])) # Atem not palying any roles\n",
    "    #Ytem = tf.reshape(Y[i,:], [n_input, 1])\n",
    "    #Atem = tf.diag(tf.ones([n_input]))\n",
    "    #x = tf.reshape(x, [-1, sn, 1]) # 100, 207, 1\n",
    "    \n",
    "    # x (?, 207, 12)\n",
    "    x = tf.transpose(x, [1, 0, 2]) # 207, ?, 12\n",
    "    x = tf.reshape(x, [sn, -1]) # 207, batch*feature_num\n",
    "    Atem1 = 0.5*(weights['A1'] + tf.transpose(weights['A1']))#+ Atem \n",
    "    Atem1 = normalize_adj(Atem1)\n",
    "    #th = tf.constant(0.01, dtype=tf.float32)\n",
    "    #where = tf.subtract(Atem1, th)\n",
    "    #Atem1 = tf.nn.relu(where)\n",
    "\n",
    "    Z1 = tf.matmul(Atem1, x) # 207, batch*feature_num  #+ tf.matmul( tf.matmul(weights['A1'], weights['A1']), Xtem)\n",
    "    Z1 = tf.reshape(Z1, [-1, frequency]) # 207* 100, frequency\n",
    "    #layer_1 = tf.matmul(Z1, weights['h1']) \n",
    "    layer_1 = tf.add(tf.matmul(Z1, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1) # 207*100, hidden1\n",
    "\n",
    "    \n",
    "    Atem2 = 0.5*(weights['A2'] + tf.transpose(weights['A2']))#+ Atem \n",
    "    Atem2 = normalize_adj(Atem2)\n",
    "    \n",
    "    layer_1 = tf.reshape(layer_1, [sn, -1])  # 207, batchsize*hidden1\n",
    "    Z2 = tf.matmul(Atem2, layer_1)\n",
    "    Z2 = tf.reshape(Z2, [-1, n_hidden_vec1]) # 207*batchsize, n_hidden_vec1\n",
    "    layer_2 = tf.add(tf.matmul(Z2, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2) # 207*batchsize, hidden2\n",
    "\n",
    "    Atem3 = 0.5*(weights['A3'] + tf.transpose(weights['A3']))#+ Atem \n",
    "    Atem3 = normalize_adj(Atem3) \n",
    "    \n",
    "    layer_2 = tf.reshape(layer_2, [sn, -1])  # 207, batchsize*hidden2\n",
    "    Z3 = tf.matmul(Atem3, layer_2)\n",
    "    Z3 = tf.reshape(Z3, [-1, n_hidden_vec2]) # 207*batchsize, hidden2\n",
    "    layer_3 = tf.add(tf.matmul(Z3, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3) # 207*batchsize, hidden3\n",
    "    \n",
    "    #Atem4 = 0.5*(weights['A4'] + tf.transpose(weights['A4']))#+ Atem \n",
    "    #Atem4 = normalize_adj(Atem4)\n",
    "    #Z4 = tf.matmul(Atem4, layer_3)\n",
    "    #layer_4 = tf.add(tf.matmul(Z4, weights['h4']), biases['b4'])\n",
    "    #layer_4 = tf.nn.relu(layer_4)\n",
    "\n",
    "    # flattern\n",
    "    #layer_3 = tf.reshape(layer_3, [1, 272*n_hidden_vec3])\n",
    "\n",
    "    #F1 = tf.add(tf.matmul(layer_3, weights['f1']), biases['bf1'])\n",
    "    #F1 = tf.nn.relu(F1)\n",
    "\n",
    "    #F2 = tf.add(tf.matmul(F1, weights['f2']), biases['bf2'])\n",
    "    #F2 = tf.nn.relu(F2)\n",
    "\n",
    "    #F3 = tf.add(tf.matmul(F1, weights['f3']), biases['bf3'])\n",
    "    #out_layer = tf.reshape(F3, [272, 1])\n",
    "\n",
    "    # Output layer with linear activation\n",
    "    layer_3 = tf.reshape(layer_3, [sn, -1, n_hidden_vec3])\n",
    "    layer_3 = tf.transpose(layer_3, [1, 0, 2]) # batchsize, sn, hidden3\n",
    "    layer_3 = tf.reshape(layer_3, [-1, sn*n_hidden_vec3]) # batchsize, sn*hidden3\n",
    "    Z4 = layer_3\n",
    "    #out_layer = tf.add(tf.matmul(Z4, weights['outg']), biases['boutg'])\n",
    "    #tf.matmul(Atem1, layer_1)#tf.matmul(Atem, layer_3)\n",
    "    #out_layer = tf.add(tf.matmul(Z4, weights['out']), biases['bout'])\n",
    "    #out_layer = tf.nn.relu(out_layer)\n",
    "    # weather layer 1\n",
    "    #x_wea_tem = tf.reshape(x_wea[i,:], [1, 9*frequency2]) # 1 by 126\n",
    "    #layer_1_wea = tf.add(tf.matmul(x_wea_tem, weights['h1_wea']), biases['b1_wea'])\n",
    "    #layer_1_wea = tf.nn.relu(layer_1_wea)\n",
    "\n",
    "    #out_layer_wea = tf.add(tf.matmul(layer_1_wea, weights['out_wea']), biases['bout_wea'])\n",
    "    #out_layer = tf.add(out_layer, tf.reshape(out_layer_wea, [272, 1]))\n",
    "\n",
    "\n",
    "    \n",
    "    return Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_corr_final(frequency, horizon, learning_rate, decay,batch_size, n_hidden_vec1,n_hidden_vec2,n_hidden_vec3,keep, early_stop_th,training_epochs, reg1, reg2):\n",
    "    # set size\n",
    "    #sn = 3 # station number\n",
    "\n",
    "    \n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    predic_res = []\n",
    "    Y_true = []\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    \n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "    n_hidden_vec1 = int(n_hidden_vec1)\n",
    "    n_hidden_vec2 = int(n_hidden_vec2)\n",
    "    n_hidden_vec3 = int(n_hidden_vec3)\n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    # Network Parameters\n",
    "\n",
    "    #n_classes = 2 # MNIST total classes (0-9 digits) # n_classes is for classification only\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None, lstm_steps, sn, feature_in]) # X is the input signal\n",
    "    #X_weather = tf.placeholder(tf.float32, [None, 9 * frequency2]) # X_weather weather and holiday information (9 is the feature number)\n",
    "    A = tf.placeholder(tf.float32, [None, n_A_vec]) # A is the normalized adj matrix\n",
    "    oldA = tf.placeholder(tf.float32, [n_input, n_input])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_output_vec]) # y is the regression output\n",
    "    #num = tf.placeholder(tf.int32,[1, 1] )\n",
    "\n",
    "    #Xtem = tf.placeholder(tf.float32, [n_input, frequency]) # for each row of X, A, Y, it can be reshaped to Xtem, Atem, Ytem\n",
    "    #Atem = tf.placeholder(tf.float32, [n_input, n_input]) # \n",
    "    #Ytem = tf.placeholder(tf.float32, [n_input, 1]) #\n",
    "\n",
    "    #Ypre = tf.placeholder(tf.float32, [None, n_output_vec])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([frequency, n_hidden_vec1]), dtype=np.float32),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_vec1, n_hidden_vec2]), dtype=np.float32),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_vec2, n_hidden_vec3]), dtype=np.float32),\n",
    "        #'h4': tf.Variable(tf.random_normal([n_hidden_vec3, n_hidden_vec4])),\n",
    "        'outg': tf.Variable(tf.random_normal([sn*n_hidden_vec3, n_hidden_vec4]), dtype=np.float32), \n",
    "        'out': tf.Variable(tf.random_normal([num_hidden, Y_whole.shape[1]]), dtype=np.float32), # dont forget to change n_hidden_vec1 when add/delete layers\n",
    "        #'f1': tf.Variable(tf.random_normal([272*n_hidden_vec3, 100])),\n",
    "        #'f2': tf.Variable(tf.random_normal([50, 10])),\n",
    "        #'f3': tf.Variable(tf.random_normal([100, 272])),\n",
    "        'A1': tf.Variable(tf.random_normal([n_input,n_input]), dtype=np.float32),\n",
    "        'A2': tf.Variable(tf.random_normal([n_input,n_input]), dtype=np.float32),\n",
    "        'A3': tf.Variable(tf.random_normal([n_input,n_input]), dtype=np.float32),\n",
    "        #'A4': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'h1_wea': tf.Variable(tf.random_normal([9*frequency2, n_hidden_weather1])),\n",
    "        #'out_wea': tf.Variable(tf.random_normal([n_hidden_weather1, n_input]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([1, n_hidden_vec1]), dtype=np.float32),# n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        'b2': tf.Variable(tf.random_normal([1, n_hidden_vec2]), dtype=np.float32), #n_hidden_vec2])),\n",
    "        'b3': tf.Variable(tf.random_normal([1, n_hidden_vec3]), dtype=np.float32),#n_hidden_vec3])),\n",
    "        #'b4': tf.Variable(tf.random_normal([n_input, n_hidden_vec4])),\n",
    "        #'b1': tf.Variable(tf.random_normal([n_input,n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        #'b2': tf.Variable(tf.random_normal([n_input,n_hidden_vec2])),\n",
    "        #'b3': tf.Variable(tf.random_normal([n_input,n_hidden_vec3])),\n",
    "        #'bf1': tf.Variable(tf.random_normal([1, 100])), \n",
    "        #'bf2': tf.Variable(tf.random_normal([1, 10])), \n",
    "        #'bf3': tf.Variable(tf.random_normal([1, 272])), \n",
    "        'boutg': tf.Variable(tf.random_normal([1, n_hidden_vec4]), dtype=np.float32), \n",
    "        'bout': tf.Variable(tf.random_normal([Y_whole.shape[1]]), dtype=np.float32), \n",
    "        #'b1_wea': tf.Variable(tf.random_normal([1, n_hidden_weather1])), \n",
    "        #'bout_wea': tf.Variable(tf.random_normal([1, n_input])), \n",
    "    }\n",
    "    \n",
    "    with tf.variable_scope('lstm'):\n",
    "        lstm = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(num_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "        rnn_input_seq = tf.unstack(X, lstm_steps, 1) # lstm_steps is the 2nd variable\n",
    "\n",
    "        for i in range(lstm_steps):\n",
    "            rnn_input_seq[i] = gcn(rnn_input_seq[i], weights, biases, batch_size,n_input, frequency, 1, n_output_vec)\n",
    "            #print (rnn_input_seq[i].shape)\n",
    "        outputs, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs[-1], [-1, num_hidden])\n",
    "        #print ('123here!!!!!!!!!!!')\n",
    "        pred = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        #print (pred)\n",
    "        #pred = tf.reshape(pred, [-1, Y_whole.shape[1]])\n",
    "        #print ('here!!!!!!!!!!!')\n",
    "        pred = scaler.inverse_transform(pred)\n",
    "        Y_true_tr = scaler.inverse_transform(Y)\n",
    "\n",
    "        cost = tf.reduce_mean(tf.pow(pred - Y_true_tr, 2)) \n",
    "        #print (cost)\n",
    "        \n",
    "    with tf.variable_scope('lstm', reuse=True):\n",
    "        rnn_input_seq_val = tf.unstack(X, lstm_steps, 1)\n",
    "\n",
    "        for i in range(lstm_steps):\n",
    "            rnn_input_seq_val[i] = gcn(rnn_input_seq_val[i], weights, biases, batch_size,n_input,frequency, 2, n_output_vec)\n",
    "        outputs_val, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq_val, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs_val[-1], [-1, num_hidden])\n",
    "\n",
    "        pred_val = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        pred_val = scaler.inverse_transform(pred_val)\n",
    "        Y_true_val = scaler.inverse_transform(Y)\n",
    "        \n",
    "        cost_val =  tf.reduce_mean(tf.pow(pred_val - Y_true_val, 2)) \n",
    "        #print ('234here!!!!!!!!!!!')\n",
    "    with tf.variable_scope('lstm', reuse=True):\n",
    "        rnn_input_seq_test = tf.unstack(X, lstm_steps, 1)\n",
    "        \n",
    "        for i in range(lstm_steps):\n",
    "            rnn_input_seq_test[i] = gcn(rnn_input_seq_test[i], weights, biases, batch_size,n_input,frequency, 3, n_output_vec)\n",
    "        outputs_test, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq_test, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs_test[-1], [-1, num_hidden])\n",
    "\n",
    "        pred_tes = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        pred_tes = scaler.inverse_transform(pred_tes)\n",
    "        Y_true_tes = scaler.inverse_transform(Y)\n",
    "        \n",
    "        cost_tes = tf.reduce_mean(tf.pow(pred_tes - Y_true_tes, 2)) \n",
    "        \n",
    "        #print ('345here!!!!!!!!!!!')\n",
    "    #rmse\n",
    "    #cost_tes = tf.reduce_mean(tf.pow(pred_tes-Y, 2))\n",
    "    # cross-entropy for classification\n",
    "    # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y_train))\n",
    "    # ratio = tf.abs(tf.reduce_sum(pred)-tf.reduce_sum(Y))/tf.reduce_sum(Y)\n",
    "    #zero = 0\n",
    "    #ratio = tf.reduce_mean(tf.divide(tf.where(tf.not_equal(Y, zero), np.abs(pred-Y), tf.zeros(Y.get_shape(), tf.float32)), tf.where(tf.not_equal(Y, zero), Y, tf.ones(Y.get_shape(), tf.float32))))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay).minimize(cost)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #total_val_cost = []\n",
    "    #total_val_ratio = []\n",
    "\n",
    "    # learning start from \n",
    "\n",
    "    #index = daily_bike[(daily_bike['year'] == 2016) & (daily_bike['monthofyear'] == 1) & (daily_bike['dayofmonth'] == 1)].index.tolist()[0]\n",
    "    #A_hat = normalize_adj(corr_matrix_trips)\n",
    "    #print(A_hat)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(num_train/batch_size) #int(num_train/batch_size)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                #print (Y_training[i*batch_size:(i+1)*batch_size,].size())\n",
    "                #num = batch_size\n",
    "                _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                      Y: Y_training[i*batch_size:(i+1)*batch_size,],  \n",
    "                                                              keep_prob: keep})\n",
    "                #print (preds)\n",
    "                #print (trueval)\n",
    "                #print (\"Epoch:\", '%04d' % (epoch+1), \"batch: \", i, \"batch cost=\", \\\n",
    "                #    \"{:.9f}\".format(c))\n",
    "                #print ('here!!!!!!!!!!!!!!!!')\n",
    "                avg_cost += c * batch_size #/ total_batch \n",
    "                #Display logs per epoch step\n",
    "                \n",
    "            # rest part of training dataset\n",
    "            #num = num_train - total_batch*batch_size \n",
    "            if total_batch * batch_size != num_train:\n",
    "                _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[total_batch*batch_size:num_train,], \n",
    "                                          Y: Y_training[total_batch*batch_size:num_train,],\n",
    "                                                  keep_prob: keep})\n",
    "                avg_cost += c * (num_train - total_batch*batch_size)\n",
    "            \n",
    "            avg_cost = np.sqrt(avg_cost / num_train)\n",
    "            \n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Epoch:\", '%04d' % (epoch+1), \"Training RMSE=\", \\\n",
    "                    \"{:.9f}\".format(avg_cost)) #np.sqrt(avg_cost)\n",
    "                \n",
    "            # also use batch to save memory\n",
    "            # validation\n",
    "            c_val = 0.\n",
    "            total_bat_val = int(num_val/batch_size)\n",
    "            for i in range(total_bat_val):\n",
    "                #num = batch_size\n",
    "                c_val_b = sess.run([cost_val], feed_dict={X: X_val[i*batch_size:(i+1)*batch_size,], \n",
    "                                                          Y: Y_val[i*batch_size:(i+1)*batch_size,],   keep_prob:1})\n",
    "                c_val += c_val_b[0]*batch_size\n",
    "            \n",
    "            if total_bat_val * batch_size != num_val:\n",
    "                #num = num_val - total_bat_val*batch_size\n",
    "                c_val_b = sess.run([cost_val], feed_dict={X: X_val[total_bat_val*batch_size:num_val,], \n",
    "                                                          Y: Y_val[total_bat_val*batch_size:num_val,],  keep_prob:1})\n",
    "                c_val += c_val_b[0] * (num_val - total_bat_val*batch_size)\n",
    "                \n",
    "            c_val = np.sqrt(c_val / num_val)\n",
    "            \n",
    "            print(\"Validation RMSE: \", c_val)\n",
    "            \n",
    "            # test\n",
    "            c_tes = 0.\n",
    "            total_bat_test = int(num_test/batch_size)\n",
    "            \n",
    "            pre_test_tem = [] # save the prediction results\n",
    "            Y_tes_true = []\n",
    "            \n",
    "            for i in range(total_bat_test):\n",
    "                #num = batch_size\n",
    "                c_tes_b, pred_tes1, Y_tes_batch = sess.run([cost_tes, pred_tes, Y_true_tes], feed_dict={X: X_test[i*batch_size:(i+1)*batch_size,],\n",
    "                                                                               Y: Y_test[i*batch_size:(i+1)*batch_size,],  keep_prob: 1})\n",
    "                c_tes += c_tes_b*batch_size\n",
    "\n",
    "                #print (cost_h)\n",
    "                #print (pred_tes1.shape)\n",
    "                pre_test_tem.append(pred_tes1)\n",
    "                Y_tes_true.append(Y_tes_batch)\n",
    "                \n",
    "            if total_bat_test * batch_size != num_test:\n",
    "                #num = num_test - total_bat_test*batch_size\n",
    "                c_tes_b, pred_tes1, Y_tes_batch = sess.run([cost_tes, pred_tes, Y_true_tes], feed_dict={X: X_test[total_bat_test*batch_size:num_test,],\n",
    "                                                                               Y: Y_test[total_bat_test*batch_size:num_test,],  keep_prob: 1})\n",
    "                c_tes += c_tes_b * (num_test - total_bat_test*batch_size) \n",
    "                 \n",
    "                \n",
    "                #print (pred_tes1.shape)\n",
    "                pre_test_tem.append(pred_tes1)\n",
    "                Y_tes_true.append(Y_tes_batch)\n",
    "            \n",
    "            #print (c_tes_h.shape)\n",
    "            pre_test_tem = np.concatenate(pre_test_tem, axis = 0)\n",
    "            #print (pre_test_tem.shape)\n",
    "            Y_tes_true = np.concatenate(Y_tes_true, axis = 0)\n",
    "            \n",
    "            c_tes = np.sqrt(c_tes / num_test)\n",
    "            #c_tes_h = c_tes_h / num_test\n",
    "            \n",
    "            print(\"Test RMSE: \", c_tes)\n",
    "            #print(\"predic step: \", cost_by_hor)\n",
    "\n",
    "            if c_val < best_val:\n",
    "                best_val = c_val\n",
    "                #saver.save(sess, './bikesharing_graph_2_th_point1')\n",
    "                test_error = c_tes\n",
    "                \n",
    "                traing_error = avg_cost#np.sqrt(avg_cost)\n",
    "                early_stop_k = 0 # reset to 0\n",
    "                #print (pred_tes1)\n",
    "                predic_res = pre_test_tem\n",
    "                Y_true = Y_tes_true\n",
    "                #predic_step = cost_by_hor\n",
    "\n",
    "            # early stopping\n",
    "            if c_val >= best_val:\n",
    "                early_stop_k += 1\n",
    "\n",
    "            # threshold\n",
    "            if early_stop_k == early_stop_th:\n",
    "              #  print (\"early stopping...\")\n",
    "                break\n",
    "            \n",
    "\n",
    "        print(\"epoch is \", epoch)\n",
    "        print(\"training RMSE is \", traing_error)\n",
    "        print(\"Optimization Finished! the lowest validation RMSE is \", best_val)#(np.sqrt(best_val)))\n",
    "        print(\"The test RMSE is \", test_error)#(np.sqrt(test_error)))\n",
    "    \n",
    "    #test_Y = Y_test\n",
    "    #test_error = np.sqrt(test_error)\n",
    "    return test_error, predic_res, Y_true#, A1#, predic_step#, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training RMSE= 8.890843447\n",
      "Validation RMSE:  7.949461327303071\n",
      "Test RMSE:  7.077811598418333\n",
      "Epoch: 0002 Training RMSE= 7.701911196\n",
      "Validation RMSE:  7.721481025429916\n",
      "Test RMSE:  6.828145391577817\n",
      "Epoch: 0003 Training RMSE= 7.501316464\n",
      "Validation RMSE:  7.4597095377728655\n",
      "Test RMSE:  6.539046019757832\n",
      "Epoch: 0004 Training RMSE= 7.181984857\n",
      "Validation RMSE:  7.202731107256635\n",
      "Test RMSE:  6.25322823122326\n",
      "Epoch: 0005 Training RMSE= 6.930948164\n",
      "Validation RMSE:  6.9651709975525\n",
      "Test RMSE:  5.987414193384698\n",
      "Epoch: 0006 Training RMSE= 6.731888668\n",
      "Validation RMSE:  6.750086042067714\n",
      "Test RMSE:  5.745383856078419\n",
      "Epoch: 0007 Training RMSE= 6.485753777\n",
      "Validation RMSE:  6.555448976645336\n",
      "Test RMSE:  5.525277917008398\n",
      "Epoch: 0008 Training RMSE= 6.309747183\n",
      "Validation RMSE:  6.381301159331378\n",
      "Test RMSE:  5.3271666081553075\n",
      "Epoch: 0009 Training RMSE= 6.126506309\n",
      "Validation RMSE:  6.226001900932102\n",
      "Test RMSE:  5.149594352079278\n",
      "Epoch: 0010 Training RMSE= 6.069273604\n",
      "Validation RMSE:  6.091311392315239\n",
      "Test RMSE:  4.995908970879324\n",
      "Epoch: 0011 Training RMSE= 5.845362933\n",
      "Validation RMSE:  5.970312263590596\n",
      "Test RMSE:  4.858151002650163\n",
      "Epoch: 0012 Training RMSE= 5.727800913\n",
      "Validation RMSE:  5.863766908704603\n",
      "Test RMSE:  4.737602619277606\n",
      "Epoch: 0013 Training RMSE= 5.674089118\n",
      "Validation RMSE:  5.771824179003768\n",
      "Test RMSE:  4.635801537927163\n",
      "Epoch: 0014 Training RMSE= 5.537973867\n",
      "Validation RMSE:  5.6913472671960825\n",
      "Test RMSE:  4.54800631681891\n",
      "Epoch: 0015 Training RMSE= 5.461825921\n",
      "Validation RMSE:  5.621238552876685\n",
      "Test RMSE:  4.4734923268434965\n",
      "Epoch: 0016 Training RMSE= 5.396570950\n",
      "Validation RMSE:  5.5610166136217885\n",
      "Test RMSE:  4.410869584623162\n",
      "Epoch: 0017 Training RMSE= 5.341006158\n",
      "Validation RMSE:  5.509754633900992\n",
      "Test RMSE:  4.358179454870824\n",
      "Epoch: 0018 Training RMSE= 5.293982341\n",
      "Validation RMSE:  5.466374257261094\n",
      "Test RMSE:  4.314187535149572\n",
      "Epoch: 0019 Training RMSE= 5.254583801\n",
      "Validation RMSE:  5.430017870176356\n",
      "Test RMSE:  4.27810603625913\n",
      "Epoch: 0020 Training RMSE= 5.221867755\n",
      "Validation RMSE:  5.399770834263343\n",
      "Test RMSE:  4.249192722168602\n",
      "Epoch: 0021 Training RMSE= 5.194869191\n",
      "Validation RMSE:  5.374927963838686\n",
      "Test RMSE:  4.226159152318462\n",
      "Epoch: 0022 Training RMSE= 5.172638556\n",
      "Validation RMSE:  5.354613833425782\n",
      "Test RMSE:  4.207719525182269\n",
      "Epoch: 0023 Training RMSE= 5.154309124\n",
      "Validation RMSE:  5.337954496021779\n",
      "Test RMSE:  4.193194431850787\n",
      "Epoch: 0024 Training RMSE= 5.139209569\n",
      "Validation RMSE:  5.3242039725755586\n",
      "Test RMSE:  4.182032189528547\n",
      "Epoch: 0025 Training RMSE= 5.126824636\n",
      "Validation RMSE:  5.312693783086798\n",
      "Test RMSE:  4.173551186998675\n",
      "Epoch: 0026 Training RMSE= 5.116677659\n",
      "Validation RMSE:  5.302821779321099\n",
      "Test RMSE:  4.166811609926077\n",
      "Epoch: 0027 Training RMSE= 5.108379162\n",
      "Validation RMSE:  5.294151483689428\n",
      "Test RMSE:  4.161282984063426\n",
      "Epoch: 0028 Training RMSE= 5.101737910\n",
      "Validation RMSE:  5.286727863324771\n",
      "Test RMSE:  4.157048887184207\n",
      "Epoch: 0029 Training RMSE= 5.096490864\n",
      "Validation RMSE:  5.280741289004922\n",
      "Test RMSE:  4.154037804679038\n",
      "Epoch: 0030 Training RMSE= 5.092498260\n",
      "Validation RMSE:  5.276093985124617\n",
      "Test RMSE:  4.152017144522431\n",
      "Epoch: 0031 Training RMSE= 5.089512472\n",
      "Validation RMSE:  5.272517099945136\n",
      "Test RMSE:  4.150942920513845\n",
      "Epoch: 0032 Training RMSE= 5.087343889\n",
      "Validation RMSE:  5.269828680536454\n",
      "Test RMSE:  4.150524411906205\n",
      "Epoch: 0033 Training RMSE= 5.085787601\n",
      "Validation RMSE:  5.26789793317395\n",
      "Test RMSE:  4.15038028491359\n",
      "Epoch: 0034 Training RMSE= 5.084686726\n",
      "Validation RMSE:  5.266628905454702\n",
      "Test RMSE:  4.15037390851413\n",
      "Epoch: 0035 Training RMSE= 5.083911132\n",
      "Validation RMSE:  5.265924281096408\n",
      "Test RMSE:  4.1503007802775524\n",
      "Epoch: 0036 Training RMSE= 5.083352393\n",
      "Validation RMSE:  5.265537068795905\n",
      "Test RMSE:  4.150245401871069\n",
      "Epoch: 0037 Training RMSE= 5.121458412\n",
      "Validation RMSE:  5.265185600987091\n",
      "Test RMSE:  4.150153083735432\n",
      "Epoch: 0038 Training RMSE= 5.082717106\n",
      "Validation RMSE:  5.265191940475099\n",
      "Test RMSE:  4.150600235975295\n",
      "Epoch: 0039 Training RMSE= 5.082651566\n",
      "Validation RMSE:  5.2652651157269945\n",
      "Test RMSE:  4.15078037002758\n",
      "Epoch: 0040 Training RMSE= 5.082646246\n",
      "Validation RMSE:  5.265283137693601\n",
      "Test RMSE:  4.15081971587055\n",
      "Epoch: 0041 Training RMSE= 5.082647597\n",
      "Validation RMSE:  5.265283047131108\n",
      "Test RMSE:  4.150811961609838\n",
      "Epoch: 0042 Training RMSE= 5.082647776\n",
      "Validation RMSE:  5.265283137693601\n",
      "Test RMSE:  4.150810353316913\n",
      "Epoch: 0043 Training RMSE= 5.082647729\n",
      "Validation RMSE:  5.265282684881124\n",
      "Test RMSE:  4.150809491731162\n",
      "Epoch: 0044 Training RMSE= 5.082647719\n",
      "Validation RMSE:  5.265282866006119\n",
      "Test RMSE:  4.150808917340561\n",
      "Epoch: 0045 Training RMSE= 5.082647832\n",
      "Validation RMSE:  5.265282775443623\n",
      "Test RMSE:  4.15080931941399\n",
      "Epoch: 0046 Training RMSE= 5.082647841\n",
      "Validation RMSE:  5.265282956568615\n",
      "Test RMSE:  4.150809032218688\n",
      "Epoch: 0047 Training RMSE= 5.082647860\n",
      "Validation RMSE:  5.265283047131108\n",
      "Test RMSE:  4.1508092045358715\n",
      "Epoch: 0048 Training RMSE= 5.082647804\n",
      "Validation RMSE:  5.265282866006119\n",
      "Test RMSE:  4.150808917340561\n",
      "Epoch: 0049 Training RMSE= 5.082647851\n",
      "Validation RMSE:  5.2652825943186246\n",
      "Test RMSE:  4.150809261974931\n",
      "Epoch: 0050 Training RMSE= 5.082647794\n",
      "Validation RMSE:  5.265282956568615\n",
      "Test RMSE:  4.1508092045358715\n",
      "Epoch: 0051 Training RMSE= 5.082647813\n",
      "Validation RMSE:  5.265282956568615\n",
      "Test RMSE:  4.150809147096811\n",
      "Epoch: 0052 Training RMSE= 5.082647701\n",
      "Validation RMSE:  5.265282775443623\n",
      "Test RMSE:  4.1508089747796255\n",
      "Epoch: 0053 Training RMSE= 5.082647729\n",
      "Validation RMSE:  5.265283137693601\n",
      "Test RMSE:  4.150809147096811\n",
      "Epoch: 0054 Training RMSE= 5.082647804\n",
      "Validation RMSE:  5.2652825943186246\n",
      "Test RMSE:  4.150808859901497\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-540d1ccacc35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     test_error, predic_res, Y_true = gcn_corr_final(frequency, horizon, learning_rate, decay, batch_size, n_hidden_vec1,\n\u001b[0;32m---> 61\u001b[0;31m                                                                 n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;31m#val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m#print (\"finished A running: \", i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-0c7c9fcb1628>\u001b[0m in \u001b[0;36mgcn_corr_final\u001b[0;34m(frequency, horizon, learning_rate, decay, batch_size, n_hidden_vec1, n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n\u001b[1;32m    177\u001b[0m                                                       \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                                               keep_prob: keep})\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0;31m#print (preds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;31m#print (trueval)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "#freq_max = 12\n",
    "#time_step = 12\n",
    "learning_rate = 0.005\n",
    "decay = 0.9 \n",
    "batch_size = 1000\n",
    "num_hidden = 50 # number of hiddent units in LSTM Cell\n",
    "early_stop_th = 150\n",
    "training_epochs = 1000\n",
    "keep = 1#0.2\n",
    "#time_step_max = 10\n",
    "\n",
    "sn = 272 # station num\n",
    "\n",
    "n_hidden_vec1 = 10\n",
    "n_hidden_vec2 = 10\n",
    "n_hidden_vec3 = 10\n",
    "#n_hidden_vec4 = 10\n",
    "#num = 0\n",
    "#All_pred = np.empty([2000, 207])\n",
    "#All_Y = np.empty([2000, 207])\n",
    "\n",
    "#24*90\n",
    "#step = 0\n",
    "#gap = 100\n",
    "#training = 0.7\n",
    "#validation = 0.1\n",
    "#test = 0.2\n",
    "\n",
    "#gcn_corr_eval(7, 0.01, 0.5, 100, 0.4, 10, 5, 5, 0.2, 50, 500)\n",
    "\n",
    "\n",
    "\n",
    "rep = 1 # repeating times\n",
    "\n",
    "#total_sn = 0\n",
    "#num_iter = 50\n",
    "#init_points = 200\n",
    "\n",
    "\n",
    "# stdbscan\n",
    "#spatial_threshold = 300\n",
    "#temporal_threshold = 300\n",
    "#min_neighbors = 1 # number of neighbor\n",
    "\n",
    "#frequency2 = skip1 + freq_max + training\n",
    "\n",
    "#while step < 2000:\n",
    "\n",
    "#hourly_bike_cluster = hourly_bike\n",
    "best = -10000\n",
    "pre_best = []\n",
    "test_Y_best = []\n",
    "test_error_best = 1000\n",
    "A1_best = []\n",
    "# A2_best = []\n",
    "for i in range(rep):\n",
    "    a = datetime.datetime.now()\n",
    "    test_error, predic_res, Y_true = gcn_corr_final(frequency, horizon, learning_rate, decay, batch_size, n_hidden_vec1,\n",
    "                                                                n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\n",
    "    #val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\n",
    "    #print (\"finished A running: \", i)\n",
    "    b = datetime.datetime.now()\n",
    "    print(b-a)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm cell num 10, hidden num [10, 10, 5], batchsize 100, hidden state 50: 4.14, not decreasing\n",
    "#lstm cell num 10, hidden num [10, 10, 5], batchsize 500, hidden state 50: 2.43\n",
    "#lstm cell num 10, hidden num [10, 10, 10], batchsize 500, hidden state 50: 4.13, not dropping\n",
    "#lstm cell num 10, hidden num [10, 10, 10], batchsize 1000, hidden state 50:4.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"lstm_gcnn_prediction_1.95.csv\", predic_res, delimiter = ',')\n",
    "#np.savetxt(\"lstm_gcnn_prediction_1.95_Y.csv\", Y_true, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
