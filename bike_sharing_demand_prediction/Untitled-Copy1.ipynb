{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"../../data/nyc_bike/NYCBikeHourly272.pickle\"\n",
    "# open the file for writing\n",
    "fileObject = open(file_Name,'rb') \n",
    "hourly_bike = pickle.load(fileObject) \n",
    "hourly_bike = pd.DataFrame(hourly_bike)\n",
    "# results from bikedemandhourly_lstm_weather.ipynb\n",
    "# new station id for the selected 272 stations\n",
    "station_list = [0, 2, 3, 7, 8, 9, 11, 12, 27, 32, 33, 34, 38, 39, 40, 42, 43, 44, 46, 48, 52, 53, 54, 58, 59, 61, 62, 64, 65, 67, 68, 69, 74, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 104, 108, 111, 112, 115, 120, 121, 122, 124, 127, 130, 131, 133, 134, 135, 136, 138, 141, 142, 143, 145, 146, 150, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181, 182, 185, 186, 187, 188, 189, 195, 197, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 212, 214, 216, 219, 220, 221, 226, 242, 245, 249, 251, 252, 255, 257, 258, 263, 267, 273, 274, 275, 276, 280, 281, 282, 284, 290, 292, 295, 298, 299, 305, 306, 307, 308, 311, 316, 318, 320, 321, 322, 323, 327, 335, 340, 341, 342, 344, 345, 347, 348, 349, 353, 359, 361, 369, 371, 372, 393, 396, 397, 403, 404, 408, 410, 412, 414, 415, 416, 422, 429, 432, 434, 435, 438, 443, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 467, 472, 475, 476, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 506, 508, 509, 510, 511, 512, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 534, 535, 537, 539, 540, 541, 542, 544, 545, 546, 547, 548, 549, 550, 553, 554]\n",
    "new_old_stationID = pd.read_csv('../../data/nyc_bike/bike_stations.csv')\n",
    "#new_old_stationID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_old_stationID = new_old_stationID[new_old_stationID['station_id_new'].isin(station_list)]\n",
    "#new_old_stationID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_old_stationID = new_old_stationID.drop_duplicates('station_id_new', keep ='last')\n",
    "new_old_stationID = new_old_stationID.reset_index()\n",
    "#new_old_stationID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_list = list(range(272))\n",
    "#station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_bike_gps = hourly_bike.transpose()\n",
    "hourly_bike_gps['lat'] = new_old_stationID['lat']\n",
    "hourly_bike_gps['lon'] = new_old_stationID['lon']\n",
    "hourly_bike_gps['index1'] = range(272)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26297</th>\n",
       "      <th>26298</th>\n",
       "      <th>26299</th>\n",
       "      <th>26300</th>\n",
       "      <th>26301</th>\n",
       "      <th>26302</th>\n",
       "      <th>26303</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>index1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>40.732219</td>\n",
       "      <td>-73.981656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>40.741444</td>\n",
       "      <td>-73.975361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.750020</td>\n",
       "      <td>-73.969053</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.750664</td>\n",
       "      <td>-74.001768</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>40.751396</td>\n",
       "      <td>-74.005226</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8   9  ...  26297  26298  26299  26300  26301  \\\n",
       "0  2  1  0  0  0  0  2  4  3   2  ...     23     21     24     12      8   \n",
       "1  1  0  0  0  0  0  1  1  4  13  ...      3     12     18     11      8   \n",
       "2  0  0  0  0  0  0  0  2  4   0  ...     17      8      7      5      1   \n",
       "3  3  1  0  1  1  0  0  2  7   4  ...      0      0      0      0      0   \n",
       "4  0  2  0  0  0  0  3  4  4   3  ...     19     41     18     24      8   \n",
       "\n",
       "   26302  26303        lat        lon  index1  \n",
       "0      5      6  40.732219 -73.981656       0  \n",
       "1      4      1  40.741444 -73.975361       1  \n",
       "2      0      1  40.750020 -73.969053       2  \n",
       "3      0      0  40.750664 -74.001768       3  \n",
       "4      5      6  40.751396 -74.005226       4  \n",
       "\n",
       "[5 rows x 26307 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_bike_gps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    #adj[np.isnan(adj)] = 0.\n",
    "    adj = tf.abs(adj)\n",
    "    rowsum = tf.reduce_sum(adj, 1)# sum by row\n",
    "\n",
    "    d_inv_sqrt = tf.pow(rowsum, -0.5)\n",
    "   \n",
    "    #d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    \n",
    "    d_mat_inv_sqrt = tf.diag(d_inv_sqrt)\n",
    "\n",
    "    return tf.matmul(tf.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def gcn(x, weights, biases, batch_size, n_input, frequency,flag):\n",
    "    # Hidden layer with RELU activation\n",
    "    \n",
    "    output_list = tf.Variable(tf.zeros([sn,1]),dtype=tf.float32) #'Tensor' object does not support item assignment, cant build Ypre\n",
    "    \n",
    "    if flag == 1:\n",
    "        rownum = batch_size\n",
    "    elif flag == 2:\n",
    "        rownum = validation\n",
    "    elif flag == 3:\n",
    "        rownum = test\n",
    "    \n",
    "    for i in range(rownum):\n",
    "        Xtem = tf.reshape(x[i,:], [frequency, n_input])\n",
    "        Xtem = tf.transpose(Xtem)\n",
    "        #Atem = tf.convert_to_tensor(A_whole_final, dtype=np.float32)\n",
    "        #Atem1 = tf.diag(tf.ones([n_input])) # Atem not palying any roles\n",
    "        #Ytem = tf.reshape(Y[i,:], [n_input, 1])\n",
    "        Atem = tf.diag(tf.ones([n_input]))\n",
    "        Atem1 = 0.5*(weights['A1'] + tf.transpose(weights['A1']))#+ Atem \n",
    "        Atem1 = normalize_adj(Atem1)\n",
    "        #th = tf.constant(0.01, dtype=tf.float32)\n",
    "        #where = tf.subtract(Atem1, th)\n",
    "        #Atem1 = tf.nn.relu(where)\n",
    "        \n",
    "        Z1 = tf.matmul(Atem1, Xtem)\n",
    "        #layer_1 = tf.matmul(Z1, weights['h1']) \n",
    "        layer_1 = tf.add(tf.matmul(Z1, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        \n",
    "        #Atem2 = 0.5*(weights['A2'] + tf.transpose(weights['A2']))#+ Atem \n",
    "        #Atem2 = normalize_adj(Atem2)\n",
    "        \n",
    "        #Z2 = tf.matmul(Atem2, layer_1)\n",
    "        #layer_2 = tf.add(tf.matmul(Z2, weights['h2']), biases['b2'])\n",
    "        #layer_2 = tf.nn.relu(layer_2)\n",
    "        \n",
    "        #Atem3 = weights['A3']+ Atem \n",
    "        #Z3 = tf.matmul(Atem3, layer_2)\n",
    "        #layer_3 = tf.add(tf.matmul(Z3, weights['h3']), biases['b3'])\n",
    "        #layer_3 = tf.nn.relu(layer_3)\n",
    "        \n",
    "        # flattern\n",
    "        #layer_3 = tf.reshape(layer_3, [1, 272*n_hidden_vec3])\n",
    "        \n",
    "        #F1 = tf.add(tf.matmul(layer_3, weights['f1']), biases['bf1'])\n",
    "        #F1 = tf.nn.relu(F1)\n",
    "        \n",
    "        #F2 = tf.add(tf.matmul(F1, weights['f2']), biases['bf2'])\n",
    "        #F2 = tf.nn.relu(F2)\n",
    "        \n",
    "        #F3 = tf.add(tf.matmul(F1, weights['f3']), biases['bf3'])\n",
    "        #out_layer = tf.reshape(F3, [272, 1])\n",
    "\n",
    "        # Output layer with linear activation\n",
    "        Z4 = layer_1#tf.matmul(Atem1, layer_1)#tf.matmul(Atem, layer_3)\n",
    "        out_layer = tf.add(tf.matmul(Z4, weights['out']), biases['bout'])\n",
    "        \n",
    "        # weather layer 1\n",
    "        #x_wea_tem = tf.reshape(x_wea[i,:], [1, 9*frequency2]) # 1 by 126\n",
    "        #layer_1_wea = tf.add(tf.matmul(x_wea_tem, weights['h1_wea']), biases['b1_wea'])\n",
    "        #layer_1_wea = tf.nn.relu(layer_1_wea)\n",
    "        \n",
    "        #out_layer_wea = tf.add(tf.matmul(layer_1_wea, weights['out_wea']), biases['bout_wea'])\n",
    "        #out_layer = tf.add(out_layer, tf.reshape(out_layer_wea, [272, 1]))\n",
    "        \n",
    "        #print (out_layer.get_shape())\n",
    "        if i ==0:\n",
    "            output_list = out_layer\n",
    "        else:\n",
    "            output_list = tf.concat([output_list, out_layer], 1)\n",
    "        \n",
    "        #print (tf.reduce_mean(tf.pow(output_list-out_layer, 2)))\n",
    "    \n",
    "    #print (output_list.get_shape())\n",
    "    \n",
    "    output_list = tf.transpose(output_list)\n",
    "    \n",
    "    #print (output_list.get_shape())\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_corr_final(frequency, learning_rate, decay,batch_size, n_hidden_vec1,n_hidden_vec2,n_hidden_vec3,keep, early_stop_th,training_epochs, reg1, reg2):\n",
    "    # set size\n",
    "    #sn = 3 # station number\n",
    "\n",
    "    frequency = int(frequency) #\n",
    "\n",
    "    i = frequency\n",
    "    X_whole = np.zeros(shape = (hourly_bike_cluster.shape[0] - frequency, sn*frequency), dtype = np.float)\n",
    "    Y_whole = np.zeros(shape = (hourly_bike_cluster.shape[0] - frequency, sn), dtype = np.float)\n",
    "\n",
    "    while i < hourly_bike_cluster.shape[0]:\n",
    "        X_whole[i - frequency, ] = hourly_bike_cluster.iloc[(i - frequency):i, 0:sn].values.flatten() # flatten by row 0, 1, 2...7\n",
    "        Y_whole[i - frequency, ] = hourly_bike_cluster.iloc[i, 0:sn]\n",
    "        i = i + 1\n",
    "        #print (i)\n",
    "    \n",
    "    skip = skip1 + freq_max - frequency # to make sure the testing datasets are the same although the frequency could be different\n",
    "\n",
    "    X_training = X_whole[skip: skip+ training, :]\n",
    "    Y_training = Y_whole[skip: skip+ training, :]\n",
    "    #A_training = A_whole[0: 0+ training, :]\n",
    "\n",
    "    X_val = X_whole[skip+training:skip+training+validation, :]\n",
    "    Y_val = Y_whole[skip+training:skip+training+validation, :]\n",
    "    #A_val = A_whole[0+training:0+training+validation, :]\n",
    "\n",
    "    X_test = X_whole[skip+training+validation:skip+training+validation+test, :]\n",
    "    Y_test = Y_whole[skip+training+validation:skip+training+validation+test, :]\n",
    "    #A_test = A_whole[0+training+validation:0+training+validation+test, :]\n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "    n_hidden_vec1 = int(n_hidden_vec1)\n",
    "    n_hidden_vec2 = int(n_hidden_vec2)\n",
    "    n_hidden_vec3 = int(n_hidden_vec3)\n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    # Network Parameters\n",
    "    n_input = sn # station number\n",
    "    n_input_vec = n_input * frequency # 317 * frequency\n",
    "    n_A_vec = n_input * n_input\n",
    "    n_output_vec = n_input * 1 # each row represent a result\n",
    "\n",
    "    #n_classes = 2 # MNIST total classes (0-9 digits) # n_classes is for classification only\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None, n_input_vec]) # X is the input signal\n",
    "    #X_weather = tf.placeholder(tf.float32, [None, 9 * frequency2]) # X_weather weather and holiday information (9 is the feature number)\n",
    "    A = tf.placeholder(tf.float32, [None, n_A_vec]) # A is the normalized adj matrix\n",
    "    oldA = tf.placeholder(tf.float32, [n_input, n_input])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_output_vec]) # y is the regression output\n",
    "\n",
    "    #Xtem = tf.placeholder(tf.float32, [n_input, frequency]) # for each row of X, A, Y, it can be reshaped to Xtem, Atem, Ytem\n",
    "    #Atem = tf.placeholder(tf.float32, [n_input, n_input]) # \n",
    "    #Ytem = tf.placeholder(tf.float32, [n_input, 1]) #\n",
    "\n",
    "    #Ypre = tf.placeholder(tf.float32, [None, n_output_vec])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([frequency, n_hidden_vec1])),\n",
    "        #'h2': tf.Variable(tf.random_normal([n_hidden_vec1, n_hidden_vec2])),\n",
    "        #'h3': tf.Variable(tf.random_normal([n_hidden_vec2, n_hidden_vec3])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_vec1, 1])), # dont forget to change n_hidden_vec1 when add/delete layers\n",
    "        #'f1': tf.Variable(tf.random_normal([272*n_hidden_vec3, 100])),\n",
    "        #'f2': tf.Variable(tf.random_normal([50, 10])),\n",
    "        #'f3': tf.Variable(tf.random_normal([100, 272])),\n",
    "        'A1': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'A2': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'A3': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'h1_wea': tf.Variable(tf.random_normal([9*frequency2, n_hidden_weather1])),\n",
    "        #'out_wea': tf.Variable(tf.random_normal([n_hidden_weather1, n_input]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_input,1])),# n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        #'b2': tf.Variable(tf.random_normal([n_input, 1])), #n_hidden_vec2])),\n",
    "        #'b3': tf.Variable(tf.random_normal([n_input, 1])),#n_hidden_vec3])),\n",
    "        #'b1': tf.Variable(tf.random_normal([n_input,n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        #'b2': tf.Variable(tf.random_normal([n_input,n_hidden_vec2])),\n",
    "        #'b3': tf.Variable(tf.random_normal([n_input,n_hidden_vec3])),\n",
    "        #'bf1': tf.Variable(tf.random_normal([1, 100])), \n",
    "        #'bf2': tf.Variable(tf.random_normal([1, 10])), \n",
    "        #'bf3': tf.Variable(tf.random_normal([1, 272])), \n",
    "        'bout': tf.Variable(tf.random_normal([n_input, 1])), \n",
    "        #'b1_wea': tf.Variable(tf.random_normal([1, n_hidden_weather1])), \n",
    "        #'bout_wea': tf.Variable(tf.random_normal([1, n_input])), \n",
    "    }\n",
    "\n",
    "    #gcn_no_A\n",
    "    #gcn\n",
    "    # Construct model\n",
    "    pred= gcn(X, weights, biases, batch_size,n_input, frequency, 1)\n",
    "    \n",
    "    #set  negative in pred to 0s\n",
    "    #Computes rectified linear: max(features, 0).\n",
    "    #pred = tf.nn.relu(pred)\n",
    "    # Define loss and optimizer\n",
    "    # RMS for regression \n",
    "    # L2 regularization\n",
    "    #cost = tf.reduce_mean(tf.pow(pred-Y, 2)) + reg1*tf.nn.l2_loss(weights['A1']) + reg2*tf.nn.l2_loss(weights['A2'])# + tf.nn.l2_loss(weights['A2']) + tf.nn.l2_loss(weights['A3'])))\n",
    "\n",
    "    # L1 regular tf.reduce_sum(tf.abs(parameters))\n",
    "    cost = tf.reduce_mean(tf.pow(pred-Y, 2)) + reg1*tf.reduce_sum(tf.abs(weights['A1']))# + reg2*tf.reduce_sum(tf.abs(weights['A2']))\n",
    "    pred_val= gcn(X, weights, biases, batch_size,n_input,frequency, 2)\n",
    "    #pred_val = tf.nn.relu(pred_val)\n",
    "    cost_val = tf.reduce_mean(tf.pow(pred_val-Y, 2))\n",
    "\n",
    "    pred_tes= gcn(X, weights, biases, batch_size,n_input,frequency, 3)\n",
    "    #pred_tes = tf.nn.relu(pred_tes)\n",
    "    cost_tes = tf.reduce_mean(tf.pow(pred_tes-Y, 2))\n",
    "    # cross-entropy for classification\n",
    "    # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y_train))\n",
    "    # ratio = tf.abs(tf.reduce_sum(pred)-tf.reduce_sum(Y))/tf.reduce_sum(Y)\n",
    "    #zero = 0\n",
    "    #ratio = tf.reduce_mean(tf.divide(tf.where(tf.not_equal(Y, zero), np.abs(pred-Y), tf.zeros(Y.get_shape(), tf.float32)), tf.where(tf.not_equal(Y, zero), Y, tf.ones(Y.get_shape(), tf.float32))))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay).minimize(cost)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #total_val_cost = []\n",
    "    #total_val_ratio = []\n",
    "\n",
    "    # learning start from \n",
    "\n",
    "    #index = daily_bike[(daily_bike['year'] == 2016) & (daily_bike['monthofyear'] == 1) & (daily_bike['dayofmonth'] == 1)].index.tolist()[0]\n",
    "    #A_hat = normalize_adj(corr_matrix_trips)\n",
    "    #print(A_hat)\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(training/batch_size)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                      Y: Y_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                              keep_prob: keep})\n",
    "                #print (\"Epoch:\", '%04d' % (epoch+1), \"batch: \", i, \"batch cost=\", \\\n",
    "                #    \"{:.9f}\".format(c))\n",
    "                #print (c)\n",
    "                avg_cost += c / total_batch \n",
    "                #Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Epoch:\", '%04d' % (epoch+1), \"Training RMSE=\", \\\n",
    "                    \"{:.9f}\".format((np.sqrt(avg_cost))))\n",
    "            # validation\n",
    "            c_val = sess.run([cost_val], feed_dict={X: X_val, Y: Y_val,  keep_prob:1})\n",
    "            print(\"Validation RMSE: \", (np.sqrt(c_val[0])))\n",
    "\n",
    "            c_tes, pred_tes1, A1= sess.run([cost_tes, pred_tes, weights['A1']], feed_dict={X: X_test,Y: Y_test, keep_prob: 1})\n",
    "            print(\"Test RMSE: \", (np.sqrt(c_tes)))\n",
    "\n",
    "            if c_val[0] < best_val:\n",
    "                best_val = c_val[0]\n",
    "                #saver.save(sess, './bikesharing_graph_2_th_point1')\n",
    "                test_error = c_tes\n",
    "                traing_error = np.sqrt(avg_cost)\n",
    "                early_stop_k = 0 # reset to 0\n",
    "                #print (pred_tes1)\n",
    "                predic_res = pred_tes1\n",
    "\n",
    "            # early stopping\n",
    "            if c_val[0] >= best_val:\n",
    "                early_stop_k += 1\n",
    "\n",
    "            # threshold\n",
    "            if early_stop_k == early_stop_th:\n",
    "              #  print (\"early stopping...\")\n",
    "                break\n",
    "            \n",
    "\n",
    "        print(\"epoch is \", epoch)\n",
    "        print(\"training error is \", traing_error)\n",
    "        print(\"Optimization Finished! the lowest validation RMSE is \", (np.sqrt(best_val)))\n",
    "        print(\"The test RMSE is \", (np.sqrt(test_error)))\n",
    "    \n",
    "    test_Y = Y_test\n",
    "    test_error = np.sqrt(test_error)\n",
    "    return -np.sqrt(best_val), predic_res,test_Y,test_error, A1#, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training RMSE= 14.504803800\n",
      "Validation RMSE:  7.2411513\n",
      "Test RMSE:  5.7073555\n",
      "Epoch: 0002 Training RMSE= 7.038956559\n",
      "Validation RMSE:  5.762159\n",
      "Test RMSE:  4.468809\n",
      "Epoch: 0003 Training RMSE= 6.173779911\n",
      "Validation RMSE:  5.146015\n",
      "Test RMSE:  3.9549868\n",
      "Epoch: 0004 Training RMSE= 5.643278755\n",
      "Validation RMSE:  4.782254\n",
      "Test RMSE:  3.64464\n",
      "Epoch: 0005 Training RMSE= 5.251906179\n",
      "Validation RMSE:  4.4922132\n",
      "Test RMSE:  3.4283123\n",
      "Epoch: 0006 Training RMSE= 4.971628227\n",
      "Validation RMSE:  4.315001\n",
      "Test RMSE:  3.2988946\n",
      "Epoch: 0007 Training RMSE= 4.781274176\n",
      "Validation RMSE:  4.1704297\n",
      "Test RMSE:  3.2050755\n",
      "Epoch: 0008 Training RMSE= 4.623632702\n",
      "Validation RMSE:  4.0570507\n",
      "Test RMSE:  3.13657\n",
      "Epoch: 0009 Training RMSE= 4.484992002\n",
      "Validation RMSE:  3.9763865\n",
      "Test RMSE:  3.08378\n",
      "Epoch: 0010 Training RMSE= 4.369435874\n",
      "Validation RMSE:  3.9036918\n",
      "Test RMSE:  3.040478\n",
      "Epoch: 0011 Training RMSE= 4.272511462\n",
      "Validation RMSE:  3.82395\n",
      "Test RMSE:  2.9990003\n",
      "Epoch: 0012 Training RMSE= 4.192393807\n",
      "Validation RMSE:  3.7796707\n",
      "Test RMSE:  2.9660969\n",
      "Epoch: 0013 Training RMSE= 4.122883423\n",
      "Validation RMSE:  3.7333944\n",
      "Test RMSE:  2.9339738\n",
      "Epoch: 0014 Training RMSE= 4.059311176\n",
      "Validation RMSE:  3.6897764\n",
      "Test RMSE:  2.9035785\n",
      "Epoch: 0015 Training RMSE= 3.990331692\n",
      "Validation RMSE:  3.6566544\n",
      "Test RMSE:  2.8749604\n",
      "Epoch: 0016 Training RMSE= 3.948869661\n",
      "Validation RMSE:  3.612439\n",
      "Test RMSE:  2.8461082\n",
      "Epoch: 0017 Training RMSE= 3.871332591\n",
      "Validation RMSE:  3.559977\n",
      "Test RMSE:  2.8229804\n",
      "Epoch: 0018 Training RMSE= 3.874975679\n",
      "Validation RMSE:  3.5630574\n",
      "Test RMSE:  2.8038871\n",
      "Epoch: 0019 Training RMSE= 3.821517924\n",
      "Validation RMSE:  3.5410736\n",
      "Test RMSE:  2.7851384\n",
      "Epoch: 0020 Training RMSE= 3.784379389\n",
      "Validation RMSE:  3.5339255\n",
      "Test RMSE:  2.764226\n",
      "Epoch: 0021 Training RMSE= 3.716151938\n",
      "Validation RMSE:  3.4730628\n",
      "Test RMSE:  2.749573\n",
      "Epoch: 0022 Training RMSE= 3.704030304\n",
      "Validation RMSE:  3.6363444\n",
      "Test RMSE:  2.819364\n",
      "Epoch: 0023 Training RMSE= 3.681425456\n",
      "Validation RMSE:  3.4969869\n",
      "Test RMSE:  2.7324476\n",
      "Epoch: 0024 Training RMSE= 3.627774812\n",
      "Validation RMSE:  3.5882347\n",
      "Test RMSE:  2.7886794\n",
      "Epoch: 0025 Training RMSE= 3.579635988\n",
      "Validation RMSE:  3.782673\n",
      "Test RMSE:  2.922307\n",
      "Epoch: 0026 Training RMSE= 3.618865963\n",
      "Validation RMSE:  3.393833\n",
      "Test RMSE:  2.6775851\n",
      "Epoch: 0027 Training RMSE= 3.556548511\n",
      "Validation RMSE:  3.3927038\n",
      "Test RMSE:  2.6652255\n",
      "Epoch: 0028 Training RMSE= 3.530617327\n",
      "Validation RMSE:  3.3783426\n",
      "Test RMSE:  2.6539545\n",
      "Epoch: 0029 Training RMSE= 3.506052496\n",
      "Validation RMSE:  3.3657186\n",
      "Test RMSE:  2.6444125\n",
      "Epoch: 0030 Training RMSE= 3.485252940\n",
      "Validation RMSE:  3.340107\n",
      "Test RMSE:  2.6359968\n",
      "Epoch: 0031 Training RMSE= 3.459338312\n",
      "Validation RMSE:  3.3308165\n",
      "Test RMSE:  2.6282098\n",
      "Epoch: 0032 Training RMSE= 3.436321816\n",
      "Validation RMSE:  3.323287\n",
      "Test RMSE:  2.6232898\n",
      "Epoch: 0033 Training RMSE= 3.377345481\n",
      "Validation RMSE:  3.4679022\n",
      "Test RMSE:  2.726189\n",
      "Epoch: 0034 Training RMSE= 3.357309708\n",
      "Validation RMSE:  3.603074\n",
      "Test RMSE:  2.8110878\n",
      "Epoch: 0035 Training RMSE= 3.344985451\n",
      "Validation RMSE:  3.5960646\n",
      "Test RMSE:  2.8085976\n",
      "Epoch: 0036 Training RMSE= 3.316971590\n",
      "Validation RMSE:  3.8161101\n",
      "Test RMSE:  2.9536593\n",
      "Epoch: 0037 Training RMSE= 3.307111673\n",
      "Validation RMSE:  3.5940092\n",
      "Test RMSE:  2.8082476\n",
      "Epoch: 0038 Training RMSE= 3.284417139\n",
      "Validation RMSE:  3.8451054\n",
      "Test RMSE:  2.9753594\n",
      "Epoch: 0039 Training RMSE= 3.272489974\n",
      "Validation RMSE:  3.5899725\n",
      "Test RMSE:  2.8072975\n",
      "Epoch: 0040 Training RMSE= 3.251473639\n",
      "Validation RMSE:  3.2861216\n",
      "Test RMSE:  2.6078227\n",
      "Epoch: 0041 Training RMSE= 3.240651814\n",
      "Validation RMSE:  3.2240171\n",
      "Test RMSE:  2.5642338\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-612732e1d8a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mval_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredic_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgcn_corr_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;31m#val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"finished A running: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-90c89f8ac439>\u001b[0m in \u001b[0;36mgcn_corr_final\u001b[0;34m(frequency, learning_rate, decay, batch_size, n_hidden_vec1, n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\u001b[0m\n\u001b[1;32m    151\u001b[0m                 _, c = sess.run([optimizer, cost], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n\u001b[1;32m    152\u001b[0m                                                       \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                                                               keep_prob: keep})\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0;31m#print (\"Epoch:\", '%04d' % (epoch+1), \"batch: \", i, \"batch cost=\", \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;31m#    \"{:.9f}\".format(c))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set size\n",
    "#from bayes_opt import BayesianOptimization\n",
    "import datetime\n",
    "\n",
    "All_pred = np.empty([2000, 272])\n",
    "All_Y = np.empty([2000, 272])\n",
    "\n",
    "training = 20000\n",
    "validation = 2000\n",
    "test = 2000 #24*90\n",
    "skip1 = 20000 + 2000 - training - validation # totally to make the testing dataset the same\n",
    "freq_max = 24\n",
    "#gcn_corr_eval(7, 0.01, 0.5, 100, 0.4, 10, 5, 5, 0.2, 50, 500)\n",
    "\n",
    "step = 0\n",
    "gap = 2000\n",
    "\n",
    "total_sn = 0\n",
    "num_iter = 50\n",
    "init_points = 200\n",
    "rep = 1000\n",
    "\n",
    "# stdbscan\n",
    "spatial_threshold = 300\n",
    "temporal_threshold = 300\n",
    "min_neighbors = 1 # number of neighbor\n",
    "\n",
    "frequency2 = skip1 + freq_max + training\n",
    "\n",
    "while step < 2000:\n",
    "    \n",
    "    #hourly_bike_gps_tem_index = hourly_bike_gps['index1']\n",
    "    #hourly_bike_gps_tem = hourly_bike_gps.iloc[:, np.r_[skip1:frequency2, 26304:26306]] # past frequency2 steps and GPS locations\n",
    "    # 0 index; 401:403 gps locations; ((i - frequency) + 1):(i + 1), temporal features\n",
    "    #print (daily_bike_gps_tem)\n",
    "    #df, cluster_num, station_in_cluster = ST_DBSCAN(hourly_bike_gps_tem, spatial_threshold, temporal_threshold, min_neighbors)\n",
    "\n",
    "    #for c in range(cluster_num):\n",
    "    #c1 = df[df['cluster'] == (c+1)].index.tolist()\n",
    "    #sn = len(c1)\n",
    "    #print (\"this cluster size is: \", sn)\n",
    "    #hourly_bike_cluster = hourly_bike[c1]\n",
    "    #hourly_bike_cluster.shape\n",
    "\n",
    "    sn = 272\n",
    "    hourly_bike_cluster = hourly_bike\n",
    "    '''\n",
    "    gcnBO = BayesianOptimization(gcn_corr_eval, \n",
    "                                 {'frequency': (24, 24),\n",
    "                                  'learning_rate': (0.01, 0.05),\n",
    "                                    'decay': (0.3, 0.7),\n",
    "                                    'batch_size': (80, 120),\n",
    "                                    'n_hidden_vec1': (3, 10),\n",
    "                                    'n_hidden_vec2': (3, 10),\n",
    "                                    'n_hidden_vec3': (5, 5),\n",
    "                                    'keep': (0.8, 1),\n",
    "                                    'early_stop_th': (30, 70),\n",
    "                                    'training_epochs': (500, 500),\n",
    "                                    'reg': (0, 10)\n",
    "                                    })\n",
    "    gcnBO.maximize(init_points=init_points, n_iter=num_iter)\n",
    "    a=gcnBO.res['max']['max_params']\n",
    "    '''\n",
    "    best = -10000\n",
    "    pre_best = []\n",
    "    test_Y_best = []\n",
    "    test_error_best = 1000\n",
    "    A1_best = []\n",
    "   # A2_best = []\n",
    "    for i in range(rep):\n",
    "        a = datetime.datetime.now()\n",
    "        val_error, predic_res, test_Y,test_error, A1=gcn_corr_final(24, 0.005, 0.9, 100, 40, 40, 5, 1, 20, 500, 0, 0)\n",
    "        #val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\n",
    "        print (\"finished A running: \", i)\n",
    "        b = datetime.datetime.now()\n",
    "        print(b-a)\n",
    "        if val_error > best:\n",
    "            best = val_error\n",
    "            pre_best = predic_res\n",
    "            test_Y_best = test_Y\n",
    "            test_error_best = test_error\n",
    "            A1_best = A1\n",
    "           # A2_best = A2\n",
    "\n",
    "    #val_error, predic_res, test_Y,test_error=gcn_corr_final(24, 0.02, 0.2, 100, 5, 10, 10, 0.8, 50, 500)\n",
    "    All_pred[step:(step+gap), total_sn:(total_sn+sn)] = pre_best\n",
    "    All_Y[step:(step+gap), total_sn:(total_sn+sn)]  = test_Y_best\n",
    "\n",
    "    total_sn = total_sn + sn\n",
    "\n",
    "    total_error = np.sqrt(np.mean((All_pred[0:(step+gap),0:total_sn] - All_Y[0:(step+gap),0:total_sn])**2))\n",
    "\n",
    "    #print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the cluster now is: \", c)\n",
    "    print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the val error of this cluster now is: \", best)\n",
    "    print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the test error by this cluster now is: \", total_error)\n",
    "    '''\n",
    "    c1 = df[df['cluster'] == -999999].index.tolist()\n",
    "    sn = len(c1)\n",
    "    print (\"this cluster size is: \", sn)\n",
    "    hourly_bike_cluster = hourly_bike[c1]\n",
    "    #hourly_bike_cluster.shape\n",
    "\n",
    "    #sn = 272\n",
    "    #hourly_bike_cluster = hourly_bike\n",
    "    \n",
    "    if sn != 0:\n",
    "        \n",
    "        gcnBO = BayesianOptimization(gcn_corr_eval, \n",
    "                                     {'frequency': (10, 24),\n",
    "                                      'learning_rate': (0.01, 0.05),\n",
    "                                        'decay': (0.1, 1),\n",
    "                                        'batch_size': (50, 200),\n",
    "                                        'n_hidden_vec1': (3, 20),\n",
    "                                        'n_hidden_vec2': (5, 5),\n",
    "                                        'n_hidden_vec3': (5, 5),\n",
    "                                        'keep': (0.001, 1),\n",
    "                                        'early_stop_th': (50, 500),\n",
    "                                        'training_epochs': (200, 1000),\n",
    "                                        'reg': (0, 100)\n",
    "                                        })\n",
    "        gcnBO.maximize(init_points=init_points, n_iter=num_iter)\n",
    "        a=gcnBO.res['max']['max_params']\n",
    "        \n",
    "        best = -10000\n",
    "        pre_best = []\n",
    "        test_Y_best = []\n",
    "        test_error_best = 1000\n",
    "        for i in range(rep):\n",
    "            #val_error, predic_res, test_Y,test_error=gcn_corr_final(24, 0.05, 0.5, 100, 5, 5, 5, 0.9, 50, 500, 0.5)\n",
    "            val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\n",
    "            print (\"finished A running: \", i)\n",
    "            if val_error > best:\n",
    "                best = val_error\n",
    "                pre_best = predic_res\n",
    "                test_Y_best = test_Y\n",
    "                test_error_best = test_error\n",
    "\n",
    "        #val_error, predic_res, test_Y,test_error=gcn_corr_final(24, 0.02, 0.2, 100, 5, 10, 10, 0.8, 50, 500)\n",
    "        All_pred[step:(step+gap), total_sn:(total_sn+sn)] = pre_best\n",
    "        All_Y[step:(step+gap), total_sn:(total_sn+sn)]  = test_Y_best\n",
    "\n",
    "        total_sn = total_sn + sn\n",
    "\n",
    "        total_error = np.sqrt(np.mean((All_pred[0:(step+gap),0:total_sn] - All_Y[0:(step+gap),0:total_sn])**2))\n",
    "\n",
    "        print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the cluster now is: \", -999999)\n",
    "        print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the val error of this cluster now is: \", best)\n",
    "        print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the test error by this cluster now is: \", total_error)\n",
    "    '''\n",
    "    step = step + gap\n",
    "    skip1 = skip1 + gap\n",
    "    \n",
    "    #np.savetxt(\"prediction_300_300.csv\", All_pred, delimiter = ',')\n",
    "    #np.savetxt(\"prediction_Y_300_300.csv\", All_Y, delimiter = ',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
