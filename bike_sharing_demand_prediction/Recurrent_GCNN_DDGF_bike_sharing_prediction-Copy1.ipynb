{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "import collections\n",
    "from tensorflow.contrib import rnn\n",
    "import h5py\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from utils import normalize_adj, StandardScaler, masked_mae_tf, masked_mae_tf_by_horizon\n",
    "from gcn import gcn, gcnn_ddgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"data/NYCBikeHourly272.pickle\"\n",
    "fileObject = open(file_Name,'rb') \n",
    "hourly_bike = pickle.load(fileObject) \n",
    "hourly_bike = pd.DataFrame(hourly_bike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_num = 272 # node number \n",
    "feature_in = 24 # number of features at each node, e.g., bike sharing demand from past 24 hours\n",
    "horizon = 1 # the length to predict, e.g., predict the future one hour bike sharing demand\n",
    "#lstm_steps = 10 # number of cells in the LSTM part\n",
    "\n",
    "X_whole = []\n",
    "Y_whole = []\n",
    "\n",
    "x_offsets = np.sort(\n",
    "    np.concatenate((np.arange(-feature_in+1, 1, 1),))\n",
    ")\n",
    "# Predict the next one hour\n",
    "y_offsets = np.sort(np.arange(1, 1+ horizon, 1))\n",
    "\n",
    "min_t = abs(min(x_offsets))\n",
    "max_t = abs(hourly_bike.shape[0] - abs(max(y_offsets)))  # Exclusive\n",
    "for t in range(min_t, max_t):\n",
    "    x_t = hourly_bike.iloc[t + x_offsets, 0:node_num].values.flatten('F')\n",
    "    y_t = hourly_bike.iloc[t + y_offsets, 0:node_num].values.flatten('F')\n",
    "    X_whole.append(x_t)\n",
    "    Y_whole.append(y_t)\n",
    "\n",
    "X_whole = np.stack(X_whole, axis=0)\n",
    "Y_whole = np.stack(Y_whole, axis=0)\n",
    "\n",
    "X_whole = np.reshape(X_whole, [X_whole.shape[0], node_num, feature_in])\n",
    "\n",
    "num_train = 20000 # Note here actually we use the first 20000 to train the model. The paper mentioned \"22304\" need to be corrected.\n",
    "num_val = 2000\n",
    "num_test = 2000\n",
    "\n",
    "X_training = X_whole[:num_train, :]\n",
    "Y_training = Y_whole[:num_train, :]\n",
    "\n",
    "# shuffle the training dataset\n",
    "perm = np.arange(X_training.shape[0])\n",
    "np.random.shuffle(perm)\n",
    "X_training = X_training[perm]\n",
    "Y_training = Y_training[perm]\n",
    "\n",
    "X_val = X_whole[num_train:num_train+num_val, :]\n",
    "Y_val = Y_whole[num_train:num_train+num_val, :]\n",
    "\n",
    "X_test = X_whole[num_train+num_val:num_train+num_val+num_test, :]\n",
    "Y_test = Y_whole[num_train+num_val:num_train+num_val+num_test, :]\n",
    "\n",
    "scaler = StandardScaler(mean=X_training.mean(), std=X_training.std())\n",
    "\n",
    "X_training = scaler.transform(X_training)\n",
    "Y_training = scaler.transform(Y_training)\n",
    "\n",
    "X_val = scaler.transform(X_val)\n",
    "Y_val = scaler.transform(Y_val)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "Y_test = scaler.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def gcn(x, weights, biases, batch_size, n_input, frequency,flag, n_output_vec):\n",
    "    # Hidden layer with RELU activation\n",
    "    \n",
    "   # output_list = tf.Variable(tf.zeros([n_output_vec,1]),dtype=tf.float32) #'Tensor' object does not support item assignment, cant build Ypre\n",
    "\n",
    "    #Xtem = tf.reshape(x[i,:], [n_input, frequency])\n",
    "    #Xtem = tf.transpose(Xtem)\n",
    "    #Atem = tf.convert_to_tensor(A_whole_final, dtype=np.float32)\n",
    "    #Atem1 = tf.diag(tf.ones([n_input])) # Atem not palying any roles\n",
    "    #Ytem = tf.reshape(Y[i,:], [n_input, 1])\n",
    "    #Atem = tf.diag(tf.ones([n_input]))\n",
    "    #x = tf.reshape(x, [-1, sn, 1]) # 100, 207, 1\n",
    "    \n",
    "    # x (?, 207, 12)\n",
    "    x = tf.transpose(x, [1, 0]) # 207, ?, 12\n",
    "    #x = tf.reshape(x, [node_num, -1]) # 207, batch*feature_num\n",
    "    Atem1 = 0.5*(weights['A1'] + tf.transpose(weights['A1']))#+ Atem \n",
    "    Atem1 = normalize_adj(Atem1)\n",
    "    #th = tf.constant(0.01, dtype=tf.float32)\n",
    "    #where = tf.subtract(Atem1, th)\n",
    "    #Atem1 = tf.nn.relu(where)\n",
    "\n",
    "    Z1 = tf.matmul(Atem1, x) #+ biases['b1'] # 207, batch*feature_num  #+ tf.matmul( tf.matmul(weights['A1'], weights['A1']), Xtem)\n",
    "    Z1 = tf.nn.relu(Z1) # 207*100, hidden1\n",
    "\n",
    "    \n",
    "    Atem2 = 0.5*(weights['A2'] + tf.transpose(weights['A2']))#+ Atem \n",
    "    Atem2 = normalize_adj(Atem2)\n",
    "    \n",
    "    Z2 = tf.matmul(Atem2, x) #+ biases['b2'] \n",
    "    Z2 = tf.nn.relu(Z2) # 207*batchsize, hidden2\n",
    "\n",
    "    Atem3 = 0.5*(weights['A3'] + tf.transpose(weights['A3']))#+ Atem \n",
    "    Atem3 = normalize_adj(Atem3)# + biases['b3'] \n",
    "    \n",
    "    Z3 = tf.matmul(Atem3, x)\n",
    "    Z3 = tf.nn.relu(Z3)\n",
    "    \n",
    "    layer_3 = tf.add(tf.add(Z1, Z2), Z3)#tf.nn.relu(Z3) # 207*batchsize, hidden3\n",
    "    \n",
    "    \n",
    "    layer_3 = tf.reshape(layer_3, [-1, sn]) # batchsize, sn*hidden3\n",
    "    Z4 = layer_3\n",
    "\n",
    "    \n",
    "    return Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_corr_final(frequency, horizon, learning_rate, decay,batch_size, n_hidden_vec1,n_hidden_vec2,n_hidden_vec3,keep, early_stop_th,training_epochs, reg1, reg2):\n",
    "    # set size\n",
    "    #sn = 3 # station number\n",
    "\n",
    "    n_output_vec = node_num*horizon\n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    predic_res = []\n",
    "    Y_true = []\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    \n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "    n_hidden_vec1 = int(n_hidden_vec1)\n",
    "    n_hidden_vec2 = int(n_hidden_vec2)\n",
    "    n_hidden_vec3 = int(n_hidden_vec3)\n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    # Network Parameters\n",
    "\n",
    "    #n_classes = 2 # MNIST total classes (0-9 digits) # n_classes is for classification only\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None, sn, feature_in]) # X is the input signal\n",
    "    Y = tf.placeholder(tf.float32, [None, n_output_vec]) # y is the regression output\n",
    "    #num = tf.placeholder(tf.int32,[1, 1] )\n",
    "\n",
    "    #Xtem = tf.placeholder(tf.float32, [n_input, frequency]) # for each row of X, A, Y, it can be reshaped to Xtem, Atem, Ytem\n",
    "    #Atem = tf.placeholder(tf.float32, [n_input, n_input]) # \n",
    "    #Ytem = tf.placeholder(tf.float32, [n_input, 1]) #\n",
    "\n",
    "    #Ypre = tf.placeholder(tf.float32, [None, n_output_vec])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([frequency, n_hidden_vec1]), dtype=np.float32),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_vec1, n_hidden_vec2]), dtype=np.float32),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_vec2, n_hidden_vec3]), dtype=np.float32),\n",
    "        #'h4': tf.Variable(tf.random_normal([n_hidden_vec3, n_hidden_vec4])),\n",
    "        #'outg': tf.Variable(tf.random_normal([sn*n_hidden_vec3, n_hidden_vec4]), dtype=np.float32), \n",
    "        'out': tf.Variable(tf.random_normal([num_hidden, Y_whole.shape[1]]), dtype=np.float32), # dont forget to change n_hidden_vec1 when add/delete layers\n",
    "        #'f1': tf.Variable(tf.random_normal([272*n_hidden_vec3, 100])),\n",
    "        #'f2': tf.Variable(tf.random_normal([50, 10])),\n",
    "        #'f3': tf.Variable(tf.random_normal([100, 272])),\n",
    "        'A1': tf.Variable(tf.random_normal([node_num,node_num]), dtype=np.float32),\n",
    "        'A2': tf.Variable(tf.random_normal([node_num,node_num]), dtype=np.float32),\n",
    "        'A3': tf.Variable(tf.random_normal([node_num,node_num]), dtype=np.float32),\n",
    "        #'A4': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'h1_wea': tf.Variable(tf.random_normal([9*frequency2, n_hidden_weather1])),\n",
    "        #'out_wea': tf.Variable(tf.random_normal([n_hidden_weather1, n_input]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([1, horizon]), dtype=np.float32),# n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        'b2': tf.Variable(tf.random_normal([1, horizon]), dtype=np.float32), #n_hidden_vec2])),\n",
    "        'b3': tf.Variable(tf.random_normal([1, horizon]), dtype=np.float32),#n_hidden_vec3])),\n",
    "        #'b4': tf.Variable(tf.random_normal([n_input, n_hidden_vec4])),\n",
    "        #'b1': tf.Variable(tf.random_normal([n_input,n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        #'b2': tf.Variable(tf.random_normal([n_input,n_hidden_vec2])),\n",
    "        #'b3': tf.Variable(tf.random_normal([n_input,n_hidden_vec3])),\n",
    "        #'bf1': tf.Variable(tf.random_normal([1, 100])), \n",
    "        #'bf2': tf.Variable(tf.random_normal([1, 10])), \n",
    "        #'bf3': tf.Variable(tf.random_normal([1, 272])), \n",
    "        #'boutg': tf.Variable(tf.random_normal([1, n_hidden_vec4]), dtype=np.float32), \n",
    "        'bout': tf.Variable(tf.random_normal([Y_whole.shape[1]]), dtype=np.float32), \n",
    "        #'b1_wea': tf.Variable(tf.random_normal([1, n_hidden_weather1])), \n",
    "        #'bout_wea': tf.Variable(tf.random_normal([1, n_input])), \n",
    "    }\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope('lstm'):\n",
    "        lstm = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(num_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "        rnn_input_seq = tf.unstack(X, feature_in, 2) #\n",
    "        \n",
    "        #print (len(rnn_input_seq))\n",
    "        #Atem = weights['A1']\n",
    "        for i in range(feature_in):\n",
    "            rnn_input_seq[i] = gcn(rnn_input_seq[i], weights, biases, batch_size,node_num, frequency, 1, n_output_vec)\n",
    "            #print (rnn_input_seq[i].shape)\n",
    "        outputs, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs[-1], [-1, num_hidden])\n",
    "        #print ('123here!!!!!!!!!!!')\n",
    "        pred = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        #print (pred)\n",
    "        #pred = tf.reshape(pred, [-1, Y_whole.shape[1]])\n",
    "        #print ('here!!!!!!!!!!!')\n",
    "        pred = scaler.inverse_transform(pred)\n",
    "        Y_true_tr = scaler.inverse_transform(Y)\n",
    "\n",
    "        cost = tf.reduce_mean(tf.pow(pred - Y_true_tr, 2)) \n",
    "        #print (cost)\n",
    "        \n",
    "    with tf.variable_scope('lstm', reuse=True):\n",
    "        rnn_input_seq_val = tf.unstack(X, feature_in, 2)\n",
    "        #Atem = weights['A1']\n",
    "        for i in range(feature_in):\n",
    "            rnn_input_seq_val[i] = gcn(rnn_input_seq_val[i], weights, biases, batch_size,node_num,frequency, 2, n_output_vec)\n",
    "        outputs_val, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq_val, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs_val[-1], [-1, num_hidden])\n",
    "\n",
    "        pred_val = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        pred_val = scaler.inverse_transform(pred_val)\n",
    "        Y_true_val = scaler.inverse_transform(Y)\n",
    "        \n",
    "        cost_val =  tf.reduce_mean(tf.pow(pred_val - Y_true_val, 2)) \n",
    "        #print ('234here!!!!!!!!!!!')\n",
    "    with tf.variable_scope('lstm', reuse=True):\n",
    "        rnn_input_seq_test = tf.unstack(X, feature_in, 2)\n",
    "        #Atem = weights['A1']\n",
    "        for i in range(feature_in):\n",
    "            rnn_input_seq_test[i] = gcn(rnn_input_seq_test[i], weights, biases, batch_size,node_num,frequency, 3, n_output_vec)\n",
    "        outputs_test, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq_test, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs_test[-1], [-1, num_hidden])\n",
    "\n",
    "        pred_tes = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        pred_tes = scaler.inverse_transform(pred_tes)\n",
    "        Y_true_tes = scaler.inverse_transform(Y)\n",
    "        \n",
    "        cost_tes = tf.reduce_mean(tf.pow(pred_tes - Y_true_tes, 2)) \n",
    "        \n",
    "        #print ('345here!!!!!!!!!!!')\n",
    "    #rmse\n",
    "    #cost_tes = tf.reduce_mean(tf.pow(pred_tes-Y, 2))\n",
    "    # cross-entropy for classification\n",
    "    # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y_train))\n",
    "    # ratio = tf.abs(tf.reduce_sum(pred)-tf.reduce_sum(Y))/tf.reduce_sum(Y)\n",
    "    #zero = 0\n",
    "    #ratio = tf.reduce_mean(tf.divide(tf.where(tf.not_equal(Y, zero), np.abs(pred-Y), tf.zeros(Y.get_shape(), tf.float32)), tf.where(tf.not_equal(Y, zero), Y, tf.ones(Y.get_shape(), tf.float32))))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay).minimize(cost)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #total_val_cost = []\n",
    "    #total_val_ratio = []\n",
    "\n",
    "    # learning start from \n",
    "\n",
    "    #index = daily_bike[(daily_bike['year'] == 2016) & (daily_bike['monthofyear'] == 1) & (daily_bike['dayofmonth'] == 1)].index.tolist()[0]\n",
    "    #A_hat = normalize_adj(corr_matrix_trips)\n",
    "    #print(A_hat)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(num_train/batch_size) #int(num_train/batch_size)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                #print (Y_training[i*batch_size:(i+1)*batch_size,].size())\n",
    "                #num = batch_size\n",
    "                _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                      Y: Y_training[i*batch_size:(i+1)*batch_size,],  \n",
    "                                                              keep_prob: keep})\n",
    "                #print (preds)\n",
    "                #print (trueval)\n",
    "                #print (\"Epoch:\", '%04d' % (epoch+1), \"batch: \", i, \"batch cost=\", \\\n",
    "                #    \"{:.9f}\".format(c))\n",
    "                #print ('here!!!!!!!!!!!!!!!!')\n",
    "                avg_cost += c * batch_size #/ total_batch \n",
    "                #Display logs per epoch step\n",
    "                \n",
    "            # rest part of training dataset\n",
    "            #num = num_train - total_batch*batch_size \n",
    "            if total_batch * batch_size != num_train:\n",
    "                _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[total_batch*batch_size:num_train,], \n",
    "                                          Y: Y_training[total_batch*batch_size:num_train,],\n",
    "                                                  keep_prob: keep})\n",
    "                avg_cost += c * (num_train - total_batch*batch_size)\n",
    "            \n",
    "            avg_cost = np.sqrt(avg_cost / num_train)\n",
    "            \n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Epoch:\", '%04d' % (epoch+1), \"Training RMSE=\", \\\n",
    "                    \"{:.9f}\".format(avg_cost)) #np.sqrt(avg_cost)\n",
    "                \n",
    "            # also use batch to save memory\n",
    "            # validation\n",
    "            c_val = 0.\n",
    "            total_bat_val = int(num_val/batch_size)\n",
    "            for i in range(total_bat_val):\n",
    "                #num = batch_size\n",
    "                c_val_b = sess.run([cost_val], feed_dict={X: X_val[i*batch_size:(i+1)*batch_size,], \n",
    "                                                          Y: Y_val[i*batch_size:(i+1)*batch_size,],   keep_prob:1})\n",
    "                c_val += c_val_b[0]*batch_size\n",
    "            \n",
    "            if total_bat_val * batch_size != num_val:\n",
    "                #num = num_val - total_bat_val*batch_size\n",
    "                c_val_b = sess.run([cost_val], feed_dict={X: X_val[total_bat_val*batch_size:num_val,], \n",
    "                                                          Y: Y_val[total_bat_val*batch_size:num_val,],  keep_prob:1})\n",
    "                c_val += c_val_b[0] * (num_val - total_bat_val*batch_size)\n",
    "                \n",
    "            c_val = np.sqrt(c_val / num_val)\n",
    "            \n",
    "            print(\"Validation RMSE: \", c_val)\n",
    "            \n",
    "            # test\n",
    "            c_tes = 0.\n",
    "            total_bat_test = int(num_test/batch_size)\n",
    "            \n",
    "            pre_test_tem = [] # save the prediction results\n",
    "            Y_tes_true = []\n",
    "            \n",
    "            for i in range(total_bat_test):\n",
    "                #num = batch_size\n",
    "                c_tes_b, pred_tes1, Y_tes_batch = sess.run([cost_tes, pred_tes, Y_true_tes], feed_dict={X: X_test[i*batch_size:(i+1)*batch_size,],\n",
    "                                                                               Y: Y_test[i*batch_size:(i+1)*batch_size,],  keep_prob: 1})\n",
    "                c_tes += c_tes_b*batch_size\n",
    "\n",
    "                #print (cost_h)\n",
    "                #print (pred_tes1.shape)\n",
    "                pre_test_tem.append(pred_tes1)\n",
    "                Y_tes_true.append(Y_tes_batch)\n",
    "                \n",
    "            if total_bat_test * batch_size != num_test:\n",
    "                #num = num_test - total_bat_test*batch_size\n",
    "                c_tes_b, pred_tes1, Y_tes_batch = sess.run([cost_tes, pred_tes, Y_true_tes], feed_dict={X: X_test[total_bat_test*batch_size:num_test,],\n",
    "                                                                               Y: Y_test[total_bat_test*batch_size:num_test,],  keep_prob: 1})\n",
    "                c_tes += c_tes_b * (num_test - total_bat_test*batch_size) \n",
    "                 \n",
    "                \n",
    "                #print (pred_tes1.shape)\n",
    "                pre_test_tem.append(pred_tes1)\n",
    "                Y_tes_true.append(Y_tes_batch)\n",
    "            \n",
    "            #print (c_tes_h.shape)\n",
    "            pre_test_tem = np.concatenate(pre_test_tem, axis = 0)\n",
    "            #print (pre_test_tem.shape)\n",
    "            Y_tes_true = np.concatenate(Y_tes_true, axis = 0)\n",
    "            \n",
    "            c_tes = np.sqrt(c_tes / num_test)\n",
    "            #c_tes_h = c_tes_h / num_test\n",
    "            \n",
    "            print(\"Test RMSE: \", c_tes)\n",
    "            #print(\"predic step: \", cost_by_hor)\n",
    "\n",
    "            if c_val < best_val:\n",
    "                best_val = c_val\n",
    "                #saver.save(sess, './bikesharing_graph_2_th_point1')\n",
    "                test_error = c_tes\n",
    "                \n",
    "                traing_error = avg_cost#np.sqrt(avg_cost)\n",
    "                early_stop_k = 0 # reset to 0\n",
    "                #print (pred_tes1)\n",
    "                predic_res = pre_test_tem\n",
    "                Y_true = Y_tes_true\n",
    "                #predic_step = cost_by_hor\n",
    "\n",
    "            # early stopping\n",
    "            if c_val >= best_val:\n",
    "                early_stop_k += 1\n",
    "\n",
    "            # threshold\n",
    "            if early_stop_k == early_stop_th:\n",
    "              #  print (\"early stopping...\")\n",
    "                break\n",
    "            \n",
    "\n",
    "        print(\"epoch is \", epoch)\n",
    "        print(\"training RMSE is \", traing_error)\n",
    "        print(\"Optimization Finished! the lowest validation RMSE is \", best_val)#(np.sqrt(best_val)))\n",
    "        print(\"The test RMSE is \", test_error)#(np.sqrt(test_error)))\n",
    "    \n",
    "    #test_Y = Y_test\n",
    "    #test_error = np.sqrt(test_error)\n",
    "    return test_error, predic_res, Y_true#, A1#, predic_step#, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training RMSE= 7.860712308\n",
      "Validation RMSE:  7.442989599431542\n",
      "Test RMSE:  6.711288780459872\n",
      "Epoch: 0002 Training RMSE= 6.860021080\n",
      "Validation RMSE:  7.0747928818219625\n",
      "Test RMSE:  6.368018123064179\n",
      "Epoch: 0003 Training RMSE= 6.456604101\n",
      "Validation RMSE:  6.6913539441275125\n",
      "Test RMSE:  5.958595145637866\n",
      "Epoch: 0004 Training RMSE= 6.083514651\n",
      "Validation RMSE:  6.345785481536982\n",
      "Test RMSE:  5.612624478393457\n",
      "Epoch: 0005 Training RMSE= 5.786654315\n",
      "Validation RMSE:  5.98587447620931\n",
      "Test RMSE:  5.060063564670252\n",
      "Epoch: 0006 Training RMSE= 5.559089610\n",
      "Validation RMSE:  5.743988045459516\n",
      "Test RMSE:  4.754923427217946\n",
      "Epoch: 0007 Training RMSE= 5.413899790\n",
      "Validation RMSE:  5.6455235226169584\n",
      "Test RMSE:  4.718099385054429\n",
      "Epoch: 0008 Training RMSE= 5.277343000\n",
      "Validation RMSE:  5.47359761780612\n",
      "Test RMSE:  4.408363322472679\n",
      "Epoch: 0009 Training RMSE= 5.197372611\n",
      "Validation RMSE:  5.460581493643169\n",
      "Test RMSE:  4.527545019354608\n",
      "Epoch: 0010 Training RMSE= 5.143364739\n",
      "Validation RMSE:  5.4016841063688235\n",
      "Test RMSE:  4.39903762002803\n",
      "Epoch: 0011 Training RMSE= 5.117806993\n",
      "Validation RMSE:  5.366580312440869\n",
      "Test RMSE:  4.3390149991351405\n",
      "Epoch: 0012 Training RMSE= 5.096135729\n",
      "Validation RMSE:  5.392648422186455\n",
      "Test RMSE:  4.4621910565285505\n",
      "Epoch: 0013 Training RMSE= 5.088509318\n",
      "Validation RMSE:  5.325865772769083\n",
      "Test RMSE:  4.272598881701681\n",
      "Epoch: 0014 Training RMSE= 5.085285751\n",
      "Validation RMSE:  5.331922592900438\n",
      "Test RMSE:  4.297890615946742\n",
      "Epoch: 0015 Training RMSE= 5.083410804\n",
      "Validation RMSE:  5.325613464693959\n",
      "Test RMSE:  4.255874780816971\n",
      "Epoch: 0016 Training RMSE= 5.090345984\n",
      "Validation RMSE:  5.3121309937618495\n",
      "Test RMSE:  4.218050354763276\n",
      "Epoch: 0017 Training RMSE= 5.083405542\n",
      "Validation RMSE:  5.308986624017716\n",
      "Test RMSE:  4.212664418167147\n",
      "Epoch: 0018 Training RMSE= 5.081795464\n",
      "Validation RMSE:  5.348015315482579\n",
      "Test RMSE:  4.299526880539808\n",
      "Epoch: 0019 Training RMSE= 5.085150395\n",
      "Validation RMSE:  5.347899047600622\n",
      "Test RMSE:  4.370249539720668\n",
      "Epoch: 0020 Training RMSE= 5.077887493\n",
      "Validation RMSE:  5.411517889767792\n",
      "Test RMSE:  4.470257911495497\n",
      "Epoch: 0021 Training RMSE= 5.075696873\n",
      "Validation RMSE:  5.542430117609201\n",
      "Test RMSE:  4.754580247003442\n",
      "Epoch: 0022 Training RMSE= 5.069324377\n",
      "Validation RMSE:  5.396184382071738\n",
      "Test RMSE:  4.4520568637421025\n",
      "Epoch: 0023 Training RMSE= 5.055717031\n",
      "Validation RMSE:  5.4914164754848604\n",
      "Test RMSE:  4.684677596749411\n",
      "Epoch: 0024 Training RMSE= 5.041335949\n",
      "Validation RMSE:  5.468741106298571\n",
      "Test RMSE:  4.7620800403852055\n",
      "Epoch: 0025 Training RMSE= 5.026337323\n",
      "Validation RMSE:  5.55387535542733\n",
      "Test RMSE:  4.821451754236748\n",
      "Epoch: 0026 Training RMSE= 5.027217486\n",
      "Validation RMSE:  5.455972534675491\n",
      "Test RMSE:  4.635069016124865\n",
      "Epoch: 0027 Training RMSE= 4.996907813\n",
      "Validation RMSE:  5.603136253877519\n",
      "Test RMSE:  4.900921205613823\n",
      "Epoch: 0028 Training RMSE= 5.025402941\n",
      "Validation RMSE:  5.716456219031294\n",
      "Test RMSE:  5.148081649962695\n",
      "Epoch: 0029 Training RMSE= 4.972268314\n",
      "Validation RMSE:  5.931146374391137\n",
      "Test RMSE:  5.5518321922238245\n",
      "Epoch: 0030 Training RMSE= 4.953819780\n",
      "Validation RMSE:  5.815442314742798\n",
      "Test RMSE:  5.517606816850178\n",
      "Epoch: 0031 Training RMSE= 4.961132088\n",
      "Validation RMSE:  5.68135300344628\n",
      "Test RMSE:  5.212880380048007\n",
      "Epoch: 0032 Training RMSE= 4.911318383\n",
      "Validation RMSE:  5.74427493812921\n",
      "Test RMSE:  5.296632744304588\n",
      "Epoch: 0033 Training RMSE= 4.907922564\n",
      "Validation RMSE:  5.711502903034975\n",
      "Test RMSE:  5.14141134723526\n",
      "Epoch: 0034 Training RMSE= 4.881813120\n",
      "Validation RMSE:  5.63822103824737\n",
      "Test RMSE:  5.324871512998586\n",
      "Epoch: 0035 Training RMSE= 4.869517652\n",
      "Validation RMSE:  5.580988899437275\n",
      "Test RMSE:  4.918059501992297\n",
      "Epoch: 0036 Training RMSE= 4.848251269\n",
      "Validation RMSE:  5.533697675100404\n",
      "Test RMSE:  4.75482244133989\n",
      "Epoch: 0037 Training RMSE= 4.837267085\n",
      "Validation RMSE:  5.5305121840296305\n",
      "Test RMSE:  4.585342290616824\n",
      "Epoch: 0038 Training RMSE= 4.812208132\n",
      "Validation RMSE:  5.482711844119606\n",
      "Test RMSE:  4.607572351757981\n",
      "Epoch: 0039 Training RMSE= 4.755175632\n",
      "Validation RMSE:  5.526460287169724\n",
      "Test RMSE:  4.767797856500594\n",
      "Epoch: 0040 Training RMSE= 4.762902094\n",
      "Validation RMSE:  5.428783928863725\n",
      "Test RMSE:  4.414435284832271\n",
      "Epoch: 0041 Training RMSE= 4.743927970\n",
      "Validation RMSE:  5.507940715623112\n",
      "Test RMSE:  4.76877187421389\n",
      "Epoch: 0042 Training RMSE= 4.715456661\n",
      "Validation RMSE:  5.49271795063514\n",
      "Test RMSE:  4.6985662424293935\n",
      "Epoch: 0043 Training RMSE= 4.719816267\n",
      "Validation RMSE:  5.617792131461613\n",
      "Test RMSE:  4.682179597808355\n",
      "Epoch: 0044 Training RMSE= 4.675473474\n",
      "Validation RMSE:  5.54165326262648\n",
      "Test RMSE:  4.862278503237139\n",
      "Epoch: 0045 Training RMSE= 4.640775397\n",
      "Validation RMSE:  5.695312918622456\n",
      "Test RMSE:  4.9304815701708025\n",
      "Epoch: 0046 Training RMSE= 4.630190056\n",
      "Validation RMSE:  5.556286631271394\n",
      "Test RMSE:  4.846835722478215\n",
      "Epoch: 0047 Training RMSE= 4.607893180\n",
      "Validation RMSE:  5.546454443233906\n",
      "Test RMSE:  4.754556478198816\n",
      "Epoch: 0048 Training RMSE= 4.595670223\n",
      "Validation RMSE:  5.593625626699767\n",
      "Test RMSE:  4.796030374031509\n",
      "Epoch: 0049 Training RMSE= 4.571273343\n",
      "Validation RMSE:  5.576131341522995\n",
      "Test RMSE:  4.810658783875186\n",
      "Epoch: 0050 Training RMSE= 4.553716707\n",
      "Validation RMSE:  5.693289105618028\n",
      "Test RMSE:  4.895849685100546\n",
      "Epoch: 0051 Training RMSE= 4.523204137\n",
      "Validation RMSE:  5.657116986844469\n",
      "Test RMSE:  4.931540357118076\n",
      "Epoch: 0052 Training RMSE= 4.512676419\n",
      "Validation RMSE:  5.717221748977167\n",
      "Test RMSE:  4.880156992747748\n",
      "Epoch: 0053 Training RMSE= 4.499463622\n",
      "Validation RMSE:  5.6819609613685405\n",
      "Test RMSE:  4.947532890561319\n",
      "Epoch: 0054 Training RMSE= 4.479821379\n",
      "Validation RMSE:  5.673947462707979\n",
      "Test RMSE:  4.999757188623306\n",
      "Epoch: 0055 Training RMSE= 4.445979921\n",
      "Validation RMSE:  5.602691153619107\n",
      "Test RMSE:  4.969638673016806\n",
      "Epoch: 0056 Training RMSE= 4.448416668\n",
      "Validation RMSE:  5.928000791994935\n",
      "Test RMSE:  5.054533824913513\n",
      "Epoch: 0057 Training RMSE= 4.428070268\n",
      "Validation RMSE:  5.734164034005679\n",
      "Test RMSE:  5.13174478466263\n",
      "Epoch: 0058 Training RMSE= 4.397882815\n",
      "Validation RMSE:  5.557527355379021\n",
      "Test RMSE:  5.005964726810341\n",
      "Epoch: 0059 Training RMSE= 4.375150046\n",
      "Validation RMSE:  5.85328728348526\n",
      "Test RMSE:  5.039811331532787\n",
      "Epoch: 0060 Training RMSE= 4.387006611\n",
      "Validation RMSE:  5.674989375974147\n",
      "Test RMSE:  5.16276166866972\n",
      "Epoch: 0061 Training RMSE= 4.364962546\n",
      "Validation RMSE:  5.678214825437728\n",
      "Test RMSE:  5.102165618810313\n",
      "Epoch: 0062 Training RMSE= 4.358922080\n",
      "Validation RMSE:  5.638226620014501\n",
      "Test RMSE:  4.940097753198536\n",
      "Epoch: 0063 Training RMSE= 4.317223767\n",
      "Validation RMSE:  5.704235648088047\n",
      "Test RMSE:  5.115643776571948\n",
      "Epoch: 0064 Training RMSE= 4.297923079\n",
      "Validation RMSE:  5.679035385454181\n",
      "Test RMSE:  5.112966226150994\n",
      "Epoch: 0065 Training RMSE= 4.300396085\n",
      "Validation RMSE:  5.886240331577438\n",
      "Test RMSE:  5.227201003687038\n",
      "Epoch: 0066 Training RMSE= 4.273995071\n",
      "Validation RMSE:  5.804804311734736\n",
      "Test RMSE:  5.272877755304753\n",
      "Epoch: 0067 Training RMSE= 4.274702749\n",
      "Validation RMSE:  5.880143612645398\n",
      "Test RMSE:  5.231493712004272\n",
      "Epoch: 0068 Training RMSE= 4.256115777\n",
      "Validation RMSE:  5.701584030365796\n",
      "Test RMSE:  5.182560418858345\n",
      "Epoch: 0069 Training RMSE= 4.230899429\n",
      "Validation RMSE:  5.877842560410922\n",
      "Test RMSE:  5.184810446766254\n",
      "Epoch: 0070 Training RMSE= 4.222190420\n",
      "Validation RMSE:  5.685703287230381\n",
      "Test RMSE:  5.1684244406226245\n",
      "Epoch: 0071 Training RMSE= 4.212909906\n",
      "Validation RMSE:  6.020018877779235\n",
      "Test RMSE:  5.3207127536781496\n",
      "Epoch: 0072 Training RMSE= 4.181175489\n",
      "Validation RMSE:  5.728825849163718\n",
      "Test RMSE:  5.210230652138764\n",
      "Epoch: 0073 Training RMSE= 4.178269270\n",
      "Validation RMSE:  6.088641485391817\n",
      "Test RMSE:  5.2461722816162295\n",
      "Epoch: 0074 Training RMSE= 4.174354651\n",
      "Validation RMSE:  5.826521256521575\n",
      "Test RMSE:  5.246419684879174\n",
      "Epoch: 0075 Training RMSE= 4.160240352\n",
      "Validation RMSE:  5.994999073018948\n",
      "Test RMSE:  5.31452840340267\n",
      "Epoch: 0076 Training RMSE= 4.159608691\n",
      "Validation RMSE:  5.85076939666562\n",
      "Test RMSE:  5.281671992490151\n",
      "Epoch: 0077 Training RMSE= 4.131285422\n",
      "Validation RMSE:  5.768209648242819\n",
      "Test RMSE:  5.287807118399736\n",
      "Epoch: 0078 Training RMSE= 4.119746643\n",
      "Validation RMSE:  6.057309632563058\n",
      "Test RMSE:  5.354586138293262\n",
      "Epoch: 0079 Training RMSE= 4.109000330\n",
      "Validation RMSE:  5.819292836830655\n",
      "Test RMSE:  5.300514433744059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0080 Training RMSE= 4.115404076\n",
      "Validation RMSE:  5.9828266102785586\n",
      "Test RMSE:  5.311113064303143\n",
      "Epoch: 0081 Training RMSE= 4.084836933\n",
      "Validation RMSE:  5.752305273600608\n",
      "Test RMSE:  5.2695583967749595\n",
      "Epoch: 0082 Training RMSE= 4.079790621\n",
      "Validation RMSE:  6.059531123966753\n",
      "Test RMSE:  5.449536211070763\n",
      "Epoch: 0083 Training RMSE= 4.060493431\n",
      "Validation RMSE:  5.933778660738966\n",
      "Test RMSE:  5.4627622248999135\n",
      "Epoch: 0084 Training RMSE= 4.054962167\n",
      "Validation RMSE:  5.982930858255266\n",
      "Test RMSE:  5.447510285352474\n",
      "Epoch: 0085 Training RMSE= 4.056015368\n",
      "Validation RMSE:  5.902810038559462\n",
      "Test RMSE:  5.488295498758194\n",
      "Epoch: 0086 Training RMSE= 4.039342201\n",
      "Validation RMSE:  5.978517539202568\n",
      "Test RMSE:  5.386768498512572\n",
      "Epoch: 0087 Training RMSE= 4.026192365\n",
      "Validation RMSE:  5.851527622663917\n",
      "Test RMSE:  5.469946245421011\n",
      "Epoch: 0088 Training RMSE= 4.020872613\n",
      "Validation RMSE:  6.078527520587735\n",
      "Test RMSE:  5.462876920986342\n",
      "Epoch: 0089 Training RMSE= 4.014905487\n",
      "Validation RMSE:  5.750286178101566\n",
      "Test RMSE:  5.406145480694638\n",
      "Epoch: 0090 Training RMSE= 3.995553627\n",
      "Validation RMSE:  6.145263938605247\n",
      "Test RMSE:  5.530465452945231\n",
      "Epoch: 0091 Training RMSE= 4.009112665\n",
      "Validation RMSE:  5.82885983004695\n",
      "Test RMSE:  5.43969927629761\n",
      "Epoch: 0092 Training RMSE= 3.981414455\n",
      "Validation RMSE:  5.945825301523359\n",
      "Test RMSE:  5.407907926328669\n",
      "Epoch: 0093 Training RMSE= 3.960025582\n",
      "Validation RMSE:  5.952653681002051\n",
      "Test RMSE:  5.61085545971986\n",
      "Epoch: 0094 Training RMSE= 3.967319195\n",
      "Validation RMSE:  5.961763697172206\n",
      "Test RMSE:  5.452179660123522\n",
      "Epoch: 0095 Training RMSE= 3.964886426\n",
      "Validation RMSE:  5.84234316614143\n",
      "Test RMSE:  5.379906566191434\n",
      "Epoch: 0096 Training RMSE= 3.962075046\n",
      "Validation RMSE:  6.037861417137173\n",
      "Test RMSE:  5.505221056194154\n",
      "Epoch: 0097 Training RMSE= 3.948115230\n",
      "Validation RMSE:  5.882967720162347\n",
      "Test RMSE:  5.553512942663724\n",
      "Epoch: 0098 Training RMSE= 3.922269386\n",
      "Validation RMSE:  5.992221717786509\n",
      "Test RMSE:  5.525488660097071\n",
      "Epoch: 0099 Training RMSE= 3.942042737\n",
      "Validation RMSE:  6.059997592711522\n",
      "Test RMSE:  5.580018307235383\n",
      "Epoch: 0100 Training RMSE= 3.928428943\n",
      "Validation RMSE:  5.929477612546524\n",
      "Test RMSE:  5.4949932718279175\n",
      "Epoch: 0101 Training RMSE= 3.903928393\n",
      "Validation RMSE:  6.104686450201471\n",
      "Test RMSE:  5.61371514774394\n",
      "Epoch: 0102 Training RMSE= 3.917626165\n",
      "Validation RMSE:  6.007906948979218\n",
      "Test RMSE:  5.535239725822781\n",
      "Epoch: 0103 Training RMSE= 3.893370521\n",
      "Validation RMSE:  6.075283376732725\n",
      "Test RMSE:  5.581070408169023\n",
      "Epoch: 0104 Training RMSE= 3.905124429\n",
      "Validation RMSE:  5.925994728382146\n",
      "Test RMSE:  5.595565416062925\n",
      "Epoch: 0105 Training RMSE= 3.882947524\n",
      "Validation RMSE:  6.033760457524732\n",
      "Test RMSE:  5.525078299127245\n",
      "Epoch: 0106 Training RMSE= 3.887199187\n",
      "Validation RMSE:  6.033087178823757\n",
      "Test RMSE:  5.609225980431555\n",
      "Epoch: 0107 Training RMSE= 3.889703744\n",
      "Validation RMSE:  5.970036073508613\n",
      "Test RMSE:  5.540424475400305\n",
      "Epoch: 0108 Training RMSE= 3.874268912\n",
      "Validation RMSE:  5.993404657719544\n",
      "Test RMSE:  5.592502220402338\n",
      "Epoch: 0109 Training RMSE= 3.872656169\n",
      "Validation RMSE:  6.097967009167683\n",
      "Test RMSE:  5.577295920256025\n",
      "Epoch: 0110 Training RMSE= 3.865144076\n",
      "Validation RMSE:  5.8512543010145786\n",
      "Test RMSE:  5.582430078501643\n",
      "Epoch: 0111 Training RMSE= 3.867199750\n",
      "Validation RMSE:  5.994152796221147\n",
      "Test RMSE:  5.600498752863245\n",
      "Epoch: 0112 Training RMSE= 3.850350062\n",
      "Validation RMSE:  6.1411116312574805\n",
      "Test RMSE:  5.63309997586895\n",
      "Epoch: 0113 Training RMSE= 3.858496173\n",
      "Validation RMSE:  5.922285563949475\n",
      "Test RMSE:  5.601718277939839\n",
      "Epoch: 0114 Training RMSE= 3.834740626\n",
      "Validation RMSE:  6.220763220863565\n",
      "Test RMSE:  5.655149574089289\n",
      "Epoch: 0115 Training RMSE= 3.834783439\n",
      "Validation RMSE:  5.972546872721099\n",
      "Test RMSE:  5.624973551370111\n",
      "Epoch: 0116 Training RMSE= 3.828530109\n",
      "Validation RMSE:  6.192619979737167\n",
      "Test RMSE:  5.646290562357112\n",
      "Epoch: 0117 Training RMSE= 3.820732253\n",
      "Validation RMSE:  6.010494432216217\n",
      "Test RMSE:  5.593517618307238\n",
      "Epoch: 0118 Training RMSE= 3.822140322\n",
      "Validation RMSE:  6.135171657607516\n",
      "Test RMSE:  5.644099467833235\n",
      "Epoch: 0119 Training RMSE= 3.814246419\n",
      "Validation RMSE:  6.029254009859368\n",
      "Test RMSE:  5.73741472633837\n",
      "Epoch: 0120 Training RMSE= 3.810841384\n",
      "Validation RMSE:  6.109830639329062\n",
      "Test RMSE:  5.643739638208148\n",
      "Epoch: 0121 Training RMSE= 3.799447476\n",
      "Validation RMSE:  6.0515014141061885\n",
      "Test RMSE:  5.6362563357948945\n",
      "Epoch: 0122 Training RMSE= 3.809960167\n",
      "Validation RMSE:  6.096067095655116\n",
      "Test RMSE:  5.5982571989384615\n",
      "Epoch: 0123 Training RMSE= 3.799475752\n",
      "Validation RMSE:  5.936339656484531\n",
      "Test RMSE:  5.54069075440506\n",
      "Epoch: 0124 Training RMSE= 3.787777863\n",
      "Validation RMSE:  6.107888511639491\n",
      "Test RMSE:  5.67042468315569\n",
      "Epoch: 0125 Training RMSE= 3.777481641\n",
      "Validation RMSE:  5.932069242313804\n",
      "Test RMSE:  5.623566678008436\n",
      "Epoch: 0126 Training RMSE= 3.773995741\n",
      "Validation RMSE:  6.137450584321457\n",
      "Test RMSE:  5.659467509128173\n",
      "Epoch: 0127 Training RMSE= 3.769398197\n",
      "Validation RMSE:  6.165879053217097\n",
      "Test RMSE:  5.756632918255427\n",
      "Epoch: 0128 Training RMSE= 3.771555568\n",
      "Validation RMSE:  6.007638360762576\n",
      "Test RMSE:  5.696178229195625\n",
      "Epoch: 0129 Training RMSE= 3.774156566\n",
      "Validation RMSE:  6.2281917034430165\n",
      "Test RMSE:  5.827047867224349\n",
      "Epoch: 0130 Training RMSE= 3.765375596\n",
      "Validation RMSE:  6.029462005756187\n",
      "Test RMSE:  5.718321487131341\n",
      "Epoch: 0131 Training RMSE= 3.761097541\n",
      "Validation RMSE:  6.1393326425863\n",
      "Test RMSE:  5.756359895367481\n",
      "Epoch: 0132 Training RMSE= 3.753472462\n",
      "Validation RMSE:  6.105609406494196\n",
      "Test RMSE:  5.7652295811804635\n",
      "Epoch: 0133 Training RMSE= 3.756838178\n",
      "Validation RMSE:  6.186181361399018\n",
      "Test RMSE:  5.830649476034203\n",
      "Epoch: 0134 Training RMSE= 3.756961433\n",
      "Validation RMSE:  5.9993309601658495\n",
      "Test RMSE:  5.700903891119482\n",
      "Epoch: 0135 Training RMSE= 3.743852211\n",
      "Validation RMSE:  6.209658931389828\n",
      "Test RMSE:  5.797662809980203\n",
      "Epoch: 0136 Training RMSE= 3.731745298\n",
      "Validation RMSE:  6.121177258923013\n",
      "Test RMSE:  5.890481233739656\n",
      "Epoch: 0137 Training RMSE= 3.741665594\n",
      "Validation RMSE:  6.189071147539954\n",
      "Test RMSE:  5.782721749691108\n",
      "Epoch: 0138 Training RMSE= 3.743688192\n",
      "Validation RMSE:  6.07510293002115\n",
      "Test RMSE:  5.799341381430655\n",
      "Epoch: 0139 Training RMSE= 3.716373114\n",
      "Validation RMSE:  6.166499865913684\n",
      "Test RMSE:  5.778191329550585\n",
      "Epoch: 0140 Training RMSE= 3.729248570\n",
      "Validation RMSE:  6.193157653506925\n",
      "Test RMSE:  5.865458430514844\n",
      "Epoch: 0141 Training RMSE= 3.717528051\n",
      "Validation RMSE:  6.031864747653446\n",
      "Test RMSE:  5.687465961061001\n",
      "Epoch: 0142 Training RMSE= 3.709844643\n",
      "Validation RMSE:  6.310695428023628\n",
      "Test RMSE:  5.8993334765487955\n",
      "Epoch: 0143 Training RMSE= 3.719951846\n",
      "Validation RMSE:  6.110511615517079\n",
      "Test RMSE:  5.7490143967834895\n",
      "Epoch: 0144 Training RMSE= 3.704880156\n",
      "Validation RMSE:  6.199369201565144\n",
      "Test RMSE:  5.883848628458859\n",
      "Epoch: 0145 Training RMSE= 3.714458708\n",
      "Validation RMSE:  6.07902116224443\n",
      "Test RMSE:  5.750473915141105\n",
      "Epoch: 0146 Training RMSE= 3.705675533\n",
      "Validation RMSE:  6.2380909183061535\n",
      "Test RMSE:  5.853422920885443\n",
      "Epoch: 0147 Training RMSE= 3.695796620\n",
      "Validation RMSE:  6.050727582955796\n",
      "Test RMSE:  5.851393652645723\n",
      "Epoch: 0148 Training RMSE= 3.703459120\n",
      "Validation RMSE:  6.168331162167923\n",
      "Test RMSE:  5.884301958324637\n",
      "Epoch: 0149 Training RMSE= 3.710561093\n",
      "Validation RMSE:  6.229465779381437\n",
      "Test RMSE:  5.870525035610153\n",
      "Epoch: 0150 Training RMSE= 3.678274751\n",
      "Validation RMSE:  6.076448890995294\n",
      "Test RMSE:  5.88215558562472\n",
      "Epoch: 0151 Training RMSE= 3.698524727\n",
      "Validation RMSE:  6.239596749505175\n",
      "Test RMSE:  5.945374577482663\n",
      "Epoch: 0152 Training RMSE= 3.691019661\n",
      "Validation RMSE:  6.132111449779719\n",
      "Test RMSE:  5.895344834146767\n",
      "Epoch: 0153 Training RMSE= 3.684506562\n",
      "Validation RMSE:  6.192183754414714\n",
      "Test RMSE:  5.933559194116379\n",
      "Epoch: 0154 Training RMSE= 3.684642997\n",
      "Validation RMSE:  6.2299634579096566\n",
      "Test RMSE:  5.956110808756543\n",
      "Epoch: 0155 Training RMSE= 3.673008047\n",
      "Validation RMSE:  6.254682236611341\n",
      "Test RMSE:  5.86030998399577\n",
      "Epoch: 0156 Training RMSE= 3.675275797\n",
      "Validation RMSE:  6.152204394503093\n",
      "Test RMSE:  5.982926076277736\n",
      "Epoch: 0157 Training RMSE= 3.678022438\n",
      "Validation RMSE:  6.21128205117416\n",
      "Test RMSE:  5.905043705868136\n",
      "Epoch: 0158 Training RMSE= 3.660084342\n",
      "Validation RMSE:  6.136777648060373\n",
      "Test RMSE:  5.8860044292779365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0159 Training RMSE= 3.667903561\n",
      "Validation RMSE:  6.289288591384384\n",
      "Test RMSE:  5.972706706580529\n",
      "Epoch: 0160 Training RMSE= 3.664361996\n",
      "Validation RMSE:  6.105793949772632\n",
      "Test RMSE:  5.942980194782837\n",
      "Epoch: 0161 Training RMSE= 3.660426508\n",
      "Validation RMSE:  6.321330022774949\n",
      "Test RMSE:  5.950368488345467\n",
      "Epoch: 0162 Training RMSE= 3.654719367\n",
      "Validation RMSE:  6.230232105125798\n",
      "Test RMSE:  5.986452065907013\n",
      "Epoch: 0163 Training RMSE= 3.661800820\n",
      "Validation RMSE:  6.182173314577931\n",
      "Test RMSE:  5.8933401958431375\n",
      "Epoch: 0164 Training RMSE= 3.648265121\n",
      "Validation RMSE:  6.205883114391713\n",
      "Test RMSE:  6.017652927764545\n",
      "Epoch: 0165 Training RMSE= 3.647251180\n",
      "Validation RMSE:  6.298700186442937\n",
      "Test RMSE:  5.934285468239179\n",
      "Epoch: 0166 Training RMSE= 3.657208048\n",
      "Validation RMSE:  6.193944330178989\n",
      "Test RMSE:  6.033008062260225\n",
      "epoch is  165\n",
      "training RMSE is  5.083405542121361\n",
      "Optimization Finished! the lowest validation RMSE is  5.308986624017716\n",
      "The test RMSE is  4.212664418167147\n",
      "0:03:45.717903\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "#freq_max = 12\n",
    "#time_step = 12\n",
    "learning_rate = 0.01\n",
    "decay = 0.9 \n",
    "batch_size = 1000\n",
    "num_hidden = 50 # number of hiddent units in LSTM Cell\n",
    "early_stop_th = 150\n",
    "training_epochs = 200\n",
    "keep = 1#0.2\n",
    "#time_step_max = 10\n",
    "frequency = 24\n",
    "sn = 272 # station num\n",
    "\n",
    "n_hidden_vec1 = 10\n",
    "n_hidden_vec2 = 10\n",
    "n_hidden_vec3 = 10\n",
    "#n_hidden_vec4 = 10\n",
    "#num = 0\n",
    "#All_pred = np.empty([2000, 207])\n",
    "#All_Y = np.empty([2000, 207])\n",
    "\n",
    "#24*90\n",
    "#step = 0\n",
    "#gap = 100\n",
    "#training = 0.7\n",
    "#validation = 0.1\n",
    "#test = 0.2\n",
    "\n",
    "#gcn_corr_eval(7, 0.01, 0.5, 100, 0.4, 10, 5, 5, 0.2, 50, 500)\n",
    "\n",
    "reg1 = 0\n",
    "reg2 = 0\n",
    "\n",
    "rep = 1 # repeating times\n",
    "\n",
    "#total_sn = 0\n",
    "#num_iter = 50\n",
    "#init_points = 200\n",
    "\n",
    "\n",
    "# stdbscan\n",
    "#spatial_threshold = 300\n",
    "#temporal_threshold = 300\n",
    "#min_neighbors = 1 # number of neighbor\n",
    "\n",
    "#frequency2 = skip1 + freq_max + training\n",
    "\n",
    "#while step < 2000:\n",
    "\n",
    "#hourly_bike_cluster = hourly_bike\n",
    "best = -10000\n",
    "pre_best = []\n",
    "test_Y_best = []\n",
    "test_error_best = 1000\n",
    "A1_best = []\n",
    "# A2_best = []\n",
    "for i in range(rep):\n",
    "    a = datetime.datetime.now()\n",
    "    test_error, predic_res, Y_true = gcn_corr_final(frequency, horizon, learning_rate, decay, batch_size, n_hidden_vec1,\n",
    "                                                                n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\n",
    "    #val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\n",
    "    #print (\"finished A running: \", i)\n",
    "    b = datetime.datetime.now()\n",
    "    print(b-a)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#lstm cell num 10, hidden num [10, 10, 10], batchsize 1000, hidden state 50: 2.40\n",
    "\n",
    "#lstm cell num 10, hidden num [10, 10, 10], batchsize 1000, hidden state 50, no relu between layers: 2.41\n",
    "\n",
    "#lstm cell num 10, hidden num [10, 10, 10], batchsize 1000, hidden state 50, sum layers: 2.4+\n",
    "\n",
    "#lstm cell num 10, hidden num 1, batchsize 1000, hidden state 50, sum layers: 3.04, 2.4\n",
    "#lstm cell num 10, hidden num 1, batchsize 1000, hidden state 100, sum layers: \n",
    "#lstm cell num 10, hidden num 1, batchsize 1000, hidden state 20, sum layers: \n",
    "#lstm cell num 10, hidden num 1, batchsize 2000, hidden state 50, sum layers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"lstm_gcnn_prediction_1.95.csv\", predic_res, delimiter = ',')\n",
    "#np.savetxt(\"lstm_gcnn_prediction_1.95_Y.csv\", Y_true, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
