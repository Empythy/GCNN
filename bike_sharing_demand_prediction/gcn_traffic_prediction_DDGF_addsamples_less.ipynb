{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "#from tensorflow.contrib.rnn.python.ops import rnn_cell as RNNCell\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "import collections\n",
    "from tensorflow.contrib import rnn\n",
    "import h5py\n",
    "#from tensorflow.python.ops.rnn_cell_impl import _RNNCell \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    #adj[np.isnan(adj)] = 0.\n",
    "    adj = tf.abs(adj)\n",
    "    rowsum = tf.reduce_sum(adj, 1)# sum by row\n",
    "\n",
    "    d_inv_sqrt = tf.pow(rowsum, -0.5)\n",
    "   \n",
    "    #d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    \n",
    "    d_mat_inv_sqrt = tf.diag(d_inv_sqrt)\n",
    "\n",
    "    return tf.matmul(tf.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "\n",
    "def masked_mae_tf(preds, labels, null_val=np.nan):\n",
    "    \"\"\"\n",
    "    Accuracy with masking.\n",
    "    :param preds:\n",
    "    :param labels:\n",
    "    :param null_val:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~tf.is_nan(labels)\n",
    "    else:\n",
    "        mask = tf.not_equal(labels, null_val)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    mask = tf.where(tf.is_nan(mask), tf.zeros_like(mask), mask)\n",
    "    loss = tf.abs(tf.subtract(preds, labels))\n",
    "    loss = loss * mask\n",
    "    loss = tf.where(tf.is_nan(loss), tf.zeros_like(loss), loss)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    Standard the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install tables\n",
    "raw_data = pd.read_hdf('../../data/METR-LA/metr-la.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34272, 207)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>773869</th>\n",
       "      <th>767541</th>\n",
       "      <th>767542</th>\n",
       "      <th>717447</th>\n",
       "      <th>717446</th>\n",
       "      <th>717445</th>\n",
       "      <th>773062</th>\n",
       "      <th>767620</th>\n",
       "      <th>737529</th>\n",
       "      <th>717816</th>\n",
       "      <th>...</th>\n",
       "      <th>772167</th>\n",
       "      <th>769372</th>\n",
       "      <th>774204</th>\n",
       "      <th>769806</th>\n",
       "      <th>717590</th>\n",
       "      <th>717592</th>\n",
       "      <th>717595</th>\n",
       "      <th>772168</th>\n",
       "      <th>718141</th>\n",
       "      <th>769373</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:00:00</th>\n",
       "      <td>64.375000</td>\n",
       "      <td>67.625000</td>\n",
       "      <td>67.125000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>66.875000</td>\n",
       "      <td>68.750000</td>\n",
       "      <td>65.125</td>\n",
       "      <td>67.125</td>\n",
       "      <td>59.625000</td>\n",
       "      <td>62.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>45.625000</td>\n",
       "      <td>65.500</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>66.428571</td>\n",
       "      <td>66.875</td>\n",
       "      <td>59.375000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>61.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:05:00</th>\n",
       "      <td>62.666667</td>\n",
       "      <td>68.555556</td>\n",
       "      <td>65.444444</td>\n",
       "      <td>62.444444</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>68.111111</td>\n",
       "      <td>65.000</td>\n",
       "      <td>65.000</td>\n",
       "      <td>57.444444</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>50.666667</td>\n",
       "      <td>69.875</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>58.555556</td>\n",
       "      <td>62.000</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>55.888889</td>\n",
       "      <td>68.444444</td>\n",
       "      <td>62.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:10:00</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>64.500</td>\n",
       "      <td>64.250</td>\n",
       "      <td>63.875000</td>\n",
       "      <td>65.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>44.125000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>68.125</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>65.625000</td>\n",
       "      <td>61.375000</td>\n",
       "      <td>69.857143</td>\n",
       "      <td>62.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:15:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-01 00:20:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        773869     767541     767542     717447     717446  \\\n",
       "2012-03-01 00:00:00  64.375000  67.625000  67.125000  61.500000  66.875000   \n",
       "2012-03-01 00:05:00  62.666667  68.555556  65.444444  62.444444  64.444444   \n",
       "2012-03-01 00:10:00  64.000000  63.750000  60.000000  59.000000  66.500000   \n",
       "2012-03-01 00:15:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2012-03-01 00:20:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "                        717445  773062  767620     737529     717816  ...  \\\n",
       "2012-03-01 00:00:00  68.750000  65.125  67.125  59.625000  62.750000  ...   \n",
       "2012-03-01 00:05:00  68.111111  65.000  65.000  57.444444  63.333333  ...   \n",
       "2012-03-01 00:10:00  66.250000  64.500  64.250  63.875000  65.375000  ...   \n",
       "2012-03-01 00:15:00   0.000000   0.000   0.000   0.000000   0.000000  ...   \n",
       "2012-03-01 00:20:00   0.000000   0.000   0.000   0.000000   0.000000  ...   \n",
       "\n",
       "                        772167  769372     774204     769806  717590  \\\n",
       "2012-03-01 00:00:00  45.625000  65.500  64.500000  66.428571  66.875   \n",
       "2012-03-01 00:05:00  50.666667  69.875  66.666667  58.555556  62.000   \n",
       "2012-03-01 00:10:00  44.125000  69.000  56.500000  59.250000  68.125   \n",
       "2012-03-01 00:15:00   0.000000   0.000   0.000000   0.000000   0.000   \n",
       "2012-03-01 00:20:00   0.000000   0.000   0.000000   0.000000   0.000   \n",
       "\n",
       "                        717592     717595     772168     718141  769373  \n",
       "2012-03-01 00:00:00  59.375000  69.000000  59.250000  69.000000  61.875  \n",
       "2012-03-01 00:05:00  61.111111  64.444444  55.888889  68.444444  62.875  \n",
       "2012-03-01 00:10:00  62.500000  65.625000  61.375000  69.857143  62.000  \n",
       "2012-03-01 00:15:00   0.000000   0.000000   0.000000   0.000000   0.000  \n",
       "2012-03-01 00:20:00   0.000000   0.000000   0.000000   0.000000   0.000  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def gcn(x, weights, biases, batch_size, n_input, frequency,flag, n_output_vec):\n",
    "    # Hidden layer with RELU activation\n",
    "    \n",
    "   # output_list = tf.Variable(tf.zeros([n_output_vec,1]),dtype=tf.float32) #'Tensor' object does not support item assignment, cant build Ypre\n",
    "\n",
    "    #Xtem = tf.reshape(x[i,:], [n_input, frequency])\n",
    "    #Xtem = tf.transpose(Xtem)\n",
    "    #Atem = tf.convert_to_tensor(A_whole_final, dtype=np.float32)\n",
    "    #Atem1 = tf.diag(tf.ones([n_input])) # Atem not palying any roles\n",
    "    #Ytem = tf.reshape(Y[i,:], [n_input, 1])\n",
    "    #Atem = tf.diag(tf.ones([n_input]))\n",
    "    #x = tf.reshape(x, [-1, sn, 1]) # 100, 207, 1\n",
    "    \n",
    "    # x (?, 207, 12)\n",
    "    x = tf.transpose(x, [1, 0, 2]) # 207, ?, 12\n",
    "    x = tf.reshape(x, [sn, -1]) # 207, batch*feature_num\n",
    "    Atem1 = 0.5*(weights['A1'] + tf.transpose(weights['A1']))#+ Atem \n",
    "    Atem1 = normalize_adj(Atem1)\n",
    "    #th = tf.constant(0.01, dtype=tf.float32)\n",
    "    #where = tf.subtract(Atem1, th)\n",
    "    #Atem1 = tf.nn.relu(where)\n",
    "\n",
    "    Z1 = tf.matmul(Atem1, x) # 207, batch*feature_num  #+ tf.matmul( tf.matmul(weights['A1'], weights['A1']), Xtem)\n",
    "    Z1 = tf.reshape(Z1, [-1, frequency]) # 207* 100, frequency\n",
    "    #layer_1 = tf.matmul(Z1, weights['h1']) \n",
    "    layer_1 = tf.add(tf.matmul(Z1, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1) # 207*100, hidden1\n",
    "\n",
    "    \n",
    "    Atem2 = 0.5*(weights['A2'] + tf.transpose(weights['A2']))#+ Atem \n",
    "    Atem2 = normalize_adj(Atem2)\n",
    "    \n",
    "    layer_1 = tf.reshape(layer_1, [sn, -1])  # 207, batchsize*hidden1\n",
    "    Z2 = tf.matmul(Atem2, layer_1)\n",
    "    Z2 = tf.reshape(Z2, [-1, n_hidden_vec1]) # 207*batchsize, n_hidden_vec1\n",
    "    layer_2 = tf.add(tf.matmul(Z2, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2) # 207*batchsize, hidden2\n",
    "\n",
    "    Atem3 = 0.5*(weights['A3'] + tf.transpose(weights['A3']))#+ Atem \n",
    "    Atem3 = normalize_adj(Atem3) \n",
    "    \n",
    "    layer_2 = tf.reshape(layer_2, [sn, -1])  # 207, batchsize*hidden2\n",
    "    Z3 = tf.matmul(Atem3, layer_2)\n",
    "    Z3 = tf.reshape(Z3, [-1, n_hidden_vec2]) # 207*batchsize, hidden2\n",
    "    layer_3 = tf.add(tf.matmul(Z3, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3) # 207*batchsize, hidden3\n",
    "    \n",
    "    #Atem4 = 0.5*(weights['A4'] + tf.transpose(weights['A4']))#+ Atem \n",
    "    #Atem4 = normalize_adj(Atem4)\n",
    "    #Z4 = tf.matmul(Atem4, layer_3)\n",
    "    #layer_4 = tf.add(tf.matmul(Z4, weights['h4']), biases['b4'])\n",
    "    #layer_4 = tf.nn.relu(layer_4)\n",
    "\n",
    "    # flattern\n",
    "    #layer_3 = tf.reshape(layer_3, [1, 272*n_hidden_vec3])\n",
    "\n",
    "    #F1 = tf.add(tf.matmul(layer_3, weights['f1']), biases['bf1'])\n",
    "    #F1 = tf.nn.relu(F1)\n",
    "\n",
    "    #F2 = tf.add(tf.matmul(F1, weights['f2']), biases['bf2'])\n",
    "    #F2 = tf.nn.relu(F2)\n",
    "\n",
    "    #F3 = tf.add(tf.matmul(F1, weights['f3']), biases['bf3'])\n",
    "    #out_layer = tf.reshape(F3, [272, 1])\n",
    "\n",
    "    # Output layer with linear activation\n",
    "    #layer_1 = tf.reshape(layer_1, [sn, -1, n_hidden_vec1])\n",
    "    #layer_1 = tf.transpose(layer_1, [1, 0, 2]) # batchsize, sn, hidden3\n",
    "    #layer_1 = tf.reshape(layer_1, [-1, n_hidden_vec1]) # batchsize * sn, hidden3\n",
    "    \n",
    "    out_layer = tf.add(tf.matmul(layer_3, weights['out']), biases['bout'])  # 207*100, horizon\n",
    "    out_layer = tf.reshape(out_layer, [sn, -1, horizon]) # 207, 100, horizon\n",
    "    out_layer = tf.transpose(out_layer, [1, 0, 2]) # 100, 207, horizon\n",
    "    out_layer = tf.reshape(out_layer, [-1, sn*horizon]) # 100, 207*horizon\n",
    "    #tf.matmul(Atem1, layer_1)#tf.matmul(Atem, layer_3)\n",
    "    #out_layer = tf.add(tf.matmul(Z4, weights['out']), biases['bout'])\n",
    "    #out_layer = tf.nn.relu(out_layer)\n",
    "    # weather layer 1\n",
    "    #x_wea_tem = tf.reshape(x_wea[i,:], [1, 9*frequency2]) # 1 by 126\n",
    "    #layer_1_wea = tf.add(tf.matmul(x_wea_tem, weights['h1_wea']), biases['b1_wea'])\n",
    "    #layer_1_wea = tf.nn.relu(layer_1_wea)\n",
    "\n",
    "    #out_layer_wea = tf.add(tf.matmul(layer_1_wea, weights['out_wea']), biases['bout_wea'])\n",
    "    #out_layer = tf.add(out_layer, tf.reshape(out_layer_wea, [272, 1]))\n",
    "\n",
    "\n",
    "    \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_corr_final(frequency, horizon, learning_rate, decay,batch_size, n_hidden_vec1,n_hidden_vec2,n_hidden_vec3,keep, early_stop_th,training_epochs, reg1, reg2):\n",
    "    # set size\n",
    "    #sn = 3 # station number\n",
    "    X_whole = []\n",
    "    Y_whole = []\n",
    "\n",
    "    x_offsets = np.sort(\n",
    "        # np.concatenate(([-week_size + 1, -day_size + 1], np.arange(-11, 1, 1)))\n",
    "        np.concatenate((np.arange(-frequency+1, 1, 1),))\n",
    "    )\n",
    "    # Predict the next one hour\n",
    "    y_offsets = np.sort(np.arange(1, 1+ horizon, 1))\n",
    "\n",
    "    min_t = abs(min(x_offsets))\n",
    "    max_t = abs(raw_data.shape[0] - abs(max(y_offsets)))  # Exclusive\n",
    "    for t in range(min_t, max_t):\n",
    "        x_t = raw_data.iloc[t + x_offsets, 0:sn].values.flatten('F')\n",
    "        y_t = raw_data.iloc[t + y_offsets, 0:sn].values.flatten('F')\n",
    "        X_whole.append(x_t)\n",
    "        Y_whole.append(y_t)\n",
    "\n",
    "    X_whole = np.stack(X_whole, axis=0)\n",
    "    Y_whole = np.stack(Y_whole, axis=0)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    time_step = int(time_step) #\n",
    "\n",
    "    #i = time_step\n",
    "    #X_whole = np.zeros(shape = (raw_data.shape[0] - time_step, sn*time_step), dtype = np.float)\n",
    "    #Y_whole = np.zeros(shape = (raw_data.shape[0] - time_step, sn), dtype = np.float)\n",
    "\n",
    "    while i < raw_data.shape[0]:\n",
    "        X_whole[i - time_step, ] = raw_data.iloc[(i - time_step):i, 0:sn].values.flatten('F') # 'F' flatten by column, default:flatten by row 0, 1, 2...7\n",
    "        Y_whole[i - time_step, ] = raw_data.iloc[i, 0:sn]\n",
    "        i = i + 1\n",
    "    '''\n",
    "    time_step = int(X_whole.shape[1] / sn)\n",
    "\n",
    "    n_input = sn # station number\n",
    "    n_input_vec = n_input * frequency # 207 * frequency\n",
    "    n_A_vec = n_input * n_input\n",
    "    n_output_vec = Y_whole.shape[1] # each row represent a result\n",
    "    #print (n_output_vec)\n",
    "\n",
    "    X_whole = np.reshape(X_whole, [X_whole.shape[0], sn, time_step])\n",
    "    num_samples = X_whole.shape[0]\n",
    "    num_test = round(num_samples * 0.2)\n",
    "    num_train = round(num_samples * 0.7)\n",
    "    num_val = num_samples - num_test - num_train\n",
    "    #skip = skip1 + freq_max - time_step#time_step_max - time_step # to make sure the testing datasets are the same although the frequency could be different\n",
    "\n",
    "    X_training = X_whole[:num_train, :]\n",
    "    Y_training = Y_whole[:num_train, :]\n",
    "    \n",
    "    # shuffle\n",
    "    perm = np.arange(X_training.shape[0])\n",
    "    np.random.shuffle(perm)\n",
    "    X_training = X_training[perm]\n",
    "    Y_training = Y_training[perm]\n",
    "    \n",
    "    #print (type(X_training))\n",
    "    #X_training = random.Random(6).shuffle(X_training)\n",
    "    #Y_training = random.Random(6).shuffle(Y_training)\n",
    "\n",
    "    X_val = X_whole[num_train:num_train+num_val, :]\n",
    "    Y_val = Y_whole[num_train:num_train+num_val, :]\n",
    "    #A_val = A_whole[0+training:0+training+validation, :]\n",
    "\n",
    "    X_test = X_whole[-num_test:, :]\n",
    "    Y_test = Y_whole[-num_test:, :]\n",
    "\n",
    "    scaler = StandardScaler(mean=X_training.mean(), std=X_training.std())\n",
    "\n",
    "    X_training = scaler.transform(X_training)\n",
    "    Y_training = scaler.transform(Y_training)\n",
    "\n",
    "    X_val = scaler.transform(X_val)\n",
    "    Y_val = scaler.transform(Y_val)\n",
    "\n",
    "    X_test = scaler.transform(X_test)\n",
    "    Y_test = scaler.transform(Y_test)\n",
    "    \n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    predic_res = []\n",
    "    predic_step = []\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    \n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "    n_hidden_vec1 = int(n_hidden_vec1)\n",
    "    n_hidden_vec2 = int(n_hidden_vec2)\n",
    "    n_hidden_vec3 = int(n_hidden_vec3)\n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    # Network Parameters\n",
    "\n",
    "    #n_classes = 2 # MNIST total classes (0-9 digits) # n_classes is for classification only\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None, sn, time_step]) # X is the input signal\n",
    "    #X_weather = tf.placeholder(tf.float32, [None, 9 * frequency2]) # X_weather weather and holiday information (9 is the feature number)\n",
    "    A = tf.placeholder(tf.float32, [None, n_A_vec]) # A is the normalized adj matrix\n",
    "    oldA = tf.placeholder(tf.float32, [n_input, n_input])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_output_vec]) # y is the regression output\n",
    "\n",
    "    #Xtem = tf.placeholder(tf.float32, [n_input, frequency]) # for each row of X, A, Y, it can be reshaped to Xtem, Atem, Ytem\n",
    "    #Atem = tf.placeholder(tf.float32, [n_input, n_input]) # \n",
    "    #Ytem = tf.placeholder(tf.float32, [n_input, 1]) #\n",
    "\n",
    "    #Ypre = tf.placeholder(tf.float32, [None, n_output_vec])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([frequency, n_hidden_vec1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_vec1, n_hidden_vec2])),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_vec2, n_hidden_vec3])),\n",
    "        #'h4': tf.Variable(tf.random_normal([n_hidden_vec3, n_hidden_vec4])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_vec3, horizon])), # dont forget to change n_hidden_vec1 when add/delete layers\n",
    "        #'f1': tf.Variable(tf.random_normal([272*n_hidden_vec3, 100])),\n",
    "        #'f2': tf.Variable(tf.random_normal([50, 10])),\n",
    "        #'f3': tf.Variable(tf.random_normal([100, 272])),\n",
    "        'A1': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        'A2': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        'A3': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'A4': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'h1_wea': tf.Variable(tf.random_normal([9*frequency2, n_hidden_weather1])),\n",
    "        #'out_wea': tf.Variable(tf.random_normal([n_hidden_weather1, n_input]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([1,n_hidden_vec1])),# n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        'b2': tf.Variable(tf.random_normal([1, n_hidden_vec2])), #n_hidden_vec2])),\n",
    "        'b3': tf.Variable(tf.random_normal([1, n_hidden_vec3])),#n_hidden_vec3])),\n",
    "        #'b4': tf.Variable(tf.random_normal([n_input, n_hidden_vec4])),\n",
    "        #'b1': tf.Variable(tf.random_normal([n_input,n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        #'b2': tf.Variable(tf.random_normal([n_input,n_hidden_vec2])),\n",
    "        #'b3': tf.Variable(tf.random_normal([n_input,n_hidden_vec3])),\n",
    "        #'bf1': tf.Variable(tf.random_normal([1, 100])), \n",
    "        #'bf2': tf.Variable(tf.random_normal([1, 10])), \n",
    "        #'bf3': tf.Variable(tf.random_normal([1, 272])), \n",
    "        'bout': tf.Variable(tf.random_normal([1, horizon])), \n",
    "        #'b1_wea': tf.Variable(tf.random_normal([1, n_hidden_weather1])), \n",
    "        #'bout_wea': tf.Variable(tf.random_normal([1, n_input])), \n",
    "    }\n",
    "\n",
    "    #gcn_no_A\n",
    "    #gcn\n",
    "    # Construct model\n",
    "    pred= gcn(X, weights, biases, batch_size,n_input, frequency, 1, n_output_vec)\n",
    "    #print ('here!!!!!!!!!!!!!!!!')\n",
    "    #set  negative in pred to 0s\n",
    "    #Computes rectified linear: max(features, 0).\n",
    "    #pred = tf.nn.relu(pred)\n",
    "    # Define loss and optimizer\n",
    "    # RMS for regression \n",
    "    # L2 regularization\n",
    "    #cost = tf.reduce_mean(tf.pow(pred-Y, 2)) + reg1*tf.nn.l2_loss(weights['A1']) + reg2*tf.nn.l2_loss(weights['A2'])# + tf.nn.l2_loss(weights['A2']) + tf.nn.l2_loss(weights['A3'])))\n",
    "\n",
    "    # L1 regular tf.reduce_sum(tf.abs(parameters))\n",
    "    # mae\n",
    "    \n",
    "    pred = scaler.inverse_transform(pred)\n",
    "    Y_true_tr = scaler.inverse_transform(Y)\n",
    "\n",
    "    cost = masked_mae_tf(pred, Y_true_tr, 0)#+ reg1*tf.reduce_sum(tf.abs(weights['A1'])) +  reg2*tf.reduce_sum(tf.abs(weights['A2'])) + reg3*tf.reduce_sum(tf.abs(weights['A3']))#tf.reduce_mean(tf.abs(pred-Y_true_tr))# + reg1*tf.reduce_sum(tf.abs(weights['A1']))\n",
    "    #cost = tf.reduce_mean(tf.pow(pred-Y, 2)) + reg1*tf.reduce_sum(tf.abs(weights['A1']))# + reg2*tf.reduce_sum(tf.abs(weights['A2']))\n",
    "    pred_val= gcn(X, weights, biases, batch_size,n_input,frequency, 2, n_output_vec)\n",
    "    #pred_val = tf.nn.relu(pred_val)\n",
    "    #mae \n",
    "    pred_val = scaler.inverse_transform(pred_val)\n",
    "    Y_true_val = scaler.inverse_transform(Y)\n",
    "    cost_val = masked_mae_tf(pred_val, Y_true_val, 0)#tf.reduce_mean(tf.abs(pred_val-Y_true_val))\n",
    "    #cost_val = tf.reduce_mean(tf.pow(pred_val-Y, 2))\n",
    "\n",
    "    pred_tes= gcn(X, weights, biases, batch_size,n_input,frequency, 3, n_output_vec)\n",
    "    #pred_tes = tf.nn.relu(pred_tes)\n",
    "    # mae\n",
    "    pred_tes = scaler.inverse_transform(pred_tes)\n",
    "    Y_true_tes = scaler.inverse_transform(Y)\n",
    "    '''\n",
    "    predic_res_re = tf.reshape(pred_tes, [Y_test.shape[0], sn, horizon])\n",
    "    test_Y_re = tf.reshape(Y_true_tes, [Y_test.shape[0], sn, horizon])\n",
    "    \n",
    "    cost_by_hor = []\n",
    "    for hor_i in range(horizon):\n",
    "        cost_by_hor.append(masked_mae_tf(predic_res_re[:, :, 0:hor_i], test_Y_re[:, :, 0:hor_i], 0))\n",
    "    '''\n",
    "    cost_tes = masked_mae_tf(pred_tes, Y_true_tes, 0)#tf.reduce_mean(tf.abs(pred_tes-Y_true_tes))\n",
    "    #rmse\n",
    "    #cost_tes = tf.reduce_mean(tf.pow(pred_tes-Y, 2))\n",
    "    # cross-entropy for classification\n",
    "    # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y_train))\n",
    "    # ratio = tf.abs(tf.reduce_sum(pred)-tf.reduce_sum(Y))/tf.reduce_sum(Y)\n",
    "    #zero = 0\n",
    "    #ratio = tf.reduce_mean(tf.divide(tf.where(tf.not_equal(Y, zero), np.abs(pred-Y), tf.zeros(Y.get_shape(), tf.float32)), tf.where(tf.not_equal(Y, zero), Y, tf.ones(Y.get_shape(), tf.float32))))\n",
    "    #optimizer = tf.train.RMSPropOptimizer(learning_rate, decay).minimize(cost)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #total_val_cost = []\n",
    "    #total_val_ratio = []\n",
    "\n",
    "    # learning start from \n",
    "\n",
    "    #index = daily_bike[(daily_bike['year'] == 2016) & (daily_bike['monthofyear'] == 1) & (daily_bike['dayofmonth'] == 1)].index.tolist()[0]\n",
    "    #A_hat = normalize_adj(corr_matrix_trips)\n",
    "    #print(A_hat)\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(num_train/batch_size)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                #print (Y_training[i*batch_size:(i+1)*batch_size,].size())\n",
    "                _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                      Y: Y_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                              keep_prob: keep})\n",
    "                #print (preds)\n",
    "                #print (trueval)\n",
    "                #print (\"Epoch:\", '%04d' % (epoch+1), \"batch: \", i, \"batch cost=\", \\\n",
    "                #    \"{:.9f}\".format(c))\n",
    "                #print ('here!!!!!!!!!!!!!!!!')\n",
    "                avg_cost += c * batch_size #/ total_batch \n",
    "                #Display logs per epoch step\n",
    "                \n",
    "            # rest part of training dataset\n",
    "            #num = num_train - total_batch*batch_size \n",
    "            if total_batch * batch_size != num_train:\n",
    "                _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[total_batch*batch_size:num_train,], \n",
    "                                          Y: Y_training[total_batch*batch_size:num_train,],\n",
    "                                                  keep_prob: keep})\n",
    "                avg_cost += c * (num_train - total_batch*batch_size)\n",
    "            \n",
    "            avg_cost = avg_cost / num_train\n",
    "            \n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Epoch:\", '%04d' % (epoch+1), \"Training MAE=\", \\\n",
    "                    \"{:.9f}\".format(avg_cost)) #np.sqrt(avg_cost)\n",
    "            # validation\n",
    "            c_val = sess.run([cost_val], feed_dict={X: X_val, Y: Y_val,  keep_prob:1})\n",
    "            print(\"Validation MAE: \", c_val[0])\n",
    "\n",
    "            c_tes, pred_tes1, A1, Y_true = sess.run([cost_tes, pred_tes, weights['A1'], Y_true_tes], feed_dict={X: X_test,Y: Y_test, keep_prob: 1})\n",
    "            print(\"Test MAE: \", c_tes)\n",
    "            #print(\"predic step: \", cost_by_hor)\n",
    "\n",
    "            if c_val[0] < best_val:\n",
    "                best_val = c_val[0]\n",
    "                #saver.save(sess, './bikesharing_graph_2_th_point1')\n",
    "                test_error = c_tes\n",
    "                traing_error = avg_cost#np.sqrt(avg_cost)\n",
    "                early_stop_k = 0 # reset to 0\n",
    "                #print (pred_tes1)\n",
    "                predic_res = pred_tes1\n",
    "                #predic_step = cost_by_hor\n",
    "\n",
    "            # early stopping\n",
    "            if c_val[0] >= best_val:\n",
    "                early_stop_k += 1\n",
    "\n",
    "            # threshold\n",
    "            if early_stop_k == early_stop_th:\n",
    "              #  print (\"early stopping...\")\n",
    "                break\n",
    "            \n",
    "\n",
    "        print(\"epoch is \", epoch)\n",
    "        print(\"training error is \", traing_error)\n",
    "        print(\"Optimization Finished! the lowest validation MAE is \", best_val)#(np.sqrt(best_val)))\n",
    "        print(\"The test MAE is \", test_error)#(np.sqrt(test_error)))\n",
    "    \n",
    "    test_Y = Y_test\n",
    "    test_error = np.sqrt(test_error)\n",
    "    return -best_val, predic_res,Y_true,test_error, A1#, predic_step#, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increas batch size from 100 to 1000: validation error 39.XX at about 100 epochs\n",
    "# batch size to 50: validation error 6.XX at about 100 epochs\n",
    "# add one more gcn layer: 8.XX\n",
    "# increase learning rate to 0.01, batch size is 100: 10.XX\n",
    "# decrease learning rate to 0.005, batch size is 100: 10.XX\n",
    "# decrease learning rate to 0.005, batch size is 50, two hidden levels (same matrix):\n",
    "    # 6.XX at 100th epoch\n",
    "# decrease learning rate to 0.005, batch size is 50, one hidden level, hidden 40:\n",
    "    # 6.XX at 100th epoch\n",
    "# decrease learning rate to 0.005, batch size is 50, one hidden level, hidden 20:\n",
    "    # drop much faster\n",
    "    # 6.XX can be get, but kind of overfitting at 100th epoch?\n",
    "# decrease learning rate to 0.005, batch size is 50, one hidden level, hidden 10:\n",
    "    # drop much faster\n",
    "    #  at 100th epoch\n",
    "# learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #reduce prediction horizon from 12 to 1, normalize the data:\n",
    "    # val: 3.16\n",
    "\n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #reduce prediction horizon from 12 to 1, normalize the data, normalize the symmetric adjacency matrix:\n",
    "    # train: 3.24, val: 3.42\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #reduce prediction horizon from 12 to 1, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # train: 3.08, val: 3.26\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #reduce prediction horizon from 12 to 1, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper\n",
    "    # train: 2.82, val: 2.75\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, one layer\n",
    "    # train: 4.08, val: 3.99, test: 4.29 \n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, two layers (same adjmatrix)\n",
    "    # train: 4.10, val: 4.19, test: 4.54 \n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, two layers (different adjmatrix)\n",
    "    # train: 3.52, val: 3.62, test: 4.03\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, two layers (different adjmatrix), bias set as a vector of same length\n",
    "    # train: 3.53, val: 3.58, test: 3.91 \n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # train: 3.36, val: 3.42, test: 3.82\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # last hidden size 10 to 20\n",
    "    # train: 3.33, val: 3.38, test: 3.71\n",
    "    # train: 3.24, val: 3.31, test: 3.66\n",
    "    # train: 3.22, val: 3.29, test: 3.66\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # three hidden size 5, 5, 10\n",
    "    # train: 3.5, val: 3.48, test: 3.82 \n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.005\n",
    "    # train: 3.23, val: 3.34, test: 3.74\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 200\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.01\n",
    "    # shuffle\n",
    "    # train: 3.19, val: 3.30, test: 3.67\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.01\n",
    "    # shuffle\n",
    "    # train: 3.15, val: 3.26, test: 3.61\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.01\n",
    "    # shuffle\n",
    "    # fully freedom adjacency matrix (not symmetric )\n",
    "    # train: , val: , test: \n",
    "    # training cost around 6.15, not decreasing anymore\n",
    "    \n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.005\n",
    "    # change gradient decent algorithm\n",
    "    # train: 3.19, val: 3.28, test: 3.67\n",
    "\n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 100\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.005\n",
    "    # change gradient decent algorithm\n",
    "    # keep: 0.8\n",
    "    # train: 3.30, val: 3.36, test: 3.78\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 200\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.01\n",
    "    # change gradient decent algorithm\n",
    "    # keep: 1\n",
    "    # train: 3.15, val: 3.27, test: 3.63\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch from 50 to 200\n",
    "    # three hidden size 10, 10, 20\n",
    "    # learning rate: 0.01\n",
    "    # change gradient decent algorithm\n",
    "    # keep: 1\n",
    "    # bias not the same size\n",
    "    # decreasing slow\n",
    "\n",
    "    \n",
    "# increase sample, learning rate to 0.01, batch size is 50, one hidden level, hidden 10, \n",
    "    #prediction horizon 12, normalize the data, normalize the symmetric adjacency matrix, \n",
    "    # remove \"0\" ground truths as the paper, three layers (three adjmatrix), bias set as a vector of same length\n",
    "    # batch size 200\n",
    "    # second size 10 to 10\n",
    "    # last hidden size 10 to 30\n",
    "    # set the same size may lead to indecreasing cost (need to increase batch size)\n",
    "    # train: 3.05, val: 3.22, test: 3.73\n",
    "\n",
    "# batch size 150 really slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training MAE= 123.990211574\n",
      "Validation MAE:  20.14675\n",
      "Test MAE:  21.65484\n",
      "Epoch: 0002 Training MAE= 13.750733587\n",
      "Validation MAE:  11.100066\n",
      "Test MAE:  12.026818\n",
      "Epoch: 0003 Training MAE= 9.664486127\n",
      "Validation MAE:  8.867375\n",
      "Test MAE:  9.639506\n",
      "Epoch: 0004 Training MAE= 8.052066818\n",
      "Validation MAE:  7.649607\n",
      "Test MAE:  8.391342\n",
      "Epoch: 0005 Training MAE= 7.177701311\n",
      "Validation MAE:  7.083384\n",
      "Test MAE:  7.79229\n",
      "Epoch: 0006 Training MAE= 6.813359119\n",
      "Validation MAE:  6.824097\n",
      "Test MAE:  7.522514\n",
      "Epoch: 0007 Training MAE= 6.631802164\n",
      "Validation MAE:  6.667398\n",
      "Test MAE:  7.3559117\n",
      "Epoch: 0008 Training MAE= 6.505465001\n",
      "Validation MAE:  6.533195\n",
      "Test MAE:  7.213554\n",
      "Epoch: 0009 Training MAE= 6.395922012\n",
      "Validation MAE:  6.4169025\n",
      "Test MAE:  7.0872974\n",
      "Epoch: 0010 Training MAE= 6.301142514\n",
      "Validation MAE:  6.3131833\n",
      "Test MAE:  6.9805727\n",
      "Epoch: 0011 Training MAE= 6.210150160\n",
      "Validation MAE:  6.193912\n",
      "Test MAE:  6.862125\n",
      "Epoch: 0012 Training MAE= 6.105813477\n",
      "Validation MAE:  6.051965\n",
      "Test MAE:  6.712725\n",
      "Epoch: 0013 Training MAE= 5.978975642\n",
      "Validation MAE:  5.8971915\n",
      "Test MAE:  6.539324\n",
      "Epoch: 0014 Training MAE= 5.848310594\n",
      "Validation MAE:  5.77389\n",
      "Test MAE:  6.3912206\n",
      "Epoch: 0015 Training MAE= 5.737667715\n",
      "Validation MAE:  5.6625376\n",
      "Test MAE:  6.2564726\n",
      "Epoch: 0016 Training MAE= 5.653154773\n",
      "Validation MAE:  5.578488\n",
      "Test MAE:  6.152307\n",
      "Epoch: 0017 Training MAE= 5.591741411\n",
      "Validation MAE:  5.5280447\n",
      "Test MAE:  6.0762215\n",
      "Epoch: 0018 Training MAE= 5.544022607\n",
      "Validation MAE:  5.4740157\n",
      "Test MAE:  6.013035\n",
      "Epoch: 0019 Training MAE= 5.504774191\n",
      "Validation MAE:  5.428318\n",
      "Test MAE:  5.9601965\n",
      "Epoch: 0020 Training MAE= 5.476516514\n",
      "Validation MAE:  5.4084783\n",
      "Test MAE:  5.941712\n",
      "Epoch: 0021 Training MAE= 5.430435634\n",
      "Validation MAE:  5.350029\n",
      "Test MAE:  5.871286\n",
      "Epoch: 0022 Training MAE= 5.372313041\n",
      "Validation MAE:  5.294225\n",
      "Test MAE:  5.8097963\n",
      "Epoch: 0023 Training MAE= 5.314873044\n",
      "Validation MAE:  5.247251\n",
      "Test MAE:  5.7487564\n",
      "Epoch: 0024 Training MAE= 5.267661747\n",
      "Validation MAE:  5.1970644\n",
      "Test MAE:  5.6925225\n",
      "Epoch: 0025 Training MAE= 5.219621177\n",
      "Validation MAE:  5.1497235\n",
      "Test MAE:  5.6376863\n",
      "Epoch: 0026 Training MAE= 5.173453178\n",
      "Validation MAE:  5.0980444\n",
      "Test MAE:  5.5779257\n",
      "Epoch: 0027 Training MAE= 5.123000712\n",
      "Validation MAE:  5.0436335\n",
      "Test MAE:  5.520536\n",
      "Epoch: 0028 Training MAE= 5.062703868\n",
      "Validation MAE:  4.987002\n",
      "Test MAE:  5.4585714\n",
      "Epoch: 0029 Training MAE= 5.022761356\n",
      "Validation MAE:  4.917506\n",
      "Test MAE:  5.3938\n",
      "Epoch: 0030 Training MAE= 4.949680100\n",
      "Validation MAE:  4.872859\n",
      "Test MAE:  5.341898\n",
      "Epoch: 0031 Training MAE= 4.912015769\n",
      "Validation MAE:  4.8200507\n",
      "Test MAE:  5.283761\n",
      "Epoch: 0032 Training MAE= 4.852569629\n",
      "Validation MAE:  4.779154\n",
      "Test MAE:  5.2445326\n",
      "Epoch: 0033 Training MAE= 4.811316085\n",
      "Validation MAE:  4.7398295\n",
      "Test MAE:  5.2020535\n",
      "Epoch: 0034 Training MAE= 4.763033619\n",
      "Validation MAE:  4.6935234\n",
      "Test MAE:  5.150131\n",
      "Epoch: 0035 Training MAE= 4.717904442\n",
      "Validation MAE:  4.647423\n",
      "Test MAE:  5.093667\n",
      "Epoch: 0036 Training MAE= 4.668880333\n",
      "Validation MAE:  4.611254\n",
      "Test MAE:  5.055845\n",
      "Epoch: 0037 Training MAE= 4.623673540\n",
      "Validation MAE:  4.567732\n",
      "Test MAE:  5.0110083\n",
      "Epoch: 0038 Training MAE= 4.580125711\n",
      "Validation MAE:  4.5228753\n",
      "Test MAE:  4.962227\n",
      "Epoch: 0039 Training MAE= 4.536985313\n",
      "Validation MAE:  4.482284\n",
      "Test MAE:  4.9221654\n",
      "Epoch: 0040 Training MAE= 4.492158919\n",
      "Validation MAE:  4.4354525\n",
      "Test MAE:  4.869111\n",
      "Epoch: 0041 Training MAE= 4.441608128\n",
      "Validation MAE:  4.3891263\n",
      "Test MAE:  4.817812\n",
      "Epoch: 0042 Training MAE= 4.391804681\n",
      "Validation MAE:  4.3326793\n",
      "Test MAE:  4.7632146\n",
      "Epoch: 0043 Training MAE= 4.348652338\n",
      "Validation MAE:  4.29027\n",
      "Test MAE:  4.719116\n",
      "Epoch: 0044 Training MAE= 4.310700807\n",
      "Validation MAE:  4.2561107\n",
      "Test MAE:  4.6819496\n",
      "Epoch: 0045 Training MAE= 4.279084035\n",
      "Validation MAE:  4.229823\n",
      "Test MAE:  4.654652\n",
      "Epoch: 0046 Training MAE= 4.249783492\n",
      "Validation MAE:  4.2133713\n",
      "Test MAE:  4.631794\n",
      "Epoch: 0047 Training MAE= 4.225325447\n",
      "Validation MAE:  4.188586\n",
      "Test MAE:  4.6088243\n",
      "Epoch: 0048 Training MAE= 4.202968523\n",
      "Validation MAE:  4.172536\n",
      "Test MAE:  4.5923243\n",
      "Epoch: 0049 Training MAE= 4.183143952\n",
      "Validation MAE:  4.155274\n",
      "Test MAE:  4.575183\n",
      "Epoch: 0050 Training MAE= 4.164529520\n",
      "Validation MAE:  4.141685\n",
      "Test MAE:  4.5626006\n",
      "Epoch: 0051 Training MAE= 4.148173496\n",
      "Validation MAE:  4.126846\n",
      "Test MAE:  4.54994\n",
      "Epoch: 0052 Training MAE= 4.132727793\n",
      "Validation MAE:  4.113548\n",
      "Test MAE:  4.534907\n",
      "Epoch: 0053 Training MAE= 4.116547906\n",
      "Validation MAE:  4.1120753\n",
      "Test MAE:  4.5361357\n",
      "Epoch: 0054 Training MAE= 4.104712718\n",
      "Validation MAE:  4.090818\n",
      "Test MAE:  4.5070515\n",
      "Epoch: 0055 Training MAE= 4.087176395\n",
      "Validation MAE:  4.108102\n",
      "Test MAE:  4.5260177\n",
      "Epoch: 0056 Training MAE= 4.075626646\n",
      "Validation MAE:  4.090794\n",
      "Test MAE:  4.5130024\n",
      "Epoch: 0057 Training MAE= 4.061365912\n",
      "Validation MAE:  4.097195\n",
      "Test MAE:  4.521551\n",
      "Epoch: 0058 Training MAE= 4.045530879\n",
      "Validation MAE:  4.08994\n",
      "Test MAE:  4.5157313\n",
      "Epoch: 0059 Training MAE= 4.034253466\n",
      "Validation MAE:  4.0535045\n",
      "Test MAE:  4.4824595\n",
      "Epoch: 0060 Training MAE= 4.029373112\n",
      "Validation MAE:  4.0271525\n",
      "Test MAE:  4.4538727\n",
      "Epoch: 0061 Training MAE= 4.042510813\n",
      "Validation MAE:  4.016422\n",
      "Test MAE:  4.4441957\n",
      "Epoch: 0062 Training MAE= 4.012793033\n",
      "Validation MAE:  4.0282187\n",
      "Test MAE:  4.451343\n",
      "Epoch: 0063 Training MAE= 3.991403176\n",
      "Validation MAE:  4.0257087\n",
      "Test MAE:  4.4560905\n",
      "Epoch: 0064 Training MAE= 3.985586227\n",
      "Validation MAE:  4.0065784\n",
      "Test MAE:  4.442318\n",
      "Epoch: 0065 Training MAE= 3.981041410\n",
      "Validation MAE:  4.01628\n",
      "Test MAE:  4.450933\n",
      "Epoch: 0066 Training MAE= 3.972519772\n",
      "Validation MAE:  4.0027766\n",
      "Test MAE:  4.4291945\n",
      "Epoch: 0067 Training MAE= 3.955170976\n",
      "Validation MAE:  3.9644463\n",
      "Test MAE:  4.3923707\n",
      "Epoch: 0068 Training MAE= 3.953733525\n",
      "Validation MAE:  3.982464\n",
      "Test MAE:  4.425291\n",
      "Epoch: 0069 Training MAE= 3.942887375\n",
      "Validation MAE:  3.9691427\n",
      "Test MAE:  4.4150734\n",
      "Epoch: 0070 Training MAE= 3.939610192\n",
      "Validation MAE:  3.9761589\n",
      "Test MAE:  4.4171276\n",
      "Epoch: 0071 Training MAE= 3.918564324\n",
      "Validation MAE:  3.9230657\n",
      "Test MAE:  4.359381\n",
      "Epoch: 0072 Training MAE= 3.915171651\n",
      "Validation MAE:  3.9234993\n",
      "Test MAE:  4.357939\n",
      "Epoch: 0073 Training MAE= 3.924099554\n",
      "Validation MAE:  3.9605784\n",
      "Test MAE:  4.4084506\n",
      "Epoch: 0074 Training MAE= 3.894807933\n",
      "Validation MAE:  3.9048266\n",
      "Test MAE:  4.346641\n",
      "Epoch: 0075 Training MAE= 3.914790267\n",
      "Validation MAE:  3.933622\n",
      "Test MAE:  4.393347\n",
      "Epoch: 0076 Training MAE= 3.887830477\n",
      "Validation MAE:  3.8997114\n",
      "Test MAE:  4.341265\n",
      "Epoch: 0077 Training MAE= 3.897075666\n",
      "Validation MAE:  3.930607\n",
      "Test MAE:  4.3845487\n",
      "Epoch: 0078 Training MAE= 3.879631655\n",
      "Validation MAE:  3.907176\n",
      "Test MAE:  4.3672905\n",
      "Epoch: 0079 Training MAE= 3.867979261\n",
      "Validation MAE:  3.8865635\n",
      "Test MAE:  4.334379\n",
      "Epoch: 0080 Training MAE= 3.879691925\n",
      "Validation MAE:  3.9201937\n",
      "Test MAE:  4.3832626\n",
      "Epoch: 0081 Training MAE= 3.861686953\n",
      "Validation MAE:  3.9122822\n",
      "Test MAE:  4.376433\n",
      "Epoch: 0082 Training MAE= 3.862157763\n",
      "Validation MAE:  3.8688543\n",
      "Test MAE:  4.3280597\n",
      "Epoch: 0083 Training MAE= 3.848344784\n",
      "Validation MAE:  3.8898242\n",
      "Test MAE:  4.3579965\n",
      "Epoch: 0084 Training MAE= 3.848781303\n",
      "Validation MAE:  3.8669078\n",
      "Test MAE:  4.318776\n",
      "Epoch: 0085 Training MAE= 3.839275956\n",
      "Validation MAE:  3.9149165\n",
      "Test MAE:  4.3584294\n",
      "Epoch: 0086 Training MAE= 3.843562963\n",
      "Validation MAE:  3.8894691\n",
      "Test MAE:  4.367774\n",
      "Epoch: 0087 Training MAE= 3.841263088\n",
      "Validation MAE:  3.8722427\n",
      "Test MAE:  4.352724\n",
      "Epoch: 0088 Training MAE= 3.826161737\n",
      "Validation MAE:  3.8363411\n",
      "Test MAE:  4.3116484\n",
      "Epoch: 0089 Training MAE= 3.834422041\n",
      "Validation MAE:  3.8379397\n",
      "Test MAE:  4.307826\n",
      "Epoch: 0090 Training MAE= 3.820681111\n",
      "Validation MAE:  3.8480058\n",
      "Test MAE:  4.326132\n",
      "Epoch: 0091 Training MAE= 3.822944982\n",
      "Validation MAE:  3.8545904\n",
      "Test MAE:  4.332337\n",
      "Epoch: 0092 Training MAE= 3.810054462\n",
      "Validation MAE:  3.839007\n",
      "Test MAE:  4.2981677\n",
      "Epoch: 0093 Training MAE= 3.818360288\n",
      "Validation MAE:  3.8537452\n",
      "Test MAE:  4.339853\n",
      "Epoch: 0094 Training MAE= 3.806566571\n",
      "Validation MAE:  3.8275156\n",
      "Test MAE:  4.3102813\n",
      "Epoch: 0095 Training MAE= 3.799999764\n",
      "Validation MAE:  3.8345966\n",
      "Test MAE:  4.305205\n",
      "Epoch: 0096 Training MAE= 3.800541813\n",
      "Validation MAE:  3.854408\n",
      "Test MAE:  4.3400917\n",
      "Epoch: 0097 Training MAE= 3.799157414\n",
      "Validation MAE:  3.8401625\n",
      "Test MAE:  4.3080044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0098 Training MAE= 3.798085485\n",
      "Validation MAE:  3.8476582\n",
      "Test MAE:  4.337153\n",
      "Epoch: 0099 Training MAE= 3.788126507\n",
      "Validation MAE:  3.8061311\n",
      "Test MAE:  4.295178\n",
      "Epoch: 0100 Training MAE= 3.792393641\n",
      "Validation MAE:  3.804786\n",
      "Test MAE:  4.281302\n",
      "Epoch: 0101 Training MAE= 3.784581155\n",
      "Validation MAE:  3.7957048\n",
      "Test MAE:  4.2782354\n",
      "Epoch: 0102 Training MAE= 3.790763546\n",
      "Validation MAE:  3.831224\n",
      "Test MAE:  4.3190036\n",
      "Epoch: 0103 Training MAE= 3.777169967\n",
      "Validation MAE:  3.8291726\n",
      "Test MAE:  4.3206277\n",
      "Epoch: 0104 Training MAE= 3.772030086\n",
      "Validation MAE:  3.7946415\n",
      "Test MAE:  4.2828126\n",
      "Epoch: 0105 Training MAE= 3.779356860\n",
      "Validation MAE:  3.8006816\n",
      "Test MAE:  4.295173\n",
      "Epoch: 0106 Training MAE= 3.767780369\n",
      "Validation MAE:  3.7988946\n",
      "Test MAE:  4.2943954\n",
      "Epoch: 0107 Training MAE= 3.762665841\n",
      "Validation MAE:  3.814115\n",
      "Test MAE:  4.2925777\n",
      "Epoch: 0108 Training MAE= 3.766523892\n",
      "Validation MAE:  3.77986\n",
      "Test MAE:  4.26909\n",
      "Epoch: 0109 Training MAE= 3.772756454\n",
      "Validation MAE:  3.7935395\n",
      "Test MAE:  4.285965\n",
      "Epoch: 0110 Training MAE= 3.753291166\n",
      "Validation MAE:  3.7797494\n",
      "Test MAE:  4.274227\n",
      "Epoch: 0111 Training MAE= 3.764227098\n",
      "Validation MAE:  3.8066204\n",
      "Test MAE:  4.3032193\n",
      "Epoch: 0112 Training MAE= 3.752468753\n",
      "Validation MAE:  3.7821512\n",
      "Test MAE:  4.276418\n",
      "Epoch: 0113 Training MAE= 3.760408431\n",
      "Validation MAE:  3.783431\n",
      "Test MAE:  4.2799306\n",
      "Epoch: 0114 Training MAE= 3.747165383\n",
      "Validation MAE:  3.7973\n",
      "Test MAE:  4.2830143\n",
      "Epoch: 0115 Training MAE= 3.753332689\n",
      "Validation MAE:  3.791825\n",
      "Test MAE:  4.296571\n",
      "Epoch: 0116 Training MAE= 3.750534802\n",
      "Validation MAE:  3.7814283\n",
      "Test MAE:  4.281963\n",
      "Epoch: 0117 Training MAE= 3.746167417\n",
      "Validation MAE:  3.7815387\n",
      "Test MAE:  4.281902\n",
      "Epoch: 0118 Training MAE= 3.745251936\n",
      "Validation MAE:  3.7913225\n",
      "Test MAE:  4.2927423\n",
      "Epoch: 0119 Training MAE= 3.730147588\n",
      "Validation MAE:  3.7557576\n",
      "Test MAE:  4.251182\n",
      "Epoch: 0120 Training MAE= 3.748504556\n",
      "Validation MAE:  3.7909665\n",
      "Test MAE:  4.288697\n",
      "Epoch: 0121 Training MAE= 3.735705800\n",
      "Validation MAE:  3.7531476\n",
      "Test MAE:  4.2538533\n",
      "Epoch: 0122 Training MAE= 3.748018605\n",
      "Validation MAE:  3.7629402\n",
      "Test MAE:  4.2709117\n",
      "Epoch: 0123 Training MAE= 3.723345887\n",
      "Validation MAE:  3.759569\n",
      "Test MAE:  4.2561936\n",
      "Epoch: 0124 Training MAE= 3.734721604\n",
      "Validation MAE:  3.755344\n",
      "Test MAE:  4.2649846\n",
      "Epoch: 0125 Training MAE= 3.728604277\n",
      "Validation MAE:  3.7653491\n",
      "Test MAE:  4.272536\n",
      "Epoch: 0126 Training MAE= 3.724314691\n",
      "Validation MAE:  3.7543256\n",
      "Test MAE:  4.265912\n",
      "Epoch: 0127 Training MAE= 3.724139331\n",
      "Validation MAE:  3.763611\n",
      "Test MAE:  4.2702303\n",
      "Epoch: 0128 Training MAE= 3.722669047\n",
      "Validation MAE:  3.775408\n",
      "Test MAE:  4.284331\n",
      "Epoch: 0129 Training MAE= 3.714141637\n",
      "Validation MAE:  3.7341514\n",
      "Test MAE:  4.242768\n",
      "Epoch: 0130 Training MAE= 3.733018795\n",
      "Validation MAE:  3.7404616\n",
      "Test MAE:  4.2609725\n",
      "Epoch: 0131 Training MAE= 3.708498676\n",
      "Validation MAE:  3.7239115\n",
      "Test MAE:  4.238137\n",
      "Epoch: 0132 Training MAE= 3.722411312\n",
      "Validation MAE:  3.7514725\n",
      "Test MAE:  4.2691627\n",
      "Epoch: 0133 Training MAE= 3.702631572\n",
      "Validation MAE:  3.71877\n",
      "Test MAE:  4.232345\n",
      "Epoch: 0134 Training MAE= 3.722469258\n",
      "Validation MAE:  3.7360413\n",
      "Test MAE:  4.2512116\n",
      "Epoch: 0135 Training MAE= 3.703129545\n",
      "Validation MAE:  3.7230604\n",
      "Test MAE:  4.23253\n",
      "Epoch: 0136 Training MAE= 3.708257904\n",
      "Validation MAE:  3.7394316\n",
      "Test MAE:  4.2603664\n",
      "Epoch: 0137 Training MAE= 3.700591582\n",
      "Validation MAE:  3.7093575\n",
      "Test MAE:  4.223909\n",
      "Epoch: 0138 Training MAE= 3.706959267\n",
      "Validation MAE:  3.7269077\n",
      "Test MAE:  4.248525\n",
      "Epoch: 0139 Training MAE= 3.696093280\n",
      "Validation MAE:  3.7155113\n",
      "Test MAE:  4.239948\n",
      "Epoch: 0140 Training MAE= 3.703381270\n",
      "Validation MAE:  3.7194116\n",
      "Test MAE:  4.2240577\n",
      "Epoch: 0141 Training MAE= 3.687568457\n",
      "Validation MAE:  3.7376492\n",
      "Test MAE:  4.228761\n",
      "Epoch: 0142 Training MAE= 3.696213015\n",
      "Validation MAE:  3.7049887\n",
      "Test MAE:  4.2163773\n",
      "Epoch: 0143 Training MAE= 3.693466183\n",
      "Validation MAE:  3.7022486\n",
      "Test MAE:  4.219958\n",
      "Epoch: 0144 Training MAE= 3.689576686\n",
      "Validation MAE:  3.7206292\n",
      "Test MAE:  4.2195005\n",
      "Epoch: 0145 Training MAE= 3.685766058\n",
      "Validation MAE:  3.6943119\n",
      "Test MAE:  4.1990347\n",
      "Epoch: 0146 Training MAE= 3.681115092\n",
      "Validation MAE:  3.6882958\n",
      "Test MAE:  4.197425\n",
      "Epoch: 0147 Training MAE= 3.690516427\n",
      "Validation MAE:  3.7213778\n",
      "Test MAE:  4.2376094\n",
      "Epoch: 0148 Training MAE= 3.692126642\n",
      "Validation MAE:  3.6970005\n",
      "Test MAE:  4.2092385\n",
      "Epoch: 0149 Training MAE= 3.670588617\n",
      "Validation MAE:  3.6753893\n",
      "Test MAE:  4.19222\n",
      "Epoch: 0150 Training MAE= 3.677497751\n",
      "Validation MAE:  3.7093432\n",
      "Test MAE:  4.22829\n",
      "Epoch: 0151 Training MAE= 3.679553497\n",
      "Validation MAE:  3.683982\n",
      "Test MAE:  4.2142377\n",
      "Epoch: 0152 Training MAE= 3.673725353\n",
      "Validation MAE:  3.6797843\n",
      "Test MAE:  4.195101\n",
      "Epoch: 0153 Training MAE= 3.672002423\n",
      "Validation MAE:  3.682803\n",
      "Test MAE:  4.2100663\n",
      "Epoch: 0154 Training MAE= 3.668427749\n",
      "Validation MAE:  3.6812994\n",
      "Test MAE:  4.19921\n",
      "Epoch: 0155 Training MAE= 3.667556016\n",
      "Validation MAE:  3.696939\n",
      "Test MAE:  4.1983724\n",
      "Epoch: 0156 Training MAE= 3.666922030\n",
      "Validation MAE:  3.6680646\n",
      "Test MAE:  4.1946554\n",
      "Epoch: 0157 Training MAE= 3.664968355\n",
      "Validation MAE:  3.6798644\n",
      "Test MAE:  4.1987576\n",
      "Epoch: 0158 Training MAE= 3.664622573\n",
      "Validation MAE:  3.6612353\n",
      "Test MAE:  4.1836476\n",
      "Epoch: 0159 Training MAE= 3.663767593\n",
      "Validation MAE:  3.6680467\n",
      "Test MAE:  4.1886926\n",
      "Epoch: 0160 Training MAE= 3.660183651\n",
      "Validation MAE:  3.6713963\n",
      "Test MAE:  4.1855683\n",
      "Epoch: 0161 Training MAE= 3.654715949\n",
      "Validation MAE:  3.682315\n",
      "Test MAE:  4.192007\n",
      "Epoch: 0162 Training MAE= 3.658757168\n",
      "Validation MAE:  3.7357383\n",
      "Test MAE:  4.253876\n",
      "Epoch: 0163 Training MAE= 3.654961287\n",
      "Validation MAE:  3.653288\n",
      "Test MAE:  4.1751657\n",
      "Epoch: 0164 Training MAE= 3.647433473\n",
      "Validation MAE:  3.68314\n",
      "Test MAE:  4.2022834\n",
      "Epoch: 0165 Training MAE= 3.647069923\n",
      "Validation MAE:  3.64876\n",
      "Test MAE:  4.1784782\n",
      "Epoch: 0166 Training MAE= 3.652808479\n",
      "Validation MAE:  3.6521883\n",
      "Test MAE:  4.1711793\n",
      "Epoch: 0167 Training MAE= 3.645130728\n",
      "Validation MAE:  3.6493561\n",
      "Test MAE:  4.1726923\n",
      "Epoch: 0168 Training MAE= 3.644692190\n",
      "Validation MAE:  3.677106\n",
      "Test MAE:  4.191591\n",
      "Epoch: 0169 Training MAE= 3.642046690\n",
      "Validation MAE:  3.6653254\n",
      "Test MAE:  4.1807985\n",
      "Epoch: 0170 Training MAE= 3.646183387\n",
      "Validation MAE:  3.64086\n",
      "Test MAE:  4.1785846\n",
      "Epoch: 0171 Training MAE= 3.659308751\n",
      "Validation MAE:  3.688082\n",
      "Test MAE:  4.219115\n",
      "Epoch: 0172 Training MAE= 3.631297071\n",
      "Validation MAE:  3.638395\n",
      "Test MAE:  4.16505\n",
      "Epoch: 0173 Training MAE= 3.632757958\n",
      "Validation MAE:  3.6350946\n",
      "Test MAE:  4.168172\n",
      "Epoch: 0174 Training MAE= 3.624428144\n",
      "Validation MAE:  3.6343167\n",
      "Test MAE:  4.1653023\n",
      "Epoch: 0175 Training MAE= 3.634680892\n",
      "Validation MAE:  3.6367478\n",
      "Test MAE:  4.1754932\n",
      "Epoch: 0176 Training MAE= 3.639296011\n",
      "Validation MAE:  3.6553495\n",
      "Test MAE:  4.1871977\n",
      "Epoch: 0177 Training MAE= 3.628463370\n",
      "Validation MAE:  3.6292634\n",
      "Test MAE:  4.1588783\n",
      "Epoch: 0178 Training MAE= 3.624606770\n",
      "Validation MAE:  3.658534\n",
      "Test MAE:  4.1951866\n",
      "Epoch: 0179 Training MAE= 3.629586084\n",
      "Validation MAE:  3.6312788\n",
      "Test MAE:  4.167577\n",
      "Epoch: 0180 Training MAE= 3.625271636\n",
      "Validation MAE:  3.6722622\n",
      "Test MAE:  4.200126\n",
      "Epoch: 0181 Training MAE= 3.621936973\n",
      "Validation MAE:  3.6357563\n",
      "Test MAE:  4.16759\n",
      "Epoch: 0182 Training MAE= 3.624659213\n",
      "Validation MAE:  3.6559732\n",
      "Test MAE:  4.186507\n",
      "Epoch: 0183 Training MAE= 3.646491890\n",
      "Validation MAE:  3.6603413\n",
      "Test MAE:  4.194258\n",
      "Epoch: 0184 Training MAE= 3.635596321\n",
      "Validation MAE:  3.6313095\n",
      "Test MAE:  4.1653743\n",
      "Epoch: 0185 Training MAE= 3.635573840\n",
      "Validation MAE:  3.6346576\n",
      "Test MAE:  4.1703787\n",
      "Epoch: 0186 Training MAE= 3.616629268\n",
      "Validation MAE:  3.6261814\n",
      "Test MAE:  4.1591845\n",
      "Epoch: 0187 Training MAE= 3.607911107\n",
      "Validation MAE:  3.6174142\n",
      "Test MAE:  4.152247\n",
      "Epoch: 0188 Training MAE= 3.610866600\n",
      "Validation MAE:  3.634593\n",
      "Test MAE:  4.1674366\n",
      "Epoch: 0189 Training MAE= 3.616725295\n",
      "Validation MAE:  3.6216323\n",
      "Test MAE:  4.1555777\n",
      "Epoch: 0190 Training MAE= 3.618340294\n",
      "Validation MAE:  3.6079853\n",
      "Test MAE:  4.1397943\n",
      "Epoch: 0191 Training MAE= 3.619724764\n",
      "Validation MAE:  3.6473105\n",
      "Test MAE:  4.1844144\n",
      "Epoch: 0192 Training MAE= 3.603740941\n",
      "Validation MAE:  3.6014543\n",
      "Test MAE:  4.138586\n",
      "Epoch: 0193 Training MAE= 3.610459765\n",
      "Validation MAE:  3.6352744\n",
      "Test MAE:  4.171961\n",
      "Epoch: 0194 Training MAE= 3.604242019\n",
      "Validation MAE:  3.6272423\n",
      "Test MAE:  4.1659617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0195 Training MAE= 3.598214961\n",
      "Validation MAE:  3.6017752\n",
      "Test MAE:  4.139998\n",
      "Epoch: 0196 Training MAE= 3.598769876\n",
      "Validation MAE:  3.607224\n",
      "Test MAE:  4.1434846\n",
      "Epoch: 0197 Training MAE= 3.599005505\n",
      "Validation MAE:  3.6210854\n",
      "Test MAE:  4.1631556\n",
      "Epoch: 0198 Training MAE= 3.599016434\n",
      "Validation MAE:  3.6246955\n",
      "Test MAE:  4.165989\n",
      "Epoch: 0199 Training MAE= 3.590879836\n",
      "Validation MAE:  3.5946543\n",
      "Test MAE:  4.1341076\n",
      "Epoch: 0200 Training MAE= 3.583844733\n",
      "Validation MAE:  3.5954597\n",
      "Test MAE:  4.131694\n",
      "Epoch: 0201 Training MAE= 3.583103460\n",
      "Validation MAE:  3.5910585\n",
      "Test MAE:  4.1324897\n",
      "Epoch: 0202 Training MAE= 3.589432724\n",
      "Validation MAE:  3.6073768\n",
      "Test MAE:  4.1439457\n",
      "Epoch: 0203 Training MAE= 3.583529048\n",
      "Validation MAE:  3.5909793\n",
      "Test MAE:  4.1332765\n",
      "Epoch: 0204 Training MAE= 3.594673559\n",
      "Validation MAE:  3.5963864\n",
      "Test MAE:  4.1363883\n",
      "Epoch: 0205 Training MAE= 3.591215365\n",
      "Validation MAE:  3.5906546\n",
      "Test MAE:  4.126217\n",
      "Epoch: 0206 Training MAE= 3.582353206\n",
      "Validation MAE:  3.5881126\n",
      "Test MAE:  4.126275\n",
      "Epoch: 0207 Training MAE= 3.583086763\n",
      "Validation MAE:  3.5875347\n",
      "Test MAE:  4.121451\n",
      "Epoch: 0208 Training MAE= 3.591514418\n",
      "Validation MAE:  3.607541\n",
      "Test MAE:  4.1436644\n",
      "Epoch: 0209 Training MAE= 3.593969732\n",
      "Validation MAE:  3.5920746\n",
      "Test MAE:  4.1421924\n",
      "Epoch: 0210 Training MAE= 3.578365673\n",
      "Validation MAE:  3.6263237\n",
      "Test MAE:  4.158504\n",
      "Epoch: 0211 Training MAE= 3.585435025\n",
      "Validation MAE:  3.579009\n",
      "Test MAE:  4.1178837\n",
      "Epoch: 0212 Training MAE= 3.575920111\n",
      "Validation MAE:  3.6042547\n",
      "Test MAE:  4.1319437\n",
      "Epoch: 0213 Training MAE= 3.574709362\n",
      "Validation MAE:  3.590856\n",
      "Test MAE:  4.123439\n",
      "Epoch: 0214 Training MAE= 3.570960252\n",
      "Validation MAE:  3.5944643\n",
      "Test MAE:  4.129999\n",
      "Epoch: 0215 Training MAE= 3.571622378\n",
      "Validation MAE:  3.5763767\n",
      "Test MAE:  4.1149173\n",
      "Epoch: 0216 Training MAE= 3.567396702\n",
      "Validation MAE:  3.5722716\n",
      "Test MAE:  4.1125746\n",
      "Epoch: 0217 Training MAE= 3.564202046\n",
      "Validation MAE:  3.5638547\n",
      "Test MAE:  4.1027484\n",
      "Epoch: 0218 Training MAE= 3.563634649\n",
      "Validation MAE:  3.5720441\n",
      "Test MAE:  4.1070805\n",
      "Epoch: 0219 Training MAE= 3.561926246\n",
      "Validation MAE:  3.5604475\n",
      "Test MAE:  4.0995502\n",
      "Epoch: 0220 Training MAE= 3.562683441\n",
      "Validation MAE:  3.5621533\n",
      "Test MAE:  4.0984445\n",
      "Epoch: 0221 Training MAE= 3.561018662\n",
      "Validation MAE:  3.5600207\n",
      "Test MAE:  4.097655\n",
      "Epoch: 0222 Training MAE= 3.559540996\n",
      "Validation MAE:  3.5596807\n",
      "Test MAE:  4.09817\n",
      "Epoch: 0223 Training MAE= 3.559365521\n",
      "Validation MAE:  3.560248\n",
      "Test MAE:  4.096329\n",
      "Epoch: 0224 Training MAE= 3.564917381\n",
      "Validation MAE:  3.5774736\n",
      "Test MAE:  4.119265\n",
      "Epoch: 0225 Training MAE= 3.559708128\n",
      "Validation MAE:  3.589038\n",
      "Test MAE:  4.1346383\n",
      "Epoch: 0226 Training MAE= 3.554838559\n",
      "Validation MAE:  3.6034465\n",
      "Test MAE:  4.14644\n",
      "Epoch: 0227 Training MAE= 3.560777347\n",
      "Validation MAE:  3.6011267\n",
      "Test MAE:  4.150569\n",
      "Epoch: 0228 Training MAE= 3.560604084\n",
      "Validation MAE:  3.562719\n",
      "Test MAE:  4.1129556\n",
      "Epoch: 0229 Training MAE= 3.552201682\n",
      "Validation MAE:  3.5633318\n",
      "Test MAE:  4.1092405\n",
      "Epoch: 0230 Training MAE= 3.551266468\n",
      "Validation MAE:  3.567873\n",
      "Test MAE:  4.1156974\n",
      "Epoch: 0231 Training MAE= 3.549909425\n",
      "Validation MAE:  3.560884\n",
      "Test MAE:  4.1056423\n",
      "Epoch: 0232 Training MAE= 3.548332890\n",
      "Validation MAE:  3.5644362\n",
      "Test MAE:  4.107088\n",
      "Epoch: 0233 Training MAE= 3.547101954\n",
      "Validation MAE:  3.5827703\n",
      "Test MAE:  4.1283355\n",
      "Epoch: 0234 Training MAE= 3.547382581\n",
      "Validation MAE:  3.568274\n",
      "Test MAE:  4.114191\n",
      "Epoch: 0235 Training MAE= 3.546590243\n",
      "Validation MAE:  3.5842974\n",
      "Test MAE:  4.1290755\n",
      "Epoch: 0236 Training MAE= 3.546333492\n",
      "Validation MAE:  3.5493505\n",
      "Test MAE:  4.094484\n",
      "Epoch: 0237 Training MAE= 3.541750753\n",
      "Validation MAE:  3.5769737\n",
      "Test MAE:  4.1246834\n",
      "Epoch: 0238 Training MAE= 3.536759757\n",
      "Validation MAE:  3.622641\n",
      "Test MAE:  4.175575\n",
      "Epoch: 0239 Training MAE= 3.578924220\n",
      "Validation MAE:  3.597699\n",
      "Test MAE:  4.158471\n",
      "Epoch: 0240 Training MAE= 3.542960519\n",
      "Validation MAE:  3.548882\n",
      "Test MAE:  4.099404\n",
      "Epoch: 0241 Training MAE= 3.536577475\n",
      "Validation MAE:  3.5624962\n",
      "Test MAE:  4.1083264\n",
      "Epoch: 0242 Training MAE= 3.537186709\n",
      "Validation MAE:  3.5474923\n",
      "Test MAE:  4.094452\n",
      "Epoch: 0243 Training MAE= 3.535545867\n",
      "Validation MAE:  3.5816016\n",
      "Test MAE:  4.130168\n",
      "Epoch: 0244 Training MAE= 3.541467496\n",
      "Validation MAE:  3.554456\n",
      "Test MAE:  4.10042\n",
      "Epoch: 0245 Training MAE= 3.536452766\n",
      "Validation MAE:  3.566525\n",
      "Test MAE:  4.104279\n",
      "Epoch: 0246 Training MAE= 3.541534741\n",
      "Validation MAE:  3.5469496\n",
      "Test MAE:  4.1077657\n",
      "Epoch: 0247 Training MAE= 3.536653012\n",
      "Validation MAE:  3.552421\n",
      "Test MAE:  4.0915165\n",
      "Epoch: 0248 Training MAE= 3.531671072\n",
      "Validation MAE:  3.560923\n",
      "Test MAE:  4.114337\n",
      "Epoch: 0249 Training MAE= 3.539454609\n",
      "Validation MAE:  3.5669997\n",
      "Test MAE:  4.122964\n",
      "Epoch: 0250 Training MAE= 3.533869147\n",
      "Validation MAE:  3.5420945\n",
      "Test MAE:  4.0791607\n",
      "Epoch: 0251 Training MAE= 3.532242754\n",
      "Validation MAE:  3.5926018\n",
      "Test MAE:  4.141602\n",
      "Epoch: 0252 Training MAE= 3.538529157\n",
      "Validation MAE:  3.5416646\n",
      "Test MAE:  4.099926\n",
      "Epoch: 0253 Training MAE= 3.530105033\n",
      "Validation MAE:  3.540951\n",
      "Test MAE:  4.0801873\n",
      "Epoch: 0254 Training MAE= 3.532997716\n",
      "Validation MAE:  3.5358646\n",
      "Test MAE:  4.085116\n",
      "Epoch: 0255 Training MAE= 3.530425426\n",
      "Validation MAE:  3.5669847\n",
      "Test MAE:  4.1030126\n",
      "Epoch: 0256 Training MAE= 3.522177593\n",
      "Validation MAE:  3.5362246\n",
      "Test MAE:  4.095149\n",
      "Epoch: 0257 Training MAE= 3.530244719\n",
      "Validation MAE:  3.5434446\n",
      "Test MAE:  4.082688\n",
      "Epoch: 0258 Training MAE= 3.536595661\n",
      "Validation MAE:  3.5524778\n",
      "Test MAE:  4.099609\n",
      "Epoch: 0259 Training MAE= 3.537095862\n",
      "Validation MAE:  3.564759\n",
      "Test MAE:  4.119757\n",
      "Epoch: 0260 Training MAE= 3.528206635\n",
      "Validation MAE:  3.5939906\n",
      "Test MAE:  4.1378326\n",
      "Epoch: 0261 Training MAE= 3.536667598\n",
      "Validation MAE:  3.538876\n",
      "Test MAE:  4.1082954\n",
      "Epoch: 0262 Training MAE= 3.522527436\n",
      "Validation MAE:  3.557696\n",
      "Test MAE:  4.103108\n",
      "Epoch: 0263 Training MAE= 3.524635466\n",
      "Validation MAE:  3.561858\n",
      "Test MAE:  4.10792\n",
      "Epoch: 0264 Training MAE= 3.514653819\n",
      "Validation MAE:  3.5358105\n",
      "Test MAE:  4.0857353\n",
      "Epoch: 0265 Training MAE= 3.520976307\n",
      "Validation MAE:  3.5385509\n",
      "Test MAE:  4.0993524\n",
      "Epoch: 0266 Training MAE= 3.518006815\n",
      "Validation MAE:  3.5363533\n",
      "Test MAE:  4.097117\n",
      "Epoch: 0267 Training MAE= 3.535092012\n",
      "Validation MAE:  3.5703762\n",
      "Test MAE:  4.117108\n",
      "Epoch: 0268 Training MAE= 3.559818961\n",
      "Validation MAE:  3.5587904\n",
      "Test MAE:  4.126825\n",
      "Epoch: 0269 Training MAE= 3.533552179\n",
      "Validation MAE:  3.5333803\n",
      "Test MAE:  4.091247\n",
      "Epoch: 0270 Training MAE= 3.520434872\n",
      "Validation MAE:  3.5460284\n",
      "Test MAE:  4.0997186\n",
      "Epoch: 0271 Training MAE= 3.512084409\n",
      "Validation MAE:  3.5545797\n",
      "Test MAE:  4.103095\n",
      "Epoch: 0272 Training MAE= 3.511075327\n",
      "Validation MAE:  3.5426044\n",
      "Test MAE:  4.0884147\n",
      "Epoch: 0273 Training MAE= 3.515224484\n",
      "Validation MAE:  3.5648775\n",
      "Test MAE:  4.1182165\n",
      "Epoch: 0274 Training MAE= 3.519023508\n",
      "Validation MAE:  3.5616894\n",
      "Test MAE:  4.112026\n",
      "Epoch: 0275 Training MAE= 3.522612138\n",
      "Validation MAE:  3.5488281\n",
      "Test MAE:  4.110672\n",
      "Epoch: 0276 Training MAE= 3.518014303\n",
      "Validation MAE:  3.5408242\n",
      "Test MAE:  4.1035013\n",
      "Epoch: 0277 Training MAE= 3.511120914\n",
      "Validation MAE:  3.5544088\n",
      "Test MAE:  4.1084695\n",
      "Epoch: 0278 Training MAE= 3.516762980\n",
      "Validation MAE:  3.548194\n",
      "Test MAE:  4.106261\n",
      "Epoch: 0279 Training MAE= 3.511256270\n",
      "Validation MAE:  3.5747466\n",
      "Test MAE:  4.133734\n",
      "Epoch: 0280 Training MAE= 3.518113489\n",
      "Validation MAE:  3.5353765\n",
      "Test MAE:  4.100076\n",
      "Epoch: 0281 Training MAE= 3.518732697\n",
      "Validation MAE:  3.5406296\n",
      "Test MAE:  4.104994\n",
      "Epoch: 0282 Training MAE= 3.505938690\n",
      "Validation MAE:  3.5602534\n",
      "Test MAE:  4.1100082\n",
      "Epoch: 0283 Training MAE= 3.509979155\n",
      "Validation MAE:  3.538628\n",
      "Test MAE:  4.0917006\n",
      "Epoch: 0284 Training MAE= 3.513312658\n",
      "Validation MAE:  3.5340207\n",
      "Test MAE:  4.0931535\n",
      "Epoch: 0285 Training MAE= 3.507779899\n",
      "Validation MAE:  3.5818846\n",
      "Test MAE:  4.13339\n",
      "Epoch: 0286 Training MAE= 3.506819841\n",
      "Validation MAE:  3.5609837\n",
      "Test MAE:  4.118445\n",
      "Epoch: 0287 Training MAE= 3.506504701\n",
      "Validation MAE:  3.5499253\n",
      "Test MAE:  4.1127787\n",
      "Epoch: 0288 Training MAE= 3.501917739\n",
      "Validation MAE:  3.5711956\n",
      "Test MAE:  4.1314907\n",
      "Epoch: 0289 Training MAE= 3.557621558\n",
      "Validation MAE:  3.550969\n",
      "Test MAE:  4.122986\n",
      "Epoch: 0290 Training MAE= 3.509906020\n",
      "Validation MAE:  3.5358706\n",
      "Test MAE:  4.1083074\n",
      "Epoch: 0291 Training MAE= 3.516891748\n",
      "Validation MAE:  3.517837\n",
      "Test MAE:  4.079299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0292 Training MAE= 3.511410186\n",
      "Validation MAE:  3.5165374\n",
      "Test MAE:  4.0806584\n",
      "Epoch: 0293 Training MAE= 3.508167719\n",
      "Validation MAE:  3.5176892\n",
      "Test MAE:  4.080048\n",
      "Epoch: 0294 Training MAE= 3.508989432\n",
      "Validation MAE:  3.5147576\n",
      "Test MAE:  4.0779743\n",
      "Epoch: 0295 Training MAE= 3.504474313\n",
      "Validation MAE:  3.5216389\n",
      "Test MAE:  4.0848613\n",
      "Epoch: 0296 Training MAE= 3.500495527\n",
      "Validation MAE:  3.5257106\n",
      "Test MAE:  4.0862203\n",
      "Epoch: 0297 Training MAE= 3.495225329\n",
      "Validation MAE:  3.5388324\n",
      "Test MAE:  4.089668\n",
      "Epoch: 0298 Training MAE= 3.495489978\n",
      "Validation MAE:  3.552229\n",
      "Test MAE:  4.092698\n",
      "Epoch: 0299 Training MAE= 3.496891622\n",
      "Validation MAE:  3.5274303\n",
      "Test MAE:  4.0792713\n",
      "Epoch: 0300 Training MAE= 3.517047144\n",
      "Validation MAE:  3.523317\n",
      "Test MAE:  4.0862036\n",
      "Epoch: 0301 Training MAE= 3.503529386\n",
      "Validation MAE:  3.535359\n",
      "Test MAE:  4.099552\n",
      "Epoch: 0302 Training MAE= 3.493707390\n",
      "Validation MAE:  3.558206\n",
      "Test MAE:  4.108438\n",
      "Epoch: 0303 Training MAE= 3.510713955\n",
      "Validation MAE:  3.5183392\n",
      "Test MAE:  4.0799775\n",
      "Epoch: 0304 Training MAE= 3.503524160\n",
      "Validation MAE:  3.5368366\n",
      "Test MAE:  4.0903587\n",
      "Epoch: 0305 Training MAE= 3.494114413\n",
      "Validation MAE:  3.5402658\n",
      "Test MAE:  4.0917344\n",
      "Epoch: 0306 Training MAE= 3.506168096\n",
      "Validation MAE:  3.5236127\n",
      "Test MAE:  4.087374\n",
      "Epoch: 0307 Training MAE= 3.499880733\n",
      "Validation MAE:  3.5272357\n",
      "Test MAE:  4.095308\n",
      "Epoch: 0308 Training MAE= 3.491902038\n",
      "Validation MAE:  3.5476158\n",
      "Test MAE:  4.1050215\n",
      "Epoch: 0309 Training MAE= 3.494908388\n",
      "Validation MAE:  3.5408924\n",
      "Test MAE:  4.095947\n",
      "Epoch: 0310 Training MAE= 3.503097710\n",
      "Validation MAE:  3.5241835\n",
      "Test MAE:  4.090773\n",
      "Epoch: 0311 Training MAE= 3.494859678\n",
      "Validation MAE:  3.5392804\n",
      "Test MAE:  4.0921073\n",
      "Epoch: 0312 Training MAE= 3.495784229\n",
      "Validation MAE:  3.5342362\n",
      "Test MAE:  4.0742784\n",
      "Epoch: 0313 Training MAE= 3.490188671\n",
      "Validation MAE:  3.5677826\n",
      "Test MAE:  4.1118703\n",
      "Epoch: 0314 Training MAE= 3.495121208\n",
      "Validation MAE:  3.5354385\n",
      "Test MAE:  4.0822473\n",
      "Epoch: 0315 Training MAE= 3.494829476\n",
      "Validation MAE:  3.5336366\n",
      "Test MAE:  4.0963492\n",
      "Epoch: 0316 Training MAE= 3.491113431\n",
      "Validation MAE:  3.5700002\n",
      "Test MAE:  4.129932\n",
      "Epoch: 0317 Training MAE= 3.511204570\n",
      "Validation MAE:  3.5320263\n",
      "Test MAE:  4.080928\n",
      "Epoch: 0318 Training MAE= 3.503785017\n",
      "Validation MAE:  3.5275986\n",
      "Test MAE:  4.084508\n",
      "Epoch: 0319 Training MAE= 3.497113500\n",
      "Validation MAE:  3.5204864\n",
      "Test MAE:  4.089455\n",
      "Epoch: 0320 Training MAE= 3.486144243\n",
      "Validation MAE:  3.520059\n",
      "Test MAE:  4.0770454\n",
      "Epoch: 0321 Training MAE= 3.515957432\n",
      "Validation MAE:  3.609014\n",
      "Test MAE:  4.1794286\n",
      "Epoch: 0322 Training MAE= 3.516696795\n",
      "Validation MAE:  3.6048353\n",
      "Test MAE:  4.166779\n",
      "Epoch: 0323 Training MAE= 3.510840013\n",
      "Validation MAE:  3.5752008\n",
      "Test MAE:  4.1315536\n",
      "Epoch: 0324 Training MAE= 3.502067045\n",
      "Validation MAE:  3.5644798\n",
      "Test MAE:  4.1256247\n",
      "Epoch: 0325 Training MAE= 3.495581974\n",
      "Validation MAE:  3.5619957\n",
      "Test MAE:  4.1123285\n",
      "Epoch: 0326 Training MAE= 3.490482527\n",
      "Validation MAE:  3.5608351\n",
      "Test MAE:  4.1124706\n",
      "Epoch: 0327 Training MAE= 3.485695447\n",
      "Validation MAE:  3.5159101\n",
      "Test MAE:  4.0936193\n",
      "Epoch: 0328 Training MAE= 3.492853160\n",
      "Validation MAE:  3.5489829\n",
      "Test MAE:  4.128893\n",
      "Epoch: 0329 Training MAE= 3.482622990\n",
      "Validation MAE:  3.51418\n",
      "Test MAE:  4.0759215\n",
      "Epoch: 0330 Training MAE= 3.481683775\n",
      "Validation MAE:  3.5136466\n",
      "Test MAE:  4.07057\n",
      "Epoch: 0331 Training MAE= 3.488019697\n",
      "Validation MAE:  3.5205321\n",
      "Test MAE:  4.0976076\n",
      "Epoch: 0332 Training MAE= 3.487378122\n",
      "Validation MAE:  3.512048\n",
      "Test MAE:  4.082647\n",
      "Epoch: 0333 Training MAE= 3.483567849\n",
      "Validation MAE:  3.5073428\n",
      "Test MAE:  4.080553\n",
      "Epoch: 0334 Training MAE= 3.476005286\n",
      "Validation MAE:  3.5053124\n",
      "Test MAE:  4.075672\n",
      "Epoch: 0335 Training MAE= 3.476121848\n",
      "Validation MAE:  3.5096037\n",
      "Test MAE:  4.07562\n",
      "Epoch: 0336 Training MAE= 3.474838911\n",
      "Validation MAE:  3.5161068\n",
      "Test MAE:  4.08323\n",
      "Epoch: 0337 Training MAE= 3.477196182\n",
      "Validation MAE:  3.5254018\n",
      "Test MAE:  4.0962267\n",
      "Epoch: 0338 Training MAE= 3.475895935\n",
      "Validation MAE:  3.5043845\n",
      "Test MAE:  4.076312\n",
      "Epoch: 0339 Training MAE= 3.492167004\n",
      "Validation MAE:  3.5006814\n",
      "Test MAE:  4.080952\n",
      "Epoch: 0340 Training MAE= 3.476327405\n",
      "Validation MAE:  3.5098515\n",
      "Test MAE:  4.081901\n",
      "Epoch: 0341 Training MAE= 3.483290533\n",
      "Validation MAE:  3.5275548\n",
      "Test MAE:  4.085247\n",
      "Epoch: 0342 Training MAE= 3.486141576\n",
      "Validation MAE:  3.5079877\n",
      "Test MAE:  4.089021\n",
      "Epoch: 0343 Training MAE= 3.474802438\n",
      "Validation MAE:  3.5058327\n",
      "Test MAE:  4.0829325\n",
      "Epoch: 0344 Training MAE= 3.476536709\n",
      "Validation MAE:  3.500715\n",
      "Test MAE:  4.0780525\n",
      "Epoch: 0345 Training MAE= 3.484144808\n",
      "Validation MAE:  3.5003524\n",
      "Test MAE:  4.066773\n",
      "Epoch: 0346 Training MAE= 3.474672549\n",
      "Validation MAE:  3.5135148\n",
      "Test MAE:  4.0843964\n",
      "Epoch: 0347 Training MAE= 3.472040415\n",
      "Validation MAE:  3.5146186\n",
      "Test MAE:  4.078026\n",
      "Epoch: 0348 Training MAE= 3.481221029\n",
      "Validation MAE:  3.5117636\n",
      "Test MAE:  4.073771\n",
      "Epoch: 0349 Training MAE= 3.493461947\n",
      "Validation MAE:  3.51396\n",
      "Test MAE:  4.077941\n",
      "Epoch: 0350 Training MAE= 3.477938721\n",
      "Validation MAE:  3.5170593\n",
      "Test MAE:  4.0818\n",
      "Epoch: 0351 Training MAE= 3.464484185\n",
      "Validation MAE:  3.5025895\n",
      "Test MAE:  4.0707607\n",
      "Epoch: 0352 Training MAE= 3.469116039\n",
      "Validation MAE:  3.500697\n",
      "Test MAE:  4.071965\n",
      "Epoch: 0353 Training MAE= 3.473531219\n",
      "Validation MAE:  3.5623813\n",
      "Test MAE:  4.129524\n",
      "Epoch: 0354 Training MAE= 3.470127378\n",
      "Validation MAE:  3.5096052\n",
      "Test MAE:  4.0745306\n",
      "Epoch: 0355 Training MAE= 3.477963068\n",
      "Validation MAE:  3.5204303\n",
      "Test MAE:  4.092315\n",
      "Epoch: 0356 Training MAE= 3.477882156\n",
      "Validation MAE:  3.5071564\n",
      "Test MAE:  4.0744796\n",
      "Epoch: 0357 Training MAE= 3.488168732\n",
      "Validation MAE:  3.5225885\n",
      "Test MAE:  4.104397\n",
      "Epoch: 0358 Training MAE= 3.473519395\n",
      "Validation MAE:  3.507246\n",
      "Test MAE:  4.078047\n",
      "Epoch: 0359 Training MAE= 3.473674686\n",
      "Validation MAE:  3.505452\n",
      "Test MAE:  4.082831\n",
      "Epoch: 0360 Training MAE= 3.477291418\n",
      "Validation MAE:  3.5029109\n",
      "Test MAE:  4.079142\n",
      "Epoch: 0361 Training MAE= 3.467272048\n",
      "Validation MAE:  3.5026174\n",
      "Test MAE:  4.075548\n",
      "Epoch: 0362 Training MAE= 3.466002328\n",
      "Validation MAE:  3.5152318\n",
      "Test MAE:  4.090481\n",
      "Epoch: 0363 Training MAE= 3.463259167\n",
      "Validation MAE:  3.5273042\n",
      "Test MAE:  4.094266\n",
      "Epoch: 0364 Training MAE= 3.460052301\n",
      "Validation MAE:  3.5279818\n",
      "Test MAE:  4.094766\n",
      "Epoch: 0365 Training MAE= 3.467201349\n",
      "Validation MAE:  3.5252938\n",
      "Test MAE:  4.1059937\n",
      "Epoch: 0366 Training MAE= 3.491532506\n",
      "Validation MAE:  3.5852947\n",
      "Test MAE:  4.1622267\n",
      "Epoch: 0367 Training MAE= 3.481679976\n",
      "Validation MAE:  3.4994202\n",
      "Test MAE:  4.0913506\n",
      "Epoch: 0368 Training MAE= 3.471215779\n",
      "Validation MAE:  3.4943917\n",
      "Test MAE:  4.070216\n",
      "Epoch: 0369 Training MAE= 3.460054604\n",
      "Validation MAE:  3.4954023\n",
      "Test MAE:  4.0653253\n",
      "Epoch: 0370 Training MAE= 3.459226694\n",
      "Validation MAE:  3.5088315\n",
      "Test MAE:  4.0781612\n",
      "Epoch: 0371 Training MAE= 3.456656713\n",
      "Validation MAE:  3.5124295\n",
      "Test MAE:  4.0767035\n",
      "Epoch: 0372 Training MAE= 3.462246854\n",
      "Validation MAE:  3.5163267\n",
      "Test MAE:  4.09817\n",
      "Epoch: 0373 Training MAE= 3.472125679\n",
      "Validation MAE:  3.504571\n",
      "Test MAE:  4.0843306\n",
      "Epoch: 0374 Training MAE= 3.459858655\n",
      "Validation MAE:  3.5031495\n",
      "Test MAE:  4.0737576\n",
      "Epoch: 0375 Training MAE= 3.463546115\n",
      "Validation MAE:  3.5105746\n",
      "Test MAE:  4.0853343\n",
      "Epoch: 0376 Training MAE= 3.462670972\n",
      "Validation MAE:  3.5003707\n",
      "Test MAE:  4.065084\n",
      "Epoch: 0377 Training MAE= 3.468491341\n",
      "Validation MAE:  3.5087695\n",
      "Test MAE:  4.0900073\n",
      "Epoch: 0378 Training MAE= 3.462206067\n",
      "Validation MAE:  3.5176685\n",
      "Test MAE:  4.081923\n",
      "Epoch: 0379 Training MAE= 3.464949643\n",
      "Validation MAE:  3.4907668\n",
      "Test MAE:  4.059402\n",
      "Epoch: 0380 Training MAE= 3.460927042\n",
      "Validation MAE:  3.4981656\n",
      "Test MAE:  4.0780377\n",
      "Epoch: 0381 Training MAE= 3.470382130\n",
      "Validation MAE:  3.4992452\n",
      "Test MAE:  4.075421\n",
      "Epoch: 0382 Training MAE= 3.460582229\n",
      "Validation MAE:  3.508905\n",
      "Test MAE:  4.074735\n",
      "Epoch: 0383 Training MAE= 3.446636695\n",
      "Validation MAE:  3.4930599\n",
      "Test MAE:  4.0667534\n",
      "Epoch: 0384 Training MAE= 3.484938840\n",
      "Validation MAE:  3.5504894\n",
      "Test MAE:  4.129451\n",
      "Epoch: 0385 Training MAE= 3.478456798\n",
      "Validation MAE:  3.560013\n",
      "Test MAE:  4.1302176\n",
      "Epoch: 0386 Training MAE= 3.468962360\n",
      "Validation MAE:  3.5176911\n",
      "Test MAE:  4.0905595\n",
      "Epoch: 0387 Training MAE= 3.454603255\n",
      "Validation MAE:  3.4928093\n",
      "Test MAE:  4.067781\n",
      "Epoch: 0388 Training MAE= 3.453321125\n",
      "Validation MAE:  3.4903076\n",
      "Test MAE:  4.063532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0389 Training MAE= 3.453858497\n",
      "Validation MAE:  3.491676\n",
      "Test MAE:  4.058645\n",
      "Epoch: 0390 Training MAE= 3.445546745\n",
      "Validation MAE:  3.5001726\n",
      "Test MAE:  4.072263\n",
      "Epoch: 0391 Training MAE= 3.448139288\n",
      "Validation MAE:  3.5079017\n",
      "Test MAE:  4.075655\n",
      "Epoch: 0392 Training MAE= 3.460588163\n",
      "Validation MAE:  3.5016673\n",
      "Test MAE:  4.072405\n",
      "Epoch: 0393 Training MAE= 3.468047393\n",
      "Validation MAE:  3.540707\n",
      "Test MAE:  4.1093464\n",
      "Epoch: 0394 Training MAE= 3.463109476\n",
      "Validation MAE:  3.5241122\n",
      "Test MAE:  4.0971084\n",
      "Epoch: 0395 Training MAE= 3.452369964\n",
      "Validation MAE:  3.4883895\n",
      "Test MAE:  4.0537453\n",
      "Epoch: 0396 Training MAE= 3.444385295\n",
      "Validation MAE:  3.4971015\n",
      "Test MAE:  4.0646834\n",
      "Epoch: 0397 Training MAE= 3.440759452\n",
      "Validation MAE:  3.491876\n",
      "Test MAE:  4.0590544\n",
      "Epoch: 0398 Training MAE= 3.442060276\n",
      "Validation MAE:  3.4885871\n",
      "Test MAE:  4.055683\n",
      "Epoch: 0399 Training MAE= 3.481824274\n",
      "Validation MAE:  3.5637968\n",
      "Test MAE:  4.1444716\n",
      "Epoch: 0400 Training MAE= 3.475072556\n",
      "Validation MAE:  3.5552697\n",
      "Test MAE:  4.1204042\n",
      "Epoch: 0401 Training MAE= 3.451857736\n",
      "Validation MAE:  3.4905443\n",
      "Test MAE:  4.0627394\n",
      "Epoch: 0402 Training MAE= 3.444048651\n",
      "Validation MAE:  3.4959953\n",
      "Test MAE:  4.066193\n",
      "Epoch: 0403 Training MAE= 3.439551152\n",
      "Validation MAE:  3.5021029\n",
      "Test MAE:  4.0728602\n",
      "Epoch: 0404 Training MAE= 3.440793792\n",
      "Validation MAE:  3.4998784\n",
      "Test MAE:  4.0671244\n",
      "Epoch: 0405 Training MAE= 3.439098927\n",
      "Validation MAE:  3.511435\n",
      "Test MAE:  4.0878158\n",
      "Epoch: 0406 Training MAE= 3.450466026\n",
      "Validation MAE:  3.5043795\n",
      "Test MAE:  4.078973\n",
      "Epoch: 0407 Training MAE= 3.475954697\n",
      "Validation MAE:  3.55497\n",
      "Test MAE:  4.136978\n",
      "Epoch: 0408 Training MAE= 3.455411939\n",
      "Validation MAE:  3.487228\n",
      "Test MAE:  4.065186\n",
      "Epoch: 0409 Training MAE= 3.448386973\n",
      "Validation MAE:  3.4940002\n",
      "Test MAE:  4.066152\n",
      "Epoch: 0410 Training MAE= 3.439983384\n",
      "Validation MAE:  3.488297\n",
      "Test MAE:  4.066313\n",
      "Epoch: 0411 Training MAE= 3.437238864\n",
      "Validation MAE:  3.482692\n",
      "Test MAE:  4.06244\n",
      "Epoch: 0412 Training MAE= 3.448917159\n",
      "Validation MAE:  3.5066402\n",
      "Test MAE:  4.0851655\n",
      "Epoch: 0413 Training MAE= 3.443653152\n",
      "Validation MAE:  3.5117235\n",
      "Test MAE:  4.0877814\n",
      "Epoch: 0414 Training MAE= 3.472483181\n",
      "Validation MAE:  3.5580115\n",
      "Test MAE:  4.139871\n",
      "Epoch: 0415 Training MAE= 3.460538469\n",
      "Validation MAE:  3.5001755\n",
      "Test MAE:  4.086415\n",
      "Epoch: 0416 Training MAE= 3.446989735\n",
      "Validation MAE:  3.4840178\n",
      "Test MAE:  4.0597878\n",
      "Epoch: 0417 Training MAE= 3.441649622\n",
      "Validation MAE:  3.4822955\n",
      "Test MAE:  4.0685673\n",
      "Epoch: 0418 Training MAE= 3.436145106\n",
      "Validation MAE:  3.480615\n",
      "Test MAE:  4.0620418\n",
      "Epoch: 0419 Training MAE= 3.446962159\n",
      "Validation MAE:  3.501422\n",
      "Test MAE:  4.0772986\n",
      "Epoch: 0420 Training MAE= 3.448604148\n",
      "Validation MAE:  3.4919515\n",
      "Test MAE:  4.073518\n",
      "Epoch: 0421 Training MAE= 3.452544509\n",
      "Validation MAE:  3.5115502\n",
      "Test MAE:  4.0883374\n",
      "Epoch: 0422 Training MAE= 3.448019907\n",
      "Validation MAE:  3.485902\n",
      "Test MAE:  4.064441\n",
      "Epoch: 0423 Training MAE= 3.453225498\n",
      "Validation MAE:  3.4841883\n",
      "Test MAE:  4.069744\n",
      "Epoch: 0424 Training MAE= 3.445524136\n",
      "Validation MAE:  3.481252\n",
      "Test MAE:  4.060171\n",
      "Epoch: 0425 Training MAE= 3.443687685\n",
      "Validation MAE:  3.4884863\n",
      "Test MAE:  4.0585084\n",
      "Epoch: 0426 Training MAE= 3.448726807\n",
      "Validation MAE:  3.4843266\n",
      "Test MAE:  4.0660696\n",
      "Epoch: 0427 Training MAE= 3.444595525\n",
      "Validation MAE:  3.4914606\n",
      "Test MAE:  4.064935\n",
      "Epoch: 0428 Training MAE= 3.451362032\n",
      "Validation MAE:  3.517949\n",
      "Test MAE:  4.0964313\n",
      "Epoch: 0429 Training MAE= 3.452373100\n",
      "Validation MAE:  3.5471146\n",
      "Test MAE:  4.118142\n",
      "Epoch: 0430 Training MAE= 3.465786963\n",
      "Validation MAE:  3.507078\n",
      "Test MAE:  4.0984654\n",
      "Epoch: 0431 Training MAE= 3.460863398\n",
      "Validation MAE:  3.497566\n",
      "Test MAE:  4.082774\n",
      "Epoch: 0432 Training MAE= 3.469838072\n",
      "Validation MAE:  3.4850674\n",
      "Test MAE:  4.0948863\n",
      "Epoch: 0433 Training MAE= 3.445789055\n",
      "Validation MAE:  3.4791718\n",
      "Test MAE:  4.0846934\n",
      "Epoch: 0434 Training MAE= 3.441246546\n",
      "Validation MAE:  3.4797924\n",
      "Test MAE:  4.082753\n",
      "Epoch: 0435 Training MAE= 3.441097144\n",
      "Validation MAE:  3.484898\n",
      "Test MAE:  4.0815773\n",
      "Epoch: 0436 Training MAE= 3.438532299\n",
      "Validation MAE:  3.4863887\n",
      "Test MAE:  4.0842323\n",
      "Epoch: 0437 Training MAE= 3.435697648\n",
      "Validation MAE:  3.4901447\n",
      "Test MAE:  4.088463\n",
      "Epoch: 0438 Training MAE= 3.438131277\n",
      "Validation MAE:  3.4971561\n",
      "Test MAE:  4.091529\n",
      "Epoch: 0439 Training MAE= 3.434978019\n",
      "Validation MAE:  3.4931202\n",
      "Test MAE:  4.0934324\n",
      "Epoch: 0440 Training MAE= 3.431473355\n",
      "Validation MAE:  3.483387\n",
      "Test MAE:  4.072625\n",
      "Epoch: 0441 Training MAE= 3.423845320\n",
      "Validation MAE:  3.469164\n",
      "Test MAE:  4.064846\n",
      "Epoch: 0442 Training MAE= 3.428255966\n",
      "Validation MAE:  3.4801893\n",
      "Test MAE:  4.059085\n",
      "Epoch: 0443 Training MAE= 3.422036204\n",
      "Validation MAE:  3.4711447\n",
      "Test MAE:  4.061011\n",
      "Epoch: 0444 Training MAE= 3.442372637\n",
      "Validation MAE:  3.4867098\n",
      "Test MAE:  4.0733027\n",
      "Epoch: 0445 Training MAE= 3.428279384\n",
      "Validation MAE:  3.4719229\n",
      "Test MAE:  4.066748\n",
      "Epoch: 0446 Training MAE= 3.447670285\n",
      "Validation MAE:  3.5488074\n",
      "Test MAE:  4.1278415\n",
      "Epoch: 0447 Training MAE= 3.437842111\n",
      "Validation MAE:  3.4685004\n",
      "Test MAE:  4.0647993\n",
      "Epoch: 0448 Training MAE= 3.427390599\n",
      "Validation MAE:  3.4745362\n",
      "Test MAE:  4.065561\n",
      "Epoch: 0449 Training MAE= 3.422503620\n",
      "Validation MAE:  3.4726548\n",
      "Test MAE:  4.0611873\n",
      "Epoch: 0450 Training MAE= 3.428342748\n",
      "Validation MAE:  3.4814825\n",
      "Test MAE:  4.063768\n",
      "Epoch: 0451 Training MAE= 3.425259059\n",
      "Validation MAE:  3.4820702\n",
      "Test MAE:  4.0734973\n",
      "Epoch: 0452 Training MAE= 3.430387628\n",
      "Validation MAE:  3.468818\n",
      "Test MAE:  4.062022\n",
      "Epoch: 0453 Training MAE= 3.436573228\n",
      "Validation MAE:  3.467309\n",
      "Test MAE:  4.06628\n",
      "Epoch: 0454 Training MAE= 3.428904101\n",
      "Validation MAE:  3.4757035\n",
      "Test MAE:  4.0652423\n",
      "Epoch: 0455 Training MAE= 3.426790068\n",
      "Validation MAE:  3.483852\n",
      "Test MAE:  4.070319\n",
      "Epoch: 0456 Training MAE= 3.431303813\n",
      "Validation MAE:  3.4879172\n",
      "Test MAE:  4.077917\n",
      "Epoch: 0457 Training MAE= 3.429269774\n",
      "Validation MAE:  3.4785368\n",
      "Test MAE:  4.0659275\n",
      "Epoch: 0458 Training MAE= 3.433686515\n",
      "Validation MAE:  3.4910169\n",
      "Test MAE:  4.076369\n",
      "Epoch: 0459 Training MAE= 3.422436013\n",
      "Validation MAE:  3.4796393\n",
      "Test MAE:  4.068998\n",
      "Epoch: 0460 Training MAE= 3.442191147\n",
      "Validation MAE:  3.4810724\n",
      "Test MAE:  4.07531\n",
      "Epoch: 0461 Training MAE= 3.427148415\n",
      "Validation MAE:  3.4677737\n",
      "Test MAE:  4.063894\n",
      "Epoch: 0462 Training MAE= 3.432492436\n",
      "Validation MAE:  3.4767857\n",
      "Test MAE:  4.064808\n",
      "Epoch: 0463 Training MAE= 3.425640832\n",
      "Validation MAE:  3.475484\n",
      "Test MAE:  4.068043\n",
      "Epoch: 0464 Training MAE= 3.421464824\n",
      "Validation MAE:  3.4803612\n",
      "Test MAE:  4.0565\n",
      "Epoch: 0465 Training MAE= 3.421028849\n",
      "Validation MAE:  3.4693077\n",
      "Test MAE:  4.059926\n",
      "Epoch: 0466 Training MAE= 3.430763317\n",
      "Validation MAE:  3.488155\n",
      "Test MAE:  4.075446\n",
      "Epoch: 0467 Training MAE= 3.427613337\n",
      "Validation MAE:  3.476874\n",
      "Test MAE:  4.061427\n",
      "Epoch: 0468 Training MAE= 3.425061067\n",
      "Validation MAE:  3.4744673\n",
      "Test MAE:  4.064514\n",
      "Epoch: 0469 Training MAE= 3.426069712\n",
      "Validation MAE:  3.491132\n",
      "Test MAE:  4.0788856\n",
      "Epoch: 0470 Training MAE= 3.422389892\n",
      "Validation MAE:  3.4853854\n",
      "Test MAE:  4.0714726\n",
      "Epoch: 0471 Training MAE= 3.422350828\n",
      "Validation MAE:  3.4800167\n",
      "Test MAE:  4.0698075\n",
      "Epoch: 0472 Training MAE= 3.419881294\n",
      "Validation MAE:  3.4754798\n",
      "Test MAE:  4.062111\n",
      "Epoch: 0473 Training MAE= 3.449104065\n",
      "Validation MAE:  3.5614245\n",
      "Test MAE:  4.149722\n",
      "Epoch: 0474 Training MAE= 3.460110028\n",
      "Validation MAE:  3.4782832\n",
      "Test MAE:  4.0883493\n",
      "Epoch: 0475 Training MAE= 3.427762849\n",
      "Validation MAE:  3.4736645\n",
      "Test MAE:  4.076853\n",
      "Epoch: 0476 Training MAE= 3.418281512\n",
      "Validation MAE:  3.466919\n",
      "Test MAE:  4.0605993\n",
      "Epoch: 0477 Training MAE= 3.412964263\n",
      "Validation MAE:  3.4630427\n",
      "Test MAE:  4.057364\n",
      "Epoch: 0478 Training MAE= 3.414660568\n",
      "Validation MAE:  3.4692943\n",
      "Test MAE:  4.063072\n",
      "Epoch: 0479 Training MAE= 3.413679998\n",
      "Validation MAE:  3.4718862\n",
      "Test MAE:  4.0644584\n",
      "Epoch: 0480 Training MAE= 3.413567428\n",
      "Validation MAE:  3.4714775\n",
      "Test MAE:  4.061173\n",
      "Epoch: 0481 Training MAE= 3.411251607\n",
      "Validation MAE:  3.4671674\n",
      "Test MAE:  4.0545497\n",
      "Epoch: 0482 Training MAE= 3.441531629\n",
      "Validation MAE:  3.47741\n",
      "Test MAE:  4.074193\n",
      "Epoch: 0483 Training MAE= 3.424524387\n",
      "Validation MAE:  3.4626482\n",
      "Test MAE:  4.0619054\n",
      "Epoch: 0484 Training MAE= 3.417805066\n",
      "Validation MAE:  3.467299\n",
      "Test MAE:  4.063266\n",
      "Epoch: 0485 Training MAE= 3.414068002\n",
      "Validation MAE:  3.46948\n",
      "Test MAE:  4.0593805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0486 Training MAE= 3.410942909\n",
      "Validation MAE:  3.4731123\n",
      "Test MAE:  4.061813\n",
      "Epoch: 0487 Training MAE= 3.415310334\n",
      "Validation MAE:  3.4632409\n",
      "Test MAE:  4.0551214\n",
      "Epoch: 0488 Training MAE= 3.431474809\n",
      "Validation MAE:  3.4747543\n",
      "Test MAE:  4.068348\n",
      "Epoch: 0489 Training MAE= 3.425529863\n",
      "Validation MAE:  3.4632244\n",
      "Test MAE:  4.059134\n",
      "Epoch: 0490 Training MAE= 3.421913904\n",
      "Validation MAE:  3.461232\n",
      "Test MAE:  4.056547\n",
      "Epoch: 0491 Training MAE= 3.417981668\n",
      "Validation MAE:  3.4669135\n",
      "Test MAE:  4.0610833\n",
      "Epoch: 0492 Training MAE= 3.415936015\n",
      "Validation MAE:  3.4583187\n",
      "Test MAE:  4.051672\n",
      "Epoch: 0493 Training MAE= 3.410537487\n",
      "Validation MAE:  3.4617152\n",
      "Test MAE:  4.051257\n",
      "Epoch: 0494 Training MAE= 3.413566096\n",
      "Validation MAE:  3.4810734\n",
      "Test MAE:  4.068924\n",
      "Epoch: 0495 Training MAE= 3.413865317\n",
      "Validation MAE:  3.481433\n",
      "Test MAE:  4.068524\n",
      "Epoch: 0496 Training MAE= 3.412510181\n",
      "Validation MAE:  3.460812\n",
      "Test MAE:  4.0521894\n",
      "Epoch: 0497 Training MAE= 3.414351539\n",
      "Validation MAE:  3.4682217\n",
      "Test MAE:  4.054033\n",
      "Epoch: 0498 Training MAE= 3.407510980\n",
      "Validation MAE:  3.4600413\n",
      "Test MAE:  4.047786\n",
      "Epoch: 0499 Training MAE= 3.414641378\n",
      "Validation MAE:  3.485602\n",
      "Test MAE:  4.0670695\n",
      "Epoch: 0500 Training MAE= 3.416138740\n",
      "Validation MAE:  3.4823835\n",
      "Test MAE:  4.0699396\n",
      "Epoch: 0501 Training MAE= 3.430612417\n",
      "Validation MAE:  3.487323\n",
      "Test MAE:  4.0768185\n",
      "Epoch: 0502 Training MAE= 3.417983895\n",
      "Validation MAE:  3.4629674\n",
      "Test MAE:  4.055427\n",
      "Epoch: 0503 Training MAE= 3.409999964\n",
      "Validation MAE:  3.4587727\n",
      "Test MAE:  4.0495467\n",
      "Epoch: 0504 Training MAE= 3.404950737\n",
      "Validation MAE:  3.4617093\n",
      "Test MAE:  4.051317\n",
      "Epoch: 0505 Training MAE= 3.408740782\n",
      "Validation MAE:  3.4634461\n",
      "Test MAE:  4.0604644\n",
      "Epoch: 0506 Training MAE= 3.414950576\n",
      "Validation MAE:  3.4641516\n",
      "Test MAE:  4.056386\n",
      "Epoch: 0507 Training MAE= 3.411666640\n",
      "Validation MAE:  3.4653468\n",
      "Test MAE:  4.057962\n",
      "Epoch: 0508 Training MAE= 3.406242440\n",
      "Validation MAE:  3.4599712\n",
      "Test MAE:  4.0527067\n",
      "Epoch: 0509 Training MAE= 3.409452282\n",
      "Validation MAE:  3.475891\n",
      "Test MAE:  4.068744\n",
      "Epoch: 0510 Training MAE= 3.409703367\n",
      "Validation MAE:  3.4608195\n",
      "Test MAE:  4.0512743\n",
      "Epoch: 0511 Training MAE= 3.405725379\n",
      "Validation MAE:  3.4641461\n",
      "Test MAE:  4.0512056\n",
      "Epoch: 0512 Training MAE= 3.412628355\n",
      "Validation MAE:  3.4558167\n",
      "Test MAE:  4.048271\n",
      "Epoch: 0513 Training MAE= 3.427478632\n",
      "Validation MAE:  3.4622688\n",
      "Test MAE:  4.0512156\n",
      "Epoch: 0514 Training MAE= 3.403852963\n",
      "Validation MAE:  3.4583657\n",
      "Test MAE:  4.04677\n",
      "Epoch: 0515 Training MAE= 3.405864597\n",
      "Validation MAE:  3.473282\n",
      "Test MAE:  4.063447\n",
      "Epoch: 0516 Training MAE= 3.412897111\n",
      "Validation MAE:  3.532369\n",
      "Test MAE:  4.1266904\n",
      "Epoch: 0517 Training MAE= 3.444408538\n",
      "Validation MAE:  3.4797814\n",
      "Test MAE:  4.0716195\n",
      "Epoch: 0518 Training MAE= 3.428187486\n",
      "Validation MAE:  3.4920266\n",
      "Test MAE:  4.0862527\n",
      "Epoch: 0519 Training MAE= 3.429668879\n",
      "Validation MAE:  3.4581125\n",
      "Test MAE:  4.070428\n",
      "Epoch: 0520 Training MAE= 3.405596570\n",
      "Validation MAE:  3.473553\n",
      "Test MAE:  4.069927\n",
      "Epoch: 0521 Training MAE= 3.407558123\n",
      "Validation MAE:  3.4724517\n",
      "Test MAE:  4.067571\n",
      "Epoch: 0522 Training MAE= 3.408790075\n",
      "Validation MAE:  3.4693325\n",
      "Test MAE:  4.065848\n",
      "Epoch: 0523 Training MAE= 3.402488621\n",
      "Validation MAE:  3.4608727\n",
      "Test MAE:  4.057395\n",
      "Epoch: 0524 Training MAE= 3.395147235\n",
      "Validation MAE:  3.4538918\n",
      "Test MAE:  4.0498395\n",
      "Epoch: 0525 Training MAE= 3.395961570\n",
      "Validation MAE:  3.4534934\n",
      "Test MAE:  4.048088\n",
      "Epoch: 0526 Training MAE= 3.394311748\n",
      "Validation MAE:  3.4535518\n",
      "Test MAE:  4.0454044\n",
      "Epoch: 0527 Training MAE= 3.398498187\n",
      "Validation MAE:  3.4587903\n",
      "Test MAE:  4.04994\n",
      "Epoch: 0528 Training MAE= 3.397411496\n",
      "Validation MAE:  3.45766\n",
      "Test MAE:  4.047872\n",
      "Epoch: 0529 Training MAE= 3.414005264\n",
      "Validation MAE:  3.4612243\n",
      "Test MAE:  4.053012\n",
      "Epoch: 0530 Training MAE= 3.401521589\n",
      "Validation MAE:  3.4607682\n",
      "Test MAE:  4.0470676\n",
      "Epoch: 0531 Training MAE= 3.408900110\n",
      "Validation MAE:  3.4640303\n",
      "Test MAE:  4.050869\n",
      "Epoch: 0532 Training MAE= 3.401992421\n",
      "Validation MAE:  3.4712532\n",
      "Test MAE:  4.0557446\n",
      "Epoch: 0533 Training MAE= 3.407084375\n",
      "Validation MAE:  3.4780998\n",
      "Test MAE:  4.067471\n",
      "Epoch: 0534 Training MAE= 3.409975109\n",
      "Validation MAE:  3.4584491\n",
      "Test MAE:  4.0477457\n",
      "Epoch: 0535 Training MAE= 3.400307147\n",
      "Validation MAE:  3.4656162\n",
      "Test MAE:  4.0515223\n",
      "Epoch: 0536 Training MAE= 3.400506922\n",
      "Validation MAE:  3.4540458\n",
      "Test MAE:  4.0470047\n",
      "Epoch: 0537 Training MAE= 3.402936094\n",
      "Validation MAE:  3.4616873\n",
      "Test MAE:  4.0510283\n",
      "Epoch: 0538 Training MAE= 3.392008140\n",
      "Validation MAE:  3.4650147\n",
      "Test MAE:  4.0553193\n",
      "Epoch: 0539 Training MAE= 3.406598635\n",
      "Validation MAE:  3.4544568\n",
      "Test MAE:  4.0483584\n",
      "Epoch: 0540 Training MAE= 3.409607050\n",
      "Validation MAE:  3.46556\n",
      "Test MAE:  4.0538745\n",
      "Epoch: 0541 Training MAE= 3.391735236\n",
      "Validation MAE:  3.4546132\n",
      "Test MAE:  4.0487313\n",
      "Epoch: 0542 Training MAE= 3.401907125\n",
      "Validation MAE:  3.459073\n",
      "Test MAE:  4.044793\n",
      "Epoch: 0543 Training MAE= 3.399312974\n",
      "Validation MAE:  3.4826488\n",
      "Test MAE:  4.0720124\n",
      "Epoch: 0544 Training MAE= 3.396425283\n",
      "Validation MAE:  3.4568362\n",
      "Test MAE:  4.045837\n",
      "Epoch: 0545 Training MAE= 3.417972384\n",
      "Validation MAE:  3.4663832\n",
      "Test MAE:  4.050058\n",
      "Epoch: 0546 Training MAE= 3.412625383\n",
      "Validation MAE:  3.464698\n",
      "Test MAE:  4.0502906\n",
      "Epoch: 0547 Training MAE= 3.410303011\n",
      "Validation MAE:  3.4989605\n",
      "Test MAE:  4.0812626\n",
      "Epoch: 0548 Training MAE= 3.406216286\n",
      "Validation MAE:  3.4761896\n",
      "Test MAE:  4.064562\n",
      "Epoch: 0549 Training MAE= 3.399426695\n",
      "Validation MAE:  3.4905007\n",
      "Test MAE:  4.0797315\n",
      "Epoch: 0550 Training MAE= 3.410972502\n",
      "Validation MAE:  3.4883223\n",
      "Test MAE:  4.0778427\n",
      "Epoch: 0551 Training MAE= 3.412744734\n",
      "Validation MAE:  3.5014427\n",
      "Test MAE:  4.088394\n",
      "Epoch: 0552 Training MAE= 3.421304190\n",
      "Validation MAE:  3.452217\n",
      "Test MAE:  4.051839\n",
      "Epoch: 0553 Training MAE= 3.409789291\n",
      "Validation MAE:  3.4525063\n",
      "Test MAE:  4.0473447\n",
      "Epoch: 0554 Training MAE= 3.399694457\n",
      "Validation MAE:  3.456187\n",
      "Test MAE:  4.052409\n",
      "Epoch: 0555 Training MAE= 3.395369547\n",
      "Validation MAE:  3.4650824\n",
      "Test MAE:  4.0585318\n",
      "Epoch: 0556 Training MAE= 3.402555074\n",
      "Validation MAE:  3.4568937\n",
      "Test MAE:  4.0508733\n",
      "Epoch: 0557 Training MAE= 3.400927814\n",
      "Validation MAE:  3.461828\n",
      "Test MAE:  4.057853\n",
      "Epoch: 0558 Training MAE= 3.394391021\n",
      "Validation MAE:  3.4666195\n",
      "Test MAE:  4.0605574\n",
      "Epoch: 0559 Training MAE= 3.389173045\n",
      "Validation MAE:  3.4546437\n",
      "Test MAE:  4.0484924\n",
      "Epoch: 0560 Training MAE= 3.385826003\n",
      "Validation MAE:  3.4554236\n",
      "Test MAE:  4.050746\n",
      "Epoch: 0561 Training MAE= 3.385653790\n",
      "Validation MAE:  3.4528136\n",
      "Test MAE:  4.0442133\n",
      "Epoch: 0562 Training MAE= 3.381866218\n",
      "Validation MAE:  3.449697\n",
      "Test MAE:  4.0464535\n",
      "Epoch: 0563 Training MAE= 3.387238737\n",
      "Validation MAE:  3.4519258\n",
      "Test MAE:  4.0471277\n",
      "Epoch: 0564 Training MAE= 3.388121277\n",
      "Validation MAE:  3.4656076\n",
      "Test MAE:  4.0588136\n",
      "Epoch: 0565 Training MAE= 3.409786939\n",
      "Validation MAE:  3.498889\n",
      "Test MAE:  4.0847907\n",
      "Epoch: 0566 Training MAE= 3.400379007\n",
      "Validation MAE:  3.455244\n",
      "Test MAE:  4.049918\n",
      "Epoch: 0567 Training MAE= 3.395125661\n",
      "Validation MAE:  3.4541256\n",
      "Test MAE:  4.049017\n",
      "Epoch: 0568 Training MAE= 3.389343984\n",
      "Validation MAE:  3.4534237\n",
      "Test MAE:  4.045604\n",
      "Epoch: 0569 Training MAE= 3.389439040\n",
      "Validation MAE:  3.4624028\n",
      "Test MAE:  4.0525007\n",
      "Epoch: 0570 Training MAE= 3.395919996\n",
      "Validation MAE:  3.4567137\n",
      "Test MAE:  4.0512457\n",
      "Epoch: 0571 Training MAE= 3.398413737\n",
      "Validation MAE:  3.4545424\n",
      "Test MAE:  4.044301\n",
      "Epoch: 0572 Training MAE= 3.389821908\n",
      "Validation MAE:  3.4591057\n",
      "Test MAE:  4.048994\n",
      "Epoch: 0573 Training MAE= 3.387244220\n",
      "Validation MAE:  3.456885\n",
      "Test MAE:  4.0434327\n",
      "Epoch: 0574 Training MAE= 3.388741135\n",
      "Validation MAE:  3.4863126\n",
      "Test MAE:  4.0795755\n",
      "Epoch: 0575 Training MAE= 3.394519562\n",
      "Validation MAE:  3.461714\n",
      "Test MAE:  4.048871\n",
      "Epoch: 0576 Training MAE= 3.417393317\n",
      "Validation MAE:  3.559492\n",
      "Test MAE:  4.141961\n",
      "Epoch: 0577 Training MAE= 3.452444254\n",
      "Validation MAE:  3.4848359\n",
      "Test MAE:  4.0880313\n",
      "Epoch: 0578 Training MAE= 3.405664822\n",
      "Validation MAE:  3.4485586\n",
      "Test MAE:  4.0475893\n",
      "Epoch: 0579 Training MAE= 3.391817222\n",
      "Validation MAE:  3.452608\n",
      "Test MAE:  4.0481315\n",
      "Epoch: 0580 Training MAE= 3.383637867\n",
      "Validation MAE:  3.4580698\n",
      "Test MAE:  4.054933\n",
      "Epoch: 0581 Training MAE= 3.381338437\n",
      "Validation MAE:  3.4609776\n",
      "Test MAE:  4.057095\n",
      "Epoch: 0582 Training MAE= 3.378930027\n",
      "Validation MAE:  3.448971\n",
      "Test MAE:  4.043469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0583 Training MAE= 3.378702009\n",
      "Validation MAE:  3.448472\n",
      "Test MAE:  4.0442734\n",
      "Epoch: 0584 Training MAE= 3.378942022\n",
      "Validation MAE:  3.4531324\n",
      "Test MAE:  4.0481424\n",
      "Epoch: 0585 Training MAE= 3.383312707\n",
      "Validation MAE:  3.45059\n",
      "Test MAE:  4.04531\n",
      "Epoch: 0586 Training MAE= 3.399383428\n",
      "Validation MAE:  3.4606414\n",
      "Test MAE:  4.0454273\n",
      "Epoch: 0587 Training MAE= 3.396964213\n",
      "Validation MAE:  3.4788404\n",
      "Test MAE:  4.064341\n",
      "Epoch: 0588 Training MAE= 3.387770117\n",
      "Validation MAE:  3.452789\n",
      "Test MAE:  4.0435486\n",
      "Epoch: 0589 Training MAE= 3.379893576\n",
      "Validation MAE:  3.4528625\n",
      "Test MAE:  4.045002\n",
      "Epoch: 0590 Training MAE= 3.392705185\n",
      "Validation MAE:  3.4604762\n",
      "Test MAE:  4.0521607\n",
      "Epoch: 0591 Training MAE= 3.391719852\n",
      "Validation MAE:  3.4682753\n",
      "Test MAE:  4.0551553\n",
      "Epoch: 0592 Training MAE= 3.393966001\n",
      "Validation MAE:  3.4670389\n",
      "Test MAE:  4.049082\n",
      "Epoch: 0593 Training MAE= 3.385638253\n",
      "Validation MAE:  3.454231\n",
      "Test MAE:  4.043989\n",
      "Epoch: 0594 Training MAE= 3.383753101\n",
      "Validation MAE:  3.4594946\n",
      "Test MAE:  4.051835\n",
      "Epoch: 0595 Training MAE= 3.388708458\n",
      "Validation MAE:  3.4882984\n",
      "Test MAE:  4.0862875\n",
      "Epoch: 0596 Training MAE= 3.389282527\n",
      "Validation MAE:  3.4510622\n",
      "Test MAE:  4.0474095\n",
      "Epoch: 0597 Training MAE= 3.384147520\n",
      "Validation MAE:  3.456319\n",
      "Test MAE:  4.046651\n",
      "Epoch: 0598 Training MAE= 3.376754895\n",
      "Validation MAE:  3.4664156\n",
      "Test MAE:  4.046338\n",
      "Epoch: 0599 Training MAE= 3.411756732\n",
      "Validation MAE:  3.526975\n",
      "Test MAE:  4.106084\n",
      "Epoch: 0600 Training MAE= 3.400048161\n",
      "Validation MAE:  3.4932284\n",
      "Test MAE:  4.0842834\n",
      "Epoch: 0601 Training MAE= 3.397091051\n",
      "Validation MAE:  3.46982\n",
      "Test MAE:  4.058828\n",
      "Epoch: 0602 Training MAE= 3.377551698\n",
      "Validation MAE:  3.4503527\n",
      "Test MAE:  4.044259\n",
      "Epoch: 0603 Training MAE= 3.373022779\n",
      "Validation MAE:  3.4476833\n",
      "Test MAE:  4.041793\n",
      "Epoch: 0604 Training MAE= 3.378138493\n",
      "Validation MAE:  3.4494104\n",
      "Test MAE:  4.042729\n",
      "Epoch: 0605 Training MAE= 3.377617788\n",
      "Validation MAE:  3.5099201\n",
      "Test MAE:  4.099024\n",
      "Epoch: 0606 Training MAE= 3.385609964\n",
      "Validation MAE:  3.4698894\n",
      "Test MAE:  4.054849\n",
      "Epoch: 0607 Training MAE= 3.428579150\n",
      "Validation MAE:  3.4804916\n",
      "Test MAE:  4.059233\n",
      "Epoch: 0608 Training MAE= 3.386929748\n",
      "Validation MAE:  3.4609907\n",
      "Test MAE:  4.0457826\n",
      "Epoch: 0609 Training MAE= 3.380595173\n",
      "Validation MAE:  3.456309\n",
      "Test MAE:  4.043898\n",
      "Epoch: 0610 Training MAE= 3.374557949\n",
      "Validation MAE:  3.4490204\n",
      "Test MAE:  4.038665\n",
      "Epoch: 0611 Training MAE= 3.385091617\n",
      "Validation MAE:  3.4689243\n",
      "Test MAE:  4.0565114\n",
      "Epoch: 0612 Training MAE= 3.375560569\n",
      "Validation MAE:  3.4525197\n",
      "Test MAE:  4.045624\n",
      "Epoch: 0613 Training MAE= 3.390857457\n",
      "Validation MAE:  3.5122538\n",
      "Test MAE:  4.1016603\n",
      "Epoch: 0614 Training MAE= 3.385540033\n",
      "Validation MAE:  3.4534202\n",
      "Test MAE:  4.0392876\n",
      "Epoch: 0615 Training MAE= 3.383272314\n",
      "Validation MAE:  3.4774323\n",
      "Test MAE:  4.0699487\n",
      "Epoch: 0616 Training MAE= 3.377393719\n",
      "Validation MAE:  3.448101\n",
      "Test MAE:  4.0430655\n",
      "Epoch: 0617 Training MAE= 3.379478170\n",
      "Validation MAE:  3.4523509\n",
      "Test MAE:  4.0416527\n",
      "Epoch: 0618 Training MAE= 3.381486043\n",
      "Validation MAE:  3.4679701\n",
      "Test MAE:  4.062018\n",
      "Epoch: 0619 Training MAE= 3.385831779\n",
      "Validation MAE:  3.4515471\n",
      "Test MAE:  4.0448585\n",
      "Epoch: 0620 Training MAE= 3.383365794\n",
      "Validation MAE:  3.457072\n",
      "Test MAE:  4.0422115\n",
      "Epoch: 0621 Training MAE= 3.381765060\n",
      "Validation MAE:  3.4494789\n",
      "Test MAE:  4.0375376\n",
      "Epoch: 0622 Training MAE= 3.382139724\n",
      "Validation MAE:  3.4754136\n",
      "Test MAE:  4.0598702\n",
      "Epoch: 0623 Training MAE= 3.379285987\n",
      "Validation MAE:  3.448696\n",
      "Test MAE:  4.042701\n",
      "Epoch: 0624 Training MAE= 3.375051861\n",
      "Validation MAE:  3.4503047\n",
      "Test MAE:  4.042534\n",
      "Epoch: 0625 Training MAE= 3.395436469\n",
      "Validation MAE:  3.4800553\n",
      "Test MAE:  4.0652914\n",
      "Epoch: 0626 Training MAE= 3.378155731\n",
      "Validation MAE:  3.448513\n",
      "Test MAE:  4.040288\n",
      "Epoch: 0627 Training MAE= 3.379522686\n",
      "Validation MAE:  3.4738097\n",
      "Test MAE:  4.051067\n",
      "Epoch: 0628 Training MAE= 3.379319215\n",
      "Validation MAE:  3.4506552\n",
      "Test MAE:  4.0424757\n",
      "Epoch: 0629 Training MAE= 3.389668749\n",
      "Validation MAE:  3.5155332\n",
      "Test MAE:  4.110312\n",
      "Epoch: 0630 Training MAE= 3.386172283\n",
      "Validation MAE:  3.4608579\n",
      "Test MAE:  4.055748\n",
      "Epoch: 0631 Training MAE= 3.372570384\n",
      "Validation MAE:  3.4508104\n",
      "Test MAE:  4.0426617\n",
      "Epoch: 0632 Training MAE= 3.369671450\n",
      "Validation MAE:  3.448495\n",
      "Test MAE:  4.039042\n",
      "Epoch: 0633 Training MAE= 3.382239275\n",
      "Validation MAE:  3.4477394\n",
      "Test MAE:  4.041845\n",
      "Epoch: 0634 Training MAE= 3.377756366\n",
      "Validation MAE:  3.4491875\n",
      "Test MAE:  4.0370264\n",
      "Epoch: 0635 Training MAE= 3.381446460\n",
      "Validation MAE:  3.454258\n",
      "Test MAE:  4.034341\n",
      "Epoch: 0636 Training MAE= 3.377492982\n",
      "Validation MAE:  3.4496021\n",
      "Test MAE:  4.0372443\n",
      "Epoch: 0637 Training MAE= 3.373110486\n",
      "Validation MAE:  3.4526525\n",
      "Test MAE:  4.04416\n",
      "Epoch: 0638 Training MAE= 3.373683948\n",
      "Validation MAE:  3.4518673\n",
      "Test MAE:  4.039088\n",
      "Epoch: 0639 Training MAE= 3.384219433\n",
      "Validation MAE:  3.449666\n",
      "Test MAE:  4.0458603\n",
      "Epoch: 0640 Training MAE= 3.379055959\n",
      "Validation MAE:  3.4495945\n",
      "Test MAE:  4.037556\n",
      "Epoch: 0641 Training MAE= 3.392155753\n",
      "Validation MAE:  3.457222\n",
      "Test MAE:  4.0378275\n",
      "Epoch: 0642 Training MAE= 3.371309420\n",
      "Validation MAE:  3.4451544\n",
      "Test MAE:  4.035242\n",
      "Epoch: 0643 Training MAE= 3.366522786\n",
      "Validation MAE:  3.4476469\n",
      "Test MAE:  4.0321884\n",
      "Epoch: 0644 Training MAE= 3.378535487\n",
      "Validation MAE:  3.4504406\n",
      "Test MAE:  4.032332\n",
      "Epoch: 0645 Training MAE= 3.373662025\n",
      "Validation MAE:  3.4452033\n",
      "Test MAE:  4.0343947\n",
      "Epoch: 0646 Training MAE= 3.373999153\n",
      "Validation MAE:  3.47043\n",
      "Test MAE:  4.0544977\n",
      "Epoch: 0647 Training MAE= 3.371035455\n",
      "Validation MAE:  3.4648108\n",
      "Test MAE:  4.0491037\n",
      "Epoch: 0648 Training MAE= 3.377653156\n",
      "Validation MAE:  3.4543653\n",
      "Test MAE:  4.039986\n",
      "Epoch: 0649 Training MAE= 3.368657306\n",
      "Validation MAE:  3.4518683\n",
      "Test MAE:  4.034455\n",
      "Epoch: 0650 Training MAE= 3.389679500\n",
      "Validation MAE:  3.4751182\n",
      "Test MAE:  4.069987\n",
      "Epoch: 0651 Training MAE= 3.379860484\n",
      "Validation MAE:  3.4647024\n",
      "Test MAE:  4.040584\n",
      "Epoch: 0652 Training MAE= 3.371459666\n",
      "Validation MAE:  3.4513843\n",
      "Test MAE:  4.0407286\n",
      "Epoch: 0653 Training MAE= 3.371415985\n",
      "Validation MAE:  3.4448268\n",
      "Test MAE:  4.030079\n",
      "Epoch: 0654 Training MAE= 3.378876632\n",
      "Validation MAE:  3.4475338\n",
      "Test MAE:  4.0412974\n",
      "Epoch: 0655 Training MAE= 3.369642848\n",
      "Validation MAE:  3.4603264\n",
      "Test MAE:  4.044206\n",
      "Epoch: 0656 Training MAE= 3.366523552\n",
      "Validation MAE:  3.4514933\n",
      "Test MAE:  4.038955\n",
      "Epoch: 0657 Training MAE= 3.409239519\n",
      "Validation MAE:  3.499806\n",
      "Test MAE:  4.093805\n",
      "Epoch: 0658 Training MAE= 3.381504818\n",
      "Validation MAE:  3.4598148\n",
      "Test MAE:  4.054318\n",
      "Epoch: 0659 Training MAE= 3.367170618\n",
      "Validation MAE:  3.4448547\n",
      "Test MAE:  4.0305424\n",
      "Epoch: 0660 Training MAE= 3.364025489\n",
      "Validation MAE:  3.457136\n",
      "Test MAE:  4.0541615\n",
      "Epoch: 0661 Training MAE= 3.370788164\n",
      "Validation MAE:  3.4496167\n",
      "Test MAE:  4.038957\n",
      "Epoch: 0662 Training MAE= 3.369663987\n",
      "Validation MAE:  3.4458768\n",
      "Test MAE:  4.031063\n",
      "Epoch: 0663 Training MAE= 3.366998845\n",
      "Validation MAE:  3.4422717\n",
      "Test MAE:  4.031335\n",
      "Epoch: 0664 Training MAE= 3.367920420\n",
      "Validation MAE:  3.4433763\n",
      "Test MAE:  4.032492\n",
      "Epoch: 0665 Training MAE= 3.370280246\n",
      "Validation MAE:  3.4694412\n",
      "Test MAE:  4.0578146\n",
      "Epoch: 0666 Training MAE= 3.364263349\n",
      "Validation MAE:  3.4648964\n",
      "Test MAE:  4.0572324\n",
      "Epoch: 0667 Training MAE= 3.369005440\n",
      "Validation MAE:  3.4540837\n",
      "Test MAE:  4.0474286\n",
      "Epoch: 0668 Training MAE= 3.391661057\n",
      "Validation MAE:  3.503234\n",
      "Test MAE:  4.0852933\n",
      "Epoch: 0669 Training MAE= 3.396235390\n",
      "Validation MAE:  3.4863782\n",
      "Test MAE:  4.06977\n",
      "Epoch: 0670 Training MAE= 3.414488269\n",
      "Validation MAE:  3.4429855\n",
      "Test MAE:  4.0417647\n",
      "Epoch: 0671 Training MAE= 3.382694242\n",
      "Validation MAE:  3.4372532\n",
      "Test MAE:  4.030265\n",
      "Epoch: 0672 Training MAE= 3.409980186\n",
      "Validation MAE:  3.4452899\n",
      "Test MAE:  4.052723\n",
      "Epoch: 0673 Training MAE= 3.401652549\n",
      "Validation MAE:  3.47219\n",
      "Test MAE:  4.070335\n",
      "Epoch: 0674 Training MAE= 3.389404627\n",
      "Validation MAE:  3.4489114\n",
      "Test MAE:  4.045418\n",
      "Epoch: 0675 Training MAE= 3.382377983\n",
      "Validation MAE:  3.4409385\n",
      "Test MAE:  4.041981\n",
      "Epoch: 0676 Training MAE= 3.377077874\n",
      "Validation MAE:  3.436815\n",
      "Test MAE:  4.039294\n",
      "Epoch: 0677 Training MAE= 3.375883692\n",
      "Validation MAE:  3.4349859\n",
      "Test MAE:  4.04265\n",
      "Epoch: 0678 Training MAE= 3.381065539\n",
      "Validation MAE:  3.4500165\n",
      "Test MAE:  4.0613055\n",
      "Epoch: 0679 Training MAE= 3.380242198\n",
      "Validation MAE:  3.445747\n",
      "Test MAE:  4.05854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0680 Training MAE= 3.378124442\n",
      "Validation MAE:  3.4400384\n",
      "Test MAE:  4.0530868\n",
      "Epoch: 0681 Training MAE= 3.375187325\n",
      "Validation MAE:  3.4354537\n",
      "Test MAE:  4.050964\n",
      "Epoch: 0682 Training MAE= 3.370306543\n",
      "Validation MAE:  3.4288678\n",
      "Test MAE:  4.0413275\n",
      "Epoch: 0683 Training MAE= 3.367162133\n",
      "Validation MAE:  3.4261377\n",
      "Test MAE:  4.042942\n",
      "Epoch: 0684 Training MAE= 3.365508987\n",
      "Validation MAE:  3.4255018\n",
      "Test MAE:  4.040661\n",
      "Epoch: 0685 Training MAE= 3.363607303\n",
      "Validation MAE:  3.4313474\n",
      "Test MAE:  4.0518737\n",
      "Epoch: 0686 Training MAE= 3.368621872\n",
      "Validation MAE:  3.4276583\n",
      "Test MAE:  4.0367327\n",
      "Epoch: 0687 Training MAE= 3.367563640\n",
      "Validation MAE:  3.4276752\n",
      "Test MAE:  4.037343\n",
      "Epoch: 0688 Training MAE= 3.360158742\n",
      "Validation MAE:  3.4254456\n",
      "Test MAE:  4.0389957\n",
      "Epoch: 0689 Training MAE= 3.353885134\n",
      "Validation MAE:  3.426425\n",
      "Test MAE:  4.0358086\n",
      "Epoch: 0690 Training MAE= 3.353435421\n",
      "Validation MAE:  3.4284084\n",
      "Test MAE:  4.03646\n",
      "Epoch: 0691 Training MAE= 3.351866168\n",
      "Validation MAE:  3.4291756\n",
      "Test MAE:  4.03753\n",
      "Epoch: 0692 Training MAE= 3.351092215\n",
      "Validation MAE:  3.4275403\n",
      "Test MAE:  4.0354347\n",
      "Epoch: 0693 Training MAE= 3.355211532\n",
      "Validation MAE:  3.4331524\n",
      "Test MAE:  4.040664\n",
      "Epoch: 0694 Training MAE= 3.363959621\n",
      "Validation MAE:  3.4304879\n",
      "Test MAE:  4.0390296\n",
      "Epoch: 0695 Training MAE= 3.356203000\n",
      "Validation MAE:  3.4494524\n",
      "Test MAE:  4.0516906\n",
      "Epoch: 0696 Training MAE= 3.370921449\n",
      "Validation MAE:  3.443497\n",
      "Test MAE:  4.031687\n",
      "Epoch: 0697 Training MAE= 3.361663918\n",
      "Validation MAE:  3.4558043\n",
      "Test MAE:  4.0505366\n",
      "Epoch: 0698 Training MAE= 3.361255483\n",
      "Validation MAE:  3.4433348\n",
      "Test MAE:  4.0386643\n",
      "Epoch: 0699 Training MAE= 3.361282005\n",
      "Validation MAE:  3.4545863\n",
      "Test MAE:  4.05115\n",
      "Epoch: 0700 Training MAE= 3.369246483\n",
      "Validation MAE:  3.4752185\n",
      "Test MAE:  4.0774484\n",
      "Epoch: 0701 Training MAE= 3.369093235\n",
      "Validation MAE:  3.4395514\n",
      "Test MAE:  4.0397253\n",
      "Epoch: 0702 Training MAE= 3.357984898\n",
      "Validation MAE:  3.429443\n",
      "Test MAE:  4.037266\n",
      "Epoch: 0703 Training MAE= 3.361564242\n",
      "Validation MAE:  3.4343753\n",
      "Test MAE:  4.037069\n",
      "Epoch: 0704 Training MAE= 3.368164771\n",
      "Validation MAE:  3.4365177\n",
      "Test MAE:  4.0327244\n",
      "Epoch: 0705 Training MAE= 3.362986713\n",
      "Validation MAE:  3.4589992\n",
      "Test MAE:  4.0467668\n",
      "Epoch: 0706 Training MAE= 3.359865313\n",
      "Validation MAE:  3.4528594\n",
      "Test MAE:  4.039725\n",
      "Epoch: 0707 Training MAE= 3.358144761\n",
      "Validation MAE:  3.445472\n",
      "Test MAE:  4.031929\n",
      "Epoch: 0708 Training MAE= 3.358368747\n",
      "Validation MAE:  3.4444003\n",
      "Test MAE:  4.032146\n",
      "Epoch: 0709 Training MAE= 3.379810830\n",
      "Validation MAE:  3.472901\n",
      "Test MAE:  4.0583816\n",
      "Epoch: 0710 Training MAE= 3.388409639\n",
      "Validation MAE:  3.4643629\n",
      "Test MAE:  4.058525\n",
      "Epoch: 0711 Training MAE= 3.360489433\n",
      "Validation MAE:  3.442491\n",
      "Test MAE:  4.0311794\n",
      "Epoch: 0712 Training MAE= 3.355868117\n",
      "Validation MAE:  3.4433434\n",
      "Test MAE:  4.031512\n",
      "Epoch: 0713 Training MAE= 3.359348059\n",
      "Validation MAE:  3.4514086\n",
      "Test MAE:  4.0326014\n",
      "Epoch: 0714 Training MAE= 3.358441458\n",
      "Validation MAE:  3.4369943\n",
      "Test MAE:  4.030095\n",
      "Epoch: 0715 Training MAE= 3.362771920\n",
      "Validation MAE:  3.4455857\n",
      "Test MAE:  4.033782\n",
      "Epoch: 0716 Training MAE= 3.358868458\n",
      "Validation MAE:  3.4436958\n",
      "Test MAE:  4.0252004\n",
      "Epoch: 0717 Training MAE= 3.369611386\n",
      "Validation MAE:  3.4823906\n",
      "Test MAE:  4.075029\n",
      "Epoch: 0718 Training MAE= 3.376956161\n",
      "Validation MAE:  3.4799047\n",
      "Test MAE:  4.084065\n",
      "Epoch: 0719 Training MAE= 3.393365409\n",
      "Validation MAE:  3.4362278\n",
      "Test MAE:  4.030166\n",
      "Epoch: 0720 Training MAE= 3.376257383\n",
      "Validation MAE:  3.4327934\n",
      "Test MAE:  4.0250044\n",
      "Epoch: 0721 Training MAE= 3.390251793\n",
      "Validation MAE:  3.434796\n",
      "Test MAE:  4.0437746\n",
      "Epoch: 0722 Training MAE= 3.392636779\n",
      "Validation MAE:  3.4605126\n",
      "Test MAE:  4.0688915\n",
      "Epoch: 0723 Training MAE= 3.385270679\n",
      "Validation MAE:  3.4529343\n",
      "Test MAE:  4.050581\n",
      "Epoch: 0724 Training MAE= 3.375012174\n",
      "Validation MAE:  3.4545832\n",
      "Test MAE:  4.052858\n",
      "Epoch: 0725 Training MAE= 3.371341247\n",
      "Validation MAE:  3.4455392\n",
      "Test MAE:  4.0495515\n",
      "Epoch: 0726 Training MAE= 3.368397244\n",
      "Validation MAE:  3.4401112\n",
      "Test MAE:  4.0460033\n",
      "Epoch: 0727 Training MAE= 3.363995129\n",
      "Validation MAE:  3.4369698\n",
      "Test MAE:  4.0428815\n",
      "Epoch: 0728 Training MAE= 3.363422736\n",
      "Validation MAE:  3.4347177\n",
      "Test MAE:  4.0397415\n",
      "Epoch: 0729 Training MAE= 3.362699630\n",
      "Validation MAE:  3.4313898\n",
      "Test MAE:  4.0395145\n",
      "Epoch: 0730 Training MAE= 3.361948810\n",
      "Validation MAE:  3.4287293\n",
      "Test MAE:  4.0378065\n",
      "Epoch: 0731 Training MAE= 3.362264898\n",
      "Validation MAE:  3.4286332\n",
      "Test MAE:  4.03617\n",
      "Epoch: 0732 Training MAE= 3.360507845\n",
      "Validation MAE:  3.4236276\n",
      "Test MAE:  4.037103\n",
      "Epoch: 0733 Training MAE= 3.359285428\n",
      "Validation MAE:  3.421437\n",
      "Test MAE:  4.0328317\n",
      "Epoch: 0734 Training MAE= 3.358691320\n",
      "Validation MAE:  3.4191666\n",
      "Test MAE:  4.0376687\n",
      "Epoch: 0735 Training MAE= 3.356189581\n",
      "Validation MAE:  3.4203377\n",
      "Test MAE:  4.0329857\n",
      "Epoch: 0736 Training MAE= 3.352985229\n",
      "Validation MAE:  3.4196255\n",
      "Test MAE:  4.0365577\n",
      "Epoch: 0737 Training MAE= 3.349416460\n",
      "Validation MAE:  3.420451\n",
      "Test MAE:  4.0320506\n",
      "Epoch: 0738 Training MAE= 3.346262826\n",
      "Validation MAE:  3.4196498\n",
      "Test MAE:  4.0318193\n",
      "Epoch: 0739 Training MAE= 3.348147230\n",
      "Validation MAE:  3.4367945\n",
      "Test MAE:  4.041129\n",
      "Epoch: 0740 Training MAE= 3.350222965\n",
      "Validation MAE:  3.4320586\n",
      "Test MAE:  4.034492\n",
      "Epoch: 0741 Training MAE= 3.345875477\n",
      "Validation MAE:  3.4343097\n",
      "Test MAE:  4.033273\n",
      "Epoch: 0742 Training MAE= 3.354331186\n",
      "Validation MAE:  3.4664826\n",
      "Test MAE:  4.0677023\n",
      "Epoch: 0743 Training MAE= 3.355678469\n",
      "Validation MAE:  3.4299529\n",
      "Test MAE:  4.033024\n",
      "Epoch: 0744 Training MAE= 3.350610707\n",
      "Validation MAE:  3.4436169\n",
      "Test MAE:  4.0404425\n",
      "Epoch: 0745 Training MAE= 3.347070993\n",
      "Validation MAE:  3.4354062\n",
      "Test MAE:  4.026854\n",
      "Epoch: 0746 Training MAE= 3.344668480\n",
      "Validation MAE:  3.4289963\n",
      "Test MAE:  4.0266304\n",
      "Epoch: 0747 Training MAE= 3.349702840\n",
      "Validation MAE:  3.4488015\n",
      "Test MAE:  4.040684\n",
      "Epoch: 0748 Training MAE= 3.360631817\n",
      "Validation MAE:  3.4465475\n",
      "Test MAE:  4.0318394\n",
      "Epoch: 0749 Training MAE= 3.350380174\n",
      "Validation MAE:  3.4332495\n",
      "Test MAE:  4.0272155\n",
      "Epoch: 0750 Training MAE= 3.368097810\n",
      "Validation MAE:  3.4934022\n",
      "Test MAE:  4.0892196\n",
      "Epoch: 0751 Training MAE= 3.402979750\n",
      "Validation MAE:  3.4516928\n",
      "Test MAE:  4.0303335\n",
      "Epoch: 0752 Training MAE= 3.391603909\n",
      "Validation MAE:  3.427631\n",
      "Test MAE:  4.033179\n",
      "Epoch: 0753 Training MAE= 3.377172121\n",
      "Validation MAE:  3.422049\n",
      "Test MAE:  4.029542\n",
      "Epoch: 0754 Training MAE= 3.385387809\n",
      "Validation MAE:  3.4789464\n",
      "Test MAE:  4.0888443\n",
      "Epoch: 0755 Training MAE= 3.378471660\n",
      "Validation MAE:  3.4430492\n",
      "Test MAE:  4.052341\n",
      "Epoch: 0756 Training MAE= 3.363790481\n",
      "Validation MAE:  3.4431984\n",
      "Test MAE:  4.0493507\n",
      "Epoch: 0757 Training MAE= 3.361607230\n",
      "Validation MAE:  3.4386644\n",
      "Test MAE:  4.045312\n",
      "Epoch: 0758 Training MAE= 3.358925664\n",
      "Validation MAE:  3.4352174\n",
      "Test MAE:  4.042454\n",
      "Epoch: 0759 Training MAE= 3.357693638\n",
      "Validation MAE:  3.435643\n",
      "Test MAE:  4.041653\n",
      "Epoch: 0760 Training MAE= 3.354634212\n",
      "Validation MAE:  3.4318004\n",
      "Test MAE:  4.0377297\n",
      "Epoch: 0761 Training MAE= 3.355578891\n",
      "Validation MAE:  3.430367\n",
      "Test MAE:  4.0388093\n",
      "Epoch: 0762 Training MAE= 3.353875455\n",
      "Validation MAE:  3.424754\n",
      "Test MAE:  4.0330286\n",
      "Epoch: 0763 Training MAE= 3.351647440\n",
      "Validation MAE:  3.4184113\n",
      "Test MAE:  4.027504\n",
      "Epoch: 0764 Training MAE= 3.349406091\n",
      "Validation MAE:  3.4179862\n",
      "Test MAE:  4.0238194\n",
      "Epoch: 0765 Training MAE= 3.348516526\n",
      "Validation MAE:  3.4184337\n",
      "Test MAE:  4.0255685\n",
      "Epoch: 0766 Training MAE= 3.347167676\n",
      "Validation MAE:  3.4181802\n",
      "Test MAE:  4.0248976\n",
      "Epoch: 0767 Training MAE= 3.346386143\n",
      "Validation MAE:  3.4184175\n",
      "Test MAE:  4.021107\n",
      "Epoch: 0768 Training MAE= 3.343631015\n",
      "Validation MAE:  3.4176257\n",
      "Test MAE:  4.020306\n",
      "Epoch: 0769 Training MAE= 3.341459388\n",
      "Validation MAE:  3.4176552\n",
      "Test MAE:  4.023436\n",
      "Epoch: 0770 Training MAE= 3.340579969\n",
      "Validation MAE:  3.4204574\n",
      "Test MAE:  4.0280867\n",
      "Epoch: 0771 Training MAE= 3.342220009\n",
      "Validation MAE:  3.4216464\n",
      "Test MAE:  4.0245214\n",
      "Epoch: 0772 Training MAE= 3.342924440\n",
      "Validation MAE:  3.4197938\n",
      "Test MAE:  4.025148\n",
      "Epoch: 0773 Training MAE= 3.342606885\n",
      "Validation MAE:  3.4201334\n",
      "Test MAE:  4.025589\n",
      "Epoch: 0774 Training MAE= 3.342486001\n",
      "Validation MAE:  3.4250317\n",
      "Test MAE:  4.027378\n",
      "Epoch: 0775 Training MAE= 3.341817169\n",
      "Validation MAE:  3.427575\n",
      "Test MAE:  4.0302854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0776 Training MAE= 3.342916944\n",
      "Validation MAE:  3.4284883\n",
      "Test MAE:  4.0328894\n",
      "Epoch: 0777 Training MAE= 3.351417240\n",
      "Validation MAE:  3.425878\n",
      "Test MAE:  4.0291095\n",
      "Epoch: 0778 Training MAE= 3.360204266\n",
      "Validation MAE:  3.4350593\n",
      "Test MAE:  4.037645\n",
      "Epoch: 0779 Training MAE= 3.354015666\n",
      "Validation MAE:  3.4249165\n",
      "Test MAE:  4.0228653\n",
      "Epoch: 0780 Training MAE= 3.345104480\n",
      "Validation MAE:  3.4243615\n",
      "Test MAE:  4.0206337\n",
      "Epoch: 0781 Training MAE= 3.340789752\n",
      "Validation MAE:  3.4214897\n",
      "Test MAE:  4.021012\n",
      "Epoch: 0782 Training MAE= 3.341903952\n",
      "Validation MAE:  3.4212468\n",
      "Test MAE:  4.025272\n",
      "Epoch: 0783 Training MAE= 3.351440430\n",
      "Validation MAE:  3.4377096\n",
      "Test MAE:  4.0316033\n",
      "Epoch: 0784 Training MAE= 3.352268477\n",
      "Validation MAE:  3.4318612\n",
      "Test MAE:  4.031846\n",
      "Epoch: 0785 Training MAE= 3.348260419\n",
      "Validation MAE:  3.421703\n",
      "Test MAE:  4.021438\n",
      "Epoch: 0786 Training MAE= 3.339682070\n",
      "Validation MAE:  3.4221509\n",
      "Test MAE:  4.019037\n",
      "Epoch: 0787 Training MAE= 3.351776192\n",
      "Validation MAE:  3.432535\n",
      "Test MAE:  4.0332904\n",
      "Epoch: 0788 Training MAE= 3.345233954\n",
      "Validation MAE:  3.4231982\n",
      "Test MAE:  4.0210047\n",
      "Epoch: 0789 Training MAE= 3.341815299\n",
      "Validation MAE:  3.4271626\n",
      "Test MAE:  4.030301\n",
      "Epoch: 0790 Training MAE= 3.350172647\n",
      "Validation MAE:  3.419397\n",
      "Test MAE:  4.024847\n",
      "Epoch: 0791 Training MAE= 3.364367355\n",
      "Validation MAE:  3.558837\n",
      "Test MAE:  4.1617713\n",
      "Epoch: 0792 Training MAE= 3.414842844\n",
      "Validation MAE:  3.4297214\n",
      "Test MAE:  4.0388618\n",
      "Epoch: 0793 Training MAE= 3.349764795\n",
      "Validation MAE:  3.418925\n",
      "Test MAE:  4.015035\n",
      "Epoch: 0794 Training MAE= 3.344514914\n",
      "Validation MAE:  3.4189842\n",
      "Test MAE:  4.0146203\n",
      "Epoch: 0795 Training MAE= 3.345975896\n",
      "Validation MAE:  3.4178865\n",
      "Test MAE:  4.0177526\n",
      "Epoch: 0796 Training MAE= 3.342273029\n",
      "Validation MAE:  3.4175065\n",
      "Test MAE:  4.0180497\n",
      "Epoch: 0797 Training MAE= 3.338327426\n",
      "Validation MAE:  3.4177823\n",
      "Test MAE:  4.021291\n",
      "Epoch: 0798 Training MAE= 3.336014305\n",
      "Validation MAE:  3.4175186\n",
      "Test MAE:  4.024106\n",
      "Epoch: 0799 Training MAE= 3.335034155\n",
      "Validation MAE:  3.415654\n",
      "Test MAE:  4.0211115\n",
      "Epoch: 0800 Training MAE= 3.337062714\n",
      "Validation MAE:  3.4195652\n",
      "Test MAE:  4.0273056\n",
      "Epoch: 0801 Training MAE= 3.339916731\n",
      "Validation MAE:  3.422855\n",
      "Test MAE:  4.028128\n",
      "Epoch: 0802 Training MAE= 3.340614931\n",
      "Validation MAE:  3.427012\n",
      "Test MAE:  4.0307894\n",
      "Epoch: 0803 Training MAE= 3.348294609\n",
      "Validation MAE:  3.4354813\n",
      "Test MAE:  4.0402064\n",
      "Epoch: 0804 Training MAE= 3.339799585\n",
      "Validation MAE:  3.420576\n",
      "Test MAE:  4.022085\n",
      "Epoch: 0805 Training MAE= 3.333710278\n",
      "Validation MAE:  3.424466\n",
      "Test MAE:  4.031031\n",
      "Epoch: 0806 Training MAE= 3.343743374\n",
      "Validation MAE:  3.4291012\n",
      "Test MAE:  4.031425\n",
      "Epoch: 0807 Training MAE= 3.382041873\n",
      "Validation MAE:  3.4296935\n",
      "Test MAE:  4.0277343\n",
      "Epoch: 0808 Training MAE= 3.401333901\n",
      "Validation MAE:  3.42717\n",
      "Test MAE:  4.038499\n",
      "Epoch: 0809 Training MAE= 3.372353999\n",
      "Validation MAE:  3.431912\n",
      "Test MAE:  4.0472403\n",
      "Epoch: 0810 Training MAE= 3.377796011\n",
      "Validation MAE:  3.489482\n",
      "Test MAE:  4.101919\n",
      "Epoch: 0811 Training MAE= 3.362626827\n",
      "Validation MAE:  3.455153\n",
      "Test MAE:  4.0590525\n",
      "Epoch: 0812 Training MAE= 3.352045644\n",
      "Validation MAE:  3.4437587\n",
      "Test MAE:  4.054903\n",
      "Epoch: 0813 Training MAE= 3.346158363\n",
      "Validation MAE:  3.4405184\n",
      "Test MAE:  4.048195\n",
      "Epoch: 0814 Training MAE= 3.344751416\n",
      "Validation MAE:  3.4354157\n",
      "Test MAE:  4.0493264\n",
      "Epoch: 0815 Training MAE= 3.346095667\n",
      "Validation MAE:  3.4392755\n",
      "Test MAE:  4.0507617\n",
      "Epoch: 0816 Training MAE= 3.343946997\n",
      "Validation MAE:  3.433231\n",
      "Test MAE:  4.0455503\n",
      "Epoch: 0817 Training MAE= 3.344413866\n",
      "Validation MAE:  3.4285076\n",
      "Test MAE:  4.0425634\n",
      "Epoch: 0818 Training MAE= 3.341590374\n",
      "Validation MAE:  3.424588\n",
      "Test MAE:  4.036201\n",
      "Epoch: 0819 Training MAE= 3.339919803\n",
      "Validation MAE:  3.4203317\n",
      "Test MAE:  4.0293965\n",
      "Epoch: 0820 Training MAE= 3.340318869\n",
      "Validation MAE:  3.4195647\n",
      "Test MAE:  4.031301\n",
      "Epoch: 0821 Training MAE= 3.339369229\n",
      "Validation MAE:  3.4209664\n",
      "Test MAE:  4.0333433\n",
      "Epoch: 0822 Training MAE= 3.339468179\n",
      "Validation MAE:  3.419885\n",
      "Test MAE:  4.0322824\n",
      "Epoch: 0823 Training MAE= 3.338742845\n",
      "Validation MAE:  3.421966\n",
      "Test MAE:  4.0291324\n",
      "Epoch: 0824 Training MAE= 3.335963944\n",
      "Validation MAE:  3.4176617\n",
      "Test MAE:  4.023886\n",
      "Epoch: 0825 Training MAE= 3.333687993\n",
      "Validation MAE:  3.4162846\n",
      "Test MAE:  4.021375\n",
      "Epoch: 0826 Training MAE= 3.332024757\n",
      "Validation MAE:  3.4123673\n",
      "Test MAE:  4.0180035\n",
      "Epoch: 0827 Training MAE= 3.329745046\n",
      "Validation MAE:  3.413514\n",
      "Test MAE:  4.0194583\n",
      "Epoch: 0828 Training MAE= 3.328261837\n",
      "Validation MAE:  3.412047\n",
      "Test MAE:  4.021191\n",
      "Epoch: 0829 Training MAE= 3.329925212\n",
      "Validation MAE:  3.4124887\n",
      "Test MAE:  4.0239315\n",
      "Epoch: 0830 Training MAE= 3.329069807\n",
      "Validation MAE:  3.4123769\n",
      "Test MAE:  4.0235395\n",
      "Epoch: 0831 Training MAE= 3.334109365\n",
      "Validation MAE:  3.4181216\n",
      "Test MAE:  4.0333004\n",
      "Epoch: 0832 Training MAE= 3.331854356\n",
      "Validation MAE:  3.415823\n",
      "Test MAE:  4.0274796\n",
      "Epoch: 0833 Training MAE= 3.350379529\n",
      "Validation MAE:  3.47181\n",
      "Test MAE:  4.0644703\n",
      "Epoch: 0834 Training MAE= 3.378814959\n",
      "Validation MAE:  3.4847627\n",
      "Test MAE:  4.0712156\n",
      "Epoch: 0835 Training MAE= 3.365118250\n",
      "Validation MAE:  3.4201748\n",
      "Test MAE:  4.0288343\n",
      "Epoch: 0836 Training MAE= 3.361941166\n",
      "Validation MAE:  3.4301436\n",
      "Test MAE:  4.040605\n",
      "Epoch: 0837 Training MAE= 3.362625457\n",
      "Validation MAE:  3.480823\n",
      "Test MAE:  4.101948\n",
      "Epoch: 0838 Training MAE= 3.359178492\n",
      "Validation MAE:  3.4505599\n",
      "Test MAE:  4.0643015\n",
      "Epoch: 0839 Training MAE= 3.345179802\n",
      "Validation MAE:  3.4470909\n",
      "Test MAE:  4.054592\n",
      "Epoch: 0840 Training MAE= 3.341727066\n",
      "Validation MAE:  3.4386833\n",
      "Test MAE:  4.048253\n",
      "Epoch: 0841 Training MAE= 3.335996680\n",
      "Validation MAE:  3.4341176\n",
      "Test MAE:  4.0409017\n",
      "Epoch: 0842 Training MAE= 3.334942197\n",
      "Validation MAE:  3.429345\n",
      "Test MAE:  4.034337\n",
      "Epoch: 0843 Training MAE= 3.332730829\n",
      "Validation MAE:  3.4220695\n",
      "Test MAE:  4.0238776\n",
      "Epoch: 0844 Training MAE= 3.332056586\n",
      "Validation MAE:  3.41854\n",
      "Test MAE:  4.021394\n",
      "Epoch: 0845 Training MAE= 3.329503575\n",
      "Validation MAE:  3.4226792\n",
      "Test MAE:  4.027537\n",
      "Epoch: 0846 Training MAE= 3.330346456\n",
      "Validation MAE:  3.424831\n",
      "Test MAE:  4.028921\n",
      "Epoch: 0847 Training MAE= 3.327989458\n",
      "Validation MAE:  3.4197538\n",
      "Test MAE:  4.021999\n",
      "Epoch: 0848 Training MAE= 3.324879915\n",
      "Validation MAE:  3.4138958\n",
      "Test MAE:  4.0161715\n",
      "Epoch: 0849 Training MAE= 3.323158559\n",
      "Validation MAE:  3.4138367\n",
      "Test MAE:  4.0160575\n",
      "Epoch: 0850 Training MAE= 3.321885121\n",
      "Validation MAE:  3.4131958\n",
      "Test MAE:  4.022944\n",
      "Epoch: 0851 Training MAE= 3.326888789\n",
      "Validation MAE:  3.4154546\n",
      "Test MAE:  4.022383\n",
      "Epoch: 0852 Training MAE= 3.324290777\n",
      "Validation MAE:  3.4191642\n",
      "Test MAE:  4.026721\n",
      "Epoch: 0853 Training MAE= 3.331893985\n",
      "Validation MAE:  3.4187284\n",
      "Test MAE:  4.0240374\n",
      "Epoch: 0854 Training MAE= 3.327878334\n",
      "Validation MAE:  3.425444\n",
      "Test MAE:  4.032905\n",
      "Epoch: 0855 Training MAE= 3.345974376\n",
      "Validation MAE:  3.499409\n",
      "Test MAE:  4.088953\n",
      "Epoch: 0856 Training MAE= 3.378552304\n",
      "Validation MAE:  3.4571435\n",
      "Test MAE:  4.0457425\n",
      "Epoch: 0857 Training MAE= 3.366808413\n",
      "Validation MAE:  3.4353213\n",
      "Test MAE:  4.0330725\n",
      "Epoch: 0858 Training MAE= 3.353970928\n",
      "Validation MAE:  3.4224946\n",
      "Test MAE:  4.0254216\n",
      "Epoch: 0859 Training MAE= 3.349506443\n",
      "Validation MAE:  3.4479587\n",
      "Test MAE:  4.05968\n",
      "Epoch: 0860 Training MAE= 3.358794628\n",
      "Validation MAE:  3.4618227\n",
      "Test MAE:  4.084042\n",
      "Epoch: 0861 Training MAE= 3.348120751\n",
      "Validation MAE:  3.4437094\n",
      "Test MAE:  4.049989\n",
      "Epoch: 0862 Training MAE= 3.338917594\n",
      "Validation MAE:  3.4418788\n",
      "Test MAE:  4.052826\n",
      "Epoch: 0863 Training MAE= 3.332312764\n",
      "Validation MAE:  3.438345\n",
      "Test MAE:  4.0435004\n",
      "Epoch: 0864 Training MAE= 3.328640199\n",
      "Validation MAE:  3.4317424\n",
      "Test MAE:  4.0337863\n",
      "Epoch: 0865 Training MAE= 3.326469558\n",
      "Validation MAE:  3.428332\n",
      "Test MAE:  4.027216\n",
      "Epoch: 0866 Training MAE= 3.326991940\n",
      "Validation MAE:  3.4308274\n",
      "Test MAE:  4.026954\n",
      "Epoch: 0867 Training MAE= 3.324676831\n",
      "Validation MAE:  3.4338646\n",
      "Test MAE:  4.033959\n",
      "Epoch: 0868 Training MAE= 3.323267127\n",
      "Validation MAE:  3.4327674\n",
      "Test MAE:  4.02959\n",
      "Epoch: 0869 Training MAE= 3.320255593\n",
      "Validation MAE:  3.4242666\n",
      "Test MAE:  4.017171\n",
      "Epoch: 0870 Training MAE= 3.316738452\n",
      "Validation MAE:  3.418662\n",
      "Test MAE:  4.0177736\n",
      "Epoch: 0871 Training MAE= 3.318721696\n",
      "Validation MAE:  3.4198666\n",
      "Test MAE:  4.0162253\n",
      "Epoch: 0872 Training MAE= 3.317741704\n",
      "Validation MAE:  3.421409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:  4.02964\n",
      "Epoch: 0873 Training MAE= 3.320278462\n",
      "Validation MAE:  3.4227035\n",
      "Test MAE:  4.0191083\n",
      "Epoch: 0874 Training MAE= 3.325115981\n",
      "Validation MAE:  3.4332185\n",
      "Test MAE:  4.0392957\n",
      "Epoch: 0875 Training MAE= 3.325323164\n",
      "Validation MAE:  3.423615\n",
      "Test MAE:  4.033794\n",
      "Epoch: 0876 Training MAE= 3.326362755\n",
      "Validation MAE:  3.463124\n",
      "Test MAE:  4.086125\n",
      "Epoch: 0877 Training MAE= 3.349543803\n",
      "Validation MAE:  3.4850647\n",
      "Test MAE:  4.0573034\n",
      "Epoch: 0878 Training MAE= 3.380113198\n",
      "Validation MAE:  3.476124\n",
      "Test MAE:  4.077391\n",
      "Epoch: 0879 Training MAE= 3.355803090\n",
      "Validation MAE:  3.4223359\n",
      "Test MAE:  4.0244484\n",
      "Epoch: 0880 Training MAE= 3.342521046\n",
      "Validation MAE:  3.419907\n",
      "Test MAE:  4.017391\n",
      "Epoch: 0881 Training MAE= 3.324024882\n",
      "Validation MAE:  3.4304838\n",
      "Test MAE:  4.022241\n",
      "Epoch: 0882 Training MAE= 3.320744923\n",
      "Validation MAE:  3.4206774\n",
      "Test MAE:  4.0114727\n",
      "Epoch: 0883 Training MAE= 3.315609789\n",
      "Validation MAE:  3.4212377\n",
      "Test MAE:  4.0136423\n",
      "Epoch: 0884 Training MAE= 3.315894366\n",
      "Validation MAE:  3.4177983\n",
      "Test MAE:  4.01557\n",
      "Epoch: 0885 Training MAE= 3.316681688\n",
      "Validation MAE:  3.418673\n",
      "Test MAE:  4.0218773\n",
      "Epoch: 0886 Training MAE= 3.320100232\n",
      "Validation MAE:  3.4329925\n",
      "Test MAE:  4.0400934\n",
      "Epoch: 0887 Training MAE= 3.324862387\n",
      "Validation MAE:  3.4208393\n",
      "Test MAE:  4.022363\n",
      "Epoch: 0888 Training MAE= 3.339862111\n",
      "Validation MAE:  3.435135\n",
      "Test MAE:  4.016032\n",
      "Epoch: 0889 Training MAE= 3.334350250\n",
      "Validation MAE:  3.4280956\n",
      "Test MAE:  4.0228267\n",
      "Epoch: 0890 Training MAE= 3.326715457\n",
      "Validation MAE:  3.4415197\n",
      "Test MAE:  4.0269094\n",
      "Epoch: 0891 Training MAE= 3.320785081\n",
      "Validation MAE:  3.4286635\n",
      "Test MAE:  4.0122848\n",
      "Epoch: 0892 Training MAE= 3.317787732\n",
      "Validation MAE:  3.420953\n",
      "Test MAE:  4.013338\n",
      "Epoch: 0893 Training MAE= 3.315722972\n",
      "Validation MAE:  3.42343\n",
      "Test MAE:  4.0136914\n",
      "Epoch: 0894 Training MAE= 3.324614329\n",
      "Validation MAE:  3.427813\n",
      "Test MAE:  4.0138497\n",
      "Epoch: 0895 Training MAE= 3.337496797\n",
      "Validation MAE:  3.425744\n",
      "Test MAE:  4.025709\n",
      "Epoch: 0896 Training MAE= 3.331429857\n",
      "Validation MAE:  3.4432967\n",
      "Test MAE:  4.023461\n",
      "Epoch: 0897 Training MAE= 3.319943730\n",
      "Validation MAE:  3.428374\n",
      "Test MAE:  4.019834\n",
      "Epoch: 0898 Training MAE= 3.323216438\n",
      "Validation MAE:  3.441929\n",
      "Test MAE:  4.0453863\n",
      "Epoch: 0899 Training MAE= 3.332072651\n",
      "Validation MAE:  3.4436593\n",
      "Test MAE:  4.030809\n",
      "Epoch: 0900 Training MAE= 3.331507089\n",
      "Validation MAE:  3.4427454\n",
      "Test MAE:  4.018785\n",
      "Epoch: 0901 Training MAE= 3.370981495\n",
      "Validation MAE:  3.454241\n",
      "Test MAE:  4.052648\n",
      "Epoch: 0902 Training MAE= 3.348787686\n",
      "Validation MAE:  3.4307225\n",
      "Test MAE:  4.028777\n",
      "Epoch: 0903 Training MAE= 3.351126091\n",
      "Validation MAE:  3.4306154\n",
      "Test MAE:  4.0277863\n",
      "Epoch: 0904 Training MAE= 3.352846136\n",
      "Validation MAE:  3.4781039\n",
      "Test MAE:  4.102522\n",
      "Epoch: 0905 Training MAE= 3.346686836\n",
      "Validation MAE:  3.4399564\n",
      "Test MAE:  4.0405564\n",
      "Epoch: 0906 Training MAE= 3.327204058\n",
      "Validation MAE:  3.445516\n",
      "Test MAE:  4.055738\n",
      "Epoch: 0907 Training MAE= 3.330555291\n",
      "Validation MAE:  3.4438484\n",
      "Test MAE:  4.055336\n",
      "Epoch: 0908 Training MAE= 3.326851509\n",
      "Validation MAE:  3.44245\n",
      "Test MAE:  4.0531588\n",
      "Epoch: 0909 Training MAE= 3.322502420\n",
      "Validation MAE:  3.436331\n",
      "Test MAE:  4.044704\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ea68ae98720e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     val_error, predic_res, test_Y,test_error, A1=gcn_corr_final(frequency, horizon, learning_rate, decay, batch_size, n_hidden_vec1,\n\u001b[0;32m---> 68\u001b[0;31m                                                                 n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;31m#val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m#print (\"finished A running: \", i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-7a2ff88f35bd>\u001b[0m in \u001b[0;36mgcn_corr_final\u001b[0;34m(frequency, horizon, learning_rate, decay, batch_size, n_hidden_vec1, n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n\u001b[1;32m    244\u001b[0m                                                       \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                                                               keep_prob: keep})\n\u001b[0m\u001b[1;32m    246\u001b[0m                 \u001b[0;31m#print (preds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;31m#print (trueval)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# change hidden number\n",
    "# change batch_size\n",
    "# set size\n",
    "#from bayes_opt import BayesianOptimization\n",
    "import datetime\n",
    "#freq_max = 12\n",
    "#time_step = 12\n",
    "learning_rate = 0.01\n",
    "decay = 0.9 \n",
    "batch_size = 500\n",
    "\n",
    "early_stop_th = 150\n",
    "training_epochs = 1000\n",
    "keep = 1#0.2\n",
    "#time_step_max = 10\n",
    "\n",
    "sn = 207 # station num\n",
    "#test = 2000 \n",
    "reg1 = 0.05#0.05\n",
    "reg2 = 0.05#0.1\n",
    "reg3 = 0.05\n",
    "frequency = 12\n",
    "horizon = 12\n",
    "n_hidden_vec1 = 10\n",
    "n_hidden_vec2 = 10\n",
    "n_hidden_vec3 = 20\n",
    "n_hidden_vec4 = 10\n",
    "#All_pred = np.empty([2000, 207])\n",
    "#All_Y = np.empty([2000, 207])\n",
    "\n",
    "#24*90\n",
    "#step = 0\n",
    "#gap = 100\n",
    "#training = 0.7\n",
    "#validation = 0.1\n",
    "#test = 0.2\n",
    "\n",
    "#gcn_corr_eval(7, 0.01, 0.5, 100, 0.4, 10, 5, 5, 0.2, 50, 500)\n",
    "\n",
    "\n",
    "\n",
    "rep = 1 # repeating times\n",
    "\n",
    "#total_sn = 0\n",
    "#num_iter = 50\n",
    "#init_points = 200\n",
    "\n",
    "\n",
    "# stdbscan\n",
    "#spatial_threshold = 300\n",
    "#temporal_threshold = 300\n",
    "#min_neighbors = 1 # number of neighbor\n",
    "\n",
    "#frequency2 = skip1 + freq_max + training\n",
    "\n",
    "#while step < 2000:\n",
    "\n",
    "#hourly_bike_cluster = hourly_bike\n",
    "best = -10000\n",
    "pre_best = []\n",
    "test_Y_best = []\n",
    "test_error_best = 1000\n",
    "A1_best = []\n",
    "# A2_best = []\n",
    "for i in range(rep):\n",
    "    a = datetime.datetime.now()\n",
    "    val_error, predic_res, test_Y,test_error, A1=gcn_corr_final(frequency, horizon, learning_rate, decay, batch_size, n_hidden_vec1,\n",
    "                                                                n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\n",
    "    #val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\n",
    "    #print (\"finished A running: \", i)\n",
    "    b = datetime.datetime.now()\n",
    "    print(b-a)\n",
    "    if val_error > best:\n",
    "        best = val_error\n",
    "        pre_best = predic_res\n",
    "        test_Y_best = test_Y\n",
    "        test_error_best = test_error\n",
    "        #A1_best = A1\n",
    "       # A2_best = A2\n",
    "\n",
    "    #val_error, predic_res, test_Y,test_error=gcn_corr_final(24, 0.02, 0.2, 100, 5, 10, 10, 0.8, 50, 500)\n",
    "    All_pred = pre_best\n",
    "    All_Y  = test_Y_best\n",
    "\n",
    "    #total_sn = total_sn + sn\n",
    "\n",
    "    #total_error = np.sqrt(np.mean((All_pred[0:(step+gap),0:total_sn] - All_Y[0:(step+gap),0:total_sn])**2))\n",
    "\n",
    "    #print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the cluster now is: \", c)\n",
    "    #print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the val error of this cluster now is: \", best)\n",
    "    #print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the test error by this cluster now is: \", total_error)\n",
    "    #step = step + gap\n",
    "    #skip1 = skip1 + gap\n",
    "    \n",
    "    #np.savetxt(\"prediction_2.csv\", All_pred, delimiter = ',')\n",
    "    #np.savetxt(\"prediction_Y_2.csv\", All_Y, delimiter = ',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_pred_tf = tf.convert_to_tensor(\n",
    "    All_pred,\n",
    "    dtype=np.float32,\n",
    "    name=None,\n",
    "    preferred_dtype=None\n",
    ")\n",
    "\n",
    "#All_Y = scaler.inverse_transform(All_Y)\n",
    "\n",
    "All_Y_tf =  tf.convert_to_tensor(\n",
    "    All_Y,\n",
    "    dtype=np.float32,\n",
    "    name=None,\n",
    "    preferred_dtype=None\n",
    ")\n",
    "\n",
    "predic_res_re = tf.reshape(All_pred_tf, [All_pred.shape[0], sn, horizon])\n",
    "\n",
    "test_Y_re = tf.reshape(All_Y_tf, [All_pred.shape[0], sn, horizon])\n",
    "\n",
    "cost_by_hor = []\n",
    "with tf.Session() as sess:  \n",
    "    for hor_i in range(horizon):\n",
    "        cost_by_hor.append(masked_mae_tf(predic_res_re[:, :, 0:hor_i+1], test_Y_re[:, :, 0:hor_i+1], 0).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_by_hor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../../data/trajectory/gcnn_prediction.csv\", All_pred, delimiter = ',')\n",
    "np.savetxt(\"../../data/trajectory/gcnn_prediction_Y.csv\", All_Y, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(6850), Dimension(2484)])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_pred_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
