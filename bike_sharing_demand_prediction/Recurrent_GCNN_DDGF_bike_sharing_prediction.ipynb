{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "#from tensorflow.contrib.rnn.python.ops import rnn_cell as RNNCell\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "import collections\n",
    "from tensorflow.contrib import rnn\n",
    "import h5py\n",
    "#from tensorflow.python.ops.rnn_cell_impl import _RNNCell \n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    #adj[np.isnan(adj)] = 0.\n",
    "    adj = tf.abs(adj)\n",
    "    rowsum = tf.reduce_sum(adj, 1)# sum by row\n",
    "\n",
    "    d_inv_sqrt = tf.pow(rowsum, -0.5)\n",
    "   \n",
    "    #d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    \n",
    "    d_mat_inv_sqrt = tf.diag(d_inv_sqrt)\n",
    "\n",
    "    return tf.matmul(tf.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "\n",
    "def masked_mae_tf(preds, labels, null_val=np.nan):\n",
    "    \"\"\"\n",
    "    Accuracy with masking.\n",
    "    :param preds:\n",
    "    :param labels:\n",
    "    :param null_val:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #print (preds.shape)\n",
    "    #print (labels.shape)\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~tf.is_nan(labels)\n",
    "    else:\n",
    "        mask = tf.not_equal(labels, null_val)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    mask = tf.where(tf.is_nan(mask), tf.zeros_like(mask), mask)\n",
    "    loss = tf.abs(tf.subtract(preds, labels))\n",
    "    loss = loss * mask\n",
    "    loss = tf.where(tf.is_nan(loss), tf.zeros_like(loss), loss)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def masked_mae_tf_by_horizon(preds, labels, null_val=np.nan):\n",
    "    \"\"\"\n",
    "    Accuracy with masking.\n",
    "    :param preds:\n",
    "    :param labels:\n",
    "    :param null_val:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    preds_reshape = tf.reshape(preds, [-1, sn, horizon])\n",
    "    labels_reshape = tf.reshape(labels, [-1, sn, horizon])\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for i in range(horizon):\n",
    "        labels = labels_reshape[:, :, 0:(i+1)]\n",
    "        preds = preds_reshape[:, :, 0:(i+1)]\n",
    "        \n",
    "        if np.isnan(null_val):\n",
    "            mask = ~tf.is_nan(labels)\n",
    "        else:\n",
    "            mask = tf.not_equal(labels, null_val)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        mask = tf.where(tf.is_nan(mask), tf.zeros_like(mask), mask)\n",
    "        loss = tf.abs(tf.subtract(preds, labels))\n",
    "        loss = loss * mask\n",
    "        loss = tf.where(tf.is_nan(loss), tf.zeros_like(loss), loss)\n",
    "        \n",
    "        res.append(tf.reduce_mean(loss))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    Standard the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"../../data/nyc_bike/NYCBikeHourly272.pickle\"\n",
    "fileObject = open(file_Name,'rb') \n",
    "hourly_bike = pickle.load(fileObject) \n",
    "hourly_bike = pd.DataFrame(hourly_bike)\n",
    "raw_data = pd.DataFrame(hourly_bike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26304, 272)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 272 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  262  263  264  265  \\\n",
       "0    2    1    0    3    0    0    0    4    1    2  ...    3    1    1    0   \n",
       "1    1    0    0    1    2    1    0    0    0    0  ...    0    2    1    0   \n",
       "2    0    0    0    0    0    3    0    0    0    0  ...    1    1    0    0   \n",
       "3    0    0    0    1    0    0    0    0    0    0  ...    0    1    0    0   \n",
       "4    0    0    0    1    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "   266  267  268  269  270  271  \n",
       "0    0    0    0    0    1    0  \n",
       "1    5    0    0    0    0    0  \n",
       "2    2    0    0    0    0    1  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 272 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revised based on https://github.com/transpaper/gconvRNN/blob/master/model.py\n",
    "\n",
    "def conv(x, ddgf, feat_out, K, W):\n",
    "    '''\n",
    "    x : [batch_size, N_node, feat_in] - input of each time step\n",
    "    nSample : number of samples = batch_size, let's say it is 100\n",
    "    nNode : number of node in graph\n",
    "    feat_in : number of input feature, usually is set as 1\n",
    "    feat_out : number of output feature\n",
    "    ddgf : data driven graph filter\n",
    "    K : size of kernel(number of cheby coefficients), is 1 in the fast graph paper\n",
    "    W : cheby_conv weight [K * feat_in, feat_out]\n",
    "    '''\n",
    "    if len(x.shape) == 2:\n",
    "        x = tf.expand_dims(x, 2) # extend a dimension \"feature_in\"\n",
    "    nSample, nNode, feat_in = x.get_shape()\n",
    "    #feat_in = 1\n",
    "    print (nSample, nNode, feat_in)\n",
    "    nSample, nNode, feat_in = int(nSample), int(nNode), int(feat_in)\n",
    "    \n",
    "    x0 = tf.transpose(x, perm=[1, 2, 0]) #change it to [nNode, feat_in, nSample]\n",
    "    x0 = tf.reshape(x0, [nNode, feat_in*nSample]) # feature_in = 1, [nNode, nSample]\n",
    "    #x = tf.expand_dims(x0, 0) # make it [1, nNode, feat_in*nSample]\n",
    "    \n",
    "    x0 = tf.matmul(ddgf, x0) # graph convolutional #[nNode, nSample]\n",
    "        \n",
    "    #x = tf.reshape(x, [K, nNode, feat_in, nSample])\n",
    "    #x = tf.transpose(x, perm=[3,1,2,0])\n",
    "    x0 = tf.reshape(x0, [nSample*nNode, feat_in*K]) #[nSample*nNode, 1]\n",
    "    \n",
    "    x = tf.matmul(x0, W) #No Bias term?? -> Yes\n",
    "    out = tf.reshape(x, [nSample, nNode, feat_out]) \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def gcn(x, weights, biases, batch_size, n_input, frequency,flag, n_output_vec):\n",
    "    # Hidden layer with RELU activation\n",
    "    \n",
    "   # output_list = tf.Variable(tf.zeros([n_output_vec,1]),dtype=tf.float32) #'Tensor' object does not support item assignment, cant build Ypre\n",
    "\n",
    "    #Xtem = tf.reshape(x[i,:], [n_input, frequency])\n",
    "    #Xtem = tf.transpose(Xtem)\n",
    "    #Atem = tf.convert_to_tensor(A_whole_final, dtype=np.float32)\n",
    "    #Atem1 = tf.diag(tf.ones([n_input])) # Atem not palying any roles\n",
    "    #Ytem = tf.reshape(Y[i,:], [n_input, 1])\n",
    "    #Atem = tf.diag(tf.ones([n_input]))\n",
    "    #x = tf.reshape(x, [-1, sn, 1]) # 100, 207, 1\n",
    "    \n",
    "    # x (?, 207, 12)\n",
    "    x = tf.transpose(x, [1, 0, 2]) # 207, ?, 12\n",
    "    x = tf.reshape(x, [sn, -1]) # 207, batch*feature_num\n",
    "    Atem1 = 0.5*(weights['A1'] + tf.transpose(weights['A1']))#+ Atem \n",
    "    Atem1 = normalize_adj(Atem1)\n",
    "    #th = tf.constant(0.01, dtype=tf.float32)\n",
    "    #where = tf.subtract(Atem1, th)\n",
    "    #Atem1 = tf.nn.relu(where)\n",
    "\n",
    "    Z1 = tf.matmul(Atem1, x) # 207, batch*feature_num  #+ tf.matmul( tf.matmul(weights['A1'], weights['A1']), Xtem)\n",
    "    Z1 = tf.reshape(Z1, [-1, frequency]) # 207* 100, frequency\n",
    "    #layer_1 = tf.matmul(Z1, weights['h1']) \n",
    "    layer_1 = tf.add(tf.matmul(Z1, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1) # 207*100, hidden1\n",
    "\n",
    "    \n",
    "    #Atem2 = 0.5*(weights['A2'] + tf.transpose(weights['A2']))#+ Atem \n",
    "    #Atem2 = normalize_adj(Atem2)\n",
    "    \n",
    "    #layer_1 = tf.reshape(layer_1, [sn, -1])  # 207, batchsize*hidden1\n",
    "    #Z2 = tf.matmul(Atem2, layer_1)\n",
    "    #Z2 = tf.reshape(Z2, [-1, n_hidden_vec1]) # 207*batchsize, n_hidden_vec1\n",
    "    #layer_2 = tf.add(tf.matmul(Z2, weights['h2']), biases['b2'])\n",
    "    #layer_2 = tf.nn.relu(layer_2) # 207*batchsize, hidden2\n",
    "\n",
    "    #Atem3 = 0.5*(weights['A3'] + tf.transpose(weights['A3']))#+ Atem \n",
    "    #Atem3 = normalize_adj(Atem3) \n",
    "    \n",
    "    #layer_2 = tf.reshape(layer_2, [sn, -1])  # 207, batchsize*hidden2\n",
    "    #Z3 = tf.matmul(Atem3, layer_2)\n",
    "    #Z3 = tf.reshape(Z3, [-1, n_hidden_vec2]) # 207*batchsize, hidden2\n",
    "    #layer_3 = tf.add(tf.matmul(Z3, weights['h3']), biases['b3'])\n",
    "    #layer_3 = tf.nn.relu(layer_3) # 207*batchsize, hidden3\n",
    "    \n",
    "    #Atem4 = 0.5*(weights['A4'] + tf.transpose(weights['A4']))#+ Atem \n",
    "    #Atem4 = normalize_adj(Atem4)\n",
    "    #Z4 = tf.matmul(Atem4, layer_3)\n",
    "    #layer_4 = tf.add(tf.matmul(Z4, weights['h4']), biases['b4'])\n",
    "    #layer_4 = tf.nn.relu(layer_4)\n",
    "\n",
    "    # flattern\n",
    "    #layer_3 = tf.reshape(layer_3, [1, 272*n_hidden_vec3])\n",
    "\n",
    "    #F1 = tf.add(tf.matmul(layer_3, weights['f1']), biases['bf1'])\n",
    "    #F1 = tf.nn.relu(F1)\n",
    "\n",
    "    #F2 = tf.add(tf.matmul(F1, weights['f2']), biases['bf2'])\n",
    "    #F2 = tf.nn.relu(F2)\n",
    "\n",
    "    #F3 = tf.add(tf.matmul(F1, weights['f3']), biases['bf3'])\n",
    "    #out_layer = tf.reshape(F3, [272, 1])\n",
    "\n",
    "    # Output layer with linear activation\n",
    "    layer_1 = tf.reshape(layer_1, [sn, -1, n_hidden_vec1])\n",
    "    layer_1 = tf.transpose(layer_1, [1, 0, 2]) # batchsize, sn, hidden3\n",
    "    layer_1 = tf.reshape(layer_1, [-1, sn*n_hidden_vec1]) # batchsize, sn*hidden3\n",
    "    Z4 = layer_1\n",
    "    #out_layer = tf.add(tf.matmul(Z4, weights['outg']), biases['boutg'])\n",
    "    #tf.matmul(Atem1, layer_1)#tf.matmul(Atem, layer_3)\n",
    "    #out_layer = tf.add(tf.matmul(Z4, weights['out']), biases['bout'])\n",
    "    #out_layer = tf.nn.relu(out_layer)\n",
    "    # weather layer 1\n",
    "    #x_wea_tem = tf.reshape(x_wea[i,:], [1, 9*frequency2]) # 1 by 126\n",
    "    #layer_1_wea = tf.add(tf.matmul(x_wea_tem, weights['h1_wea']), biases['b1_wea'])\n",
    "    #layer_1_wea = tf.nn.relu(layer_1_wea)\n",
    "\n",
    "    #out_layer_wea = tf.add(tf.matmul(layer_1_wea, weights['out_wea']), biases['bout_wea'])\n",
    "    #out_layer = tf.add(out_layer, tf.reshape(out_layer_wea, [272, 1]))\n",
    "\n",
    "\n",
    "    \n",
    "    return Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_corr_final(frequency, horizon, learning_rate, decay,batch_size, n_hidden_vec1,n_hidden_vec2,n_hidden_vec3,keep, early_stop_th,training_epochs, reg1, reg2):\n",
    "    # set size\n",
    "    #sn = 3 # station number\n",
    "    X_whole = []\n",
    "    Y_whole = []\n",
    "\n",
    "    x_offsets = np.sort(\n",
    "        # np.concatenate(([-week_size + 1, -day_size + 1], np.arange(-11, 1, 1)))\n",
    "        np.concatenate((np.arange(-frequency+1, 1, 1),))\n",
    "    )\n",
    "    # Predict the next one hour\n",
    "    y_offsets = np.sort(np.arange(1, 1+ horizon, 1))\n",
    "\n",
    "    min_t = abs(min(x_offsets))\n",
    "    max_t = abs(raw_data.shape[0] - abs(max(y_offsets)))  # Exclusive\n",
    "    for t in range(min_t, max_t):\n",
    "        x_t = raw_data.iloc[t + x_offsets, 0:sn].values.flatten('F')\n",
    "        y_t = raw_data.iloc[t + y_offsets, 0:sn].values.flatten('F')\n",
    "        X_whole.append(x_t)\n",
    "        Y_whole.append(y_t)\n",
    "\n",
    "    X_whole = np.stack(X_whole, axis=0)\n",
    "    time_step = int(X_whole.shape[1] / sn)\n",
    "    X_whole = np.reshape(X_whole, [X_whole.shape[0], sn, time_step])\n",
    "    Y_whole = np.stack(Y_whole, axis=0)\n",
    "    \n",
    "    i = lstm_steps\n",
    "    X_whole_lstm = []\n",
    "    Y_whole_lstm = []\n",
    "    \n",
    "    while i < X_whole.shape[0]:\n",
    "        X_whole_lstm.append(X_whole[i-lstm_steps:i,:])\n",
    "        Y_whole_lstm.append(Y_whole[i])\n",
    "        i = i + 1\n",
    "    \n",
    "    X_whole_lstm = np.stack(X_whole_lstm, axis = 0) # (34239, 10, 207, 12)\n",
    "    Y_whole_lstm = np.stack(Y_whole_lstm, axis = 0) # (34239, 2484)\n",
    "    #print (Y_whole_lstm.shape)\n",
    "    '''\n",
    "    time_step = int(time_step) #\n",
    "\n",
    "    #i = time_step\n",
    "    #X_whole = np.zeros(shape = (raw_data.shape[0] - time_step, sn*time_step), dtype = np.float)\n",
    "    #Y_whole = np.zeros(shape = (raw_data.shape[0] - time_step, sn), dtype = np.float)\n",
    "\n",
    "    while i < raw_data.shape[0]:\n",
    "        X_whole[i - time_step, ] = raw_data.iloc[(i - time_step):i, 0:sn].values.flatten('F') # 'F' flatten by column, default:flatten by row 0, 1, 2...7\n",
    "        Y_whole[i - time_step, ] = raw_data.iloc[i, 0:sn]\n",
    "        i = i + 1\n",
    "    '''\n",
    "\n",
    "\n",
    "    n_input = sn # station number\n",
    "    n_input_vec = n_input * frequency # 207 * frequency\n",
    "    n_A_vec = n_input * n_input\n",
    "    n_output_vec = Y_whole_lstm.shape[1] # each row represent a result\n",
    "    #print (n_output_vec)\n",
    "\n",
    "    \n",
    "    num_samples = X_whole_lstm.shape[0]\n",
    "    num_train = 20000 # Note here actually we use the first 20000 to train the model. The paper mentioned \"22304\" need to be corrected.\n",
    "    num_val = 2000\n",
    "    num_test = 2000\n",
    "    #skip = skip1 + freq_max - time_step#time_step_max - time_step # to make sure the testing datasets are the same although the frequency could be different\n",
    "\n",
    "    X_training = X_whole_lstm[:num_train, :]\n",
    "    Y_training = Y_whole_lstm[:num_train, :]\n",
    "    \n",
    "    # shuffle\n",
    "    perm = np.arange(X_whole_lstm.shape[0])\n",
    "    np.random.shuffle(perm)\n",
    "    X_training = X_whole_lstm[perm]\n",
    "    Y_training = Y_whole_lstm[perm]\n",
    "    \n",
    "    #print (type(X_training))\n",
    "    #X_training = random.Random(6).shuffle(X_training)\n",
    "    #Y_training = random.Random(6).shuffle(Y_training)\n",
    "\n",
    "    X_val = X_whole_lstm[num_train:num_train+num_val, :]\n",
    "    Y_val = Y_whole_lstm[num_train:num_train+num_val, :]\n",
    "    #A_val = A_whole[0+training:0+training+validation, :]\n",
    "\n",
    "    X_test = X_whole_lstm[num_train+num_val:num_train+num_val+num_test, :]\n",
    "    Y_test = Y_whole_lstm[num_train+num_val:num_train+num_val+num_test, :]\n",
    "\n",
    "    scaler = StandardScaler(mean=X_training.mean(), std=X_training.std())\n",
    "\n",
    "    X_training = scaler.transform(X_training)\n",
    "    Y_training = scaler.transform(Y_training)\n",
    "\n",
    "    X_val = scaler.transform(X_val)\n",
    "    Y_val = scaler.transform(Y_val)\n",
    "\n",
    "    X_test = scaler.transform(X_test)\n",
    "    Y_test = scaler.transform(Y_test)\n",
    "    \n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    predic_res = []\n",
    "    Y_true = []\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    \n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "    n_hidden_vec1 = int(n_hidden_vec1)\n",
    "    n_hidden_vec2 = int(n_hidden_vec2)\n",
    "    n_hidden_vec3 = int(n_hidden_vec3)\n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    # Network Parameters\n",
    "\n",
    "    #n_classes = 2 # MNIST total classes (0-9 digits) # n_classes is for classification only\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None, lstm_steps, sn, time_step]) # X is the input signal\n",
    "    #X_weather = tf.placeholder(tf.float32, [None, 9 * frequency2]) # X_weather weather and holiday information (9 is the feature number)\n",
    "    A = tf.placeholder(tf.float32, [None, n_A_vec]) # A is the normalized adj matrix\n",
    "    oldA = tf.placeholder(tf.float32, [n_input, n_input])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_output_vec]) # y is the regression output\n",
    "    #num = tf.placeholder(tf.int32,[1, 1] )\n",
    "\n",
    "    #Xtem = tf.placeholder(tf.float32, [n_input, frequency]) # for each row of X, A, Y, it can be reshaped to Xtem, Atem, Ytem\n",
    "    #Atem = tf.placeholder(tf.float32, [n_input, n_input]) # \n",
    "    #Ytem = tf.placeholder(tf.float32, [n_input, 1]) #\n",
    "\n",
    "    #Ypre = tf.placeholder(tf.float32, [None, n_output_vec])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([frequency, n_hidden_vec1]), dtype=np.float32),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_vec1, n_hidden_vec2]), dtype=np.float32),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_vec2, n_hidden_vec3]), dtype=np.float32),\n",
    "        #'h4': tf.Variable(tf.random_normal([n_hidden_vec3, n_hidden_vec4])),\n",
    "        'outg': tf.Variable(tf.random_normal([sn*n_hidden_vec1, n_hidden_vec4]), dtype=np.float32), \n",
    "        'out': tf.Variable(tf.random_normal([num_hidden, Y_whole.shape[1]]), dtype=np.float32), # dont forget to change n_hidden_vec1 when add/delete layers\n",
    "        #'f1': tf.Variable(tf.random_normal([272*n_hidden_vec3, 100])),\n",
    "        #'f2': tf.Variable(tf.random_normal([50, 10])),\n",
    "        #'f3': tf.Variable(tf.random_normal([100, 272])),\n",
    "        'A1': tf.Variable(tf.random_normal([n_input,n_input]), dtype=np.float32),\n",
    "        'A2': tf.Variable(tf.random_normal([n_input,n_input]), dtype=np.float32),\n",
    "        'A3': tf.Variable(tf.random_normal([n_input,n_input]), dtype=np.float32),\n",
    "        #'A4': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'h1_wea': tf.Variable(tf.random_normal([9*frequency2, n_hidden_weather1])),\n",
    "        #'out_wea': tf.Variable(tf.random_normal([n_hidden_weather1, n_input]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([1, n_hidden_vec1]), dtype=np.float32),# n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        'b2': tf.Variable(tf.random_normal([1, n_hidden_vec2]), dtype=np.float32), #n_hidden_vec2])),\n",
    "        'b3': tf.Variable(tf.random_normal([1, n_hidden_vec3]), dtype=np.float32),#n_hidden_vec3])),\n",
    "        #'b4': tf.Variable(tf.random_normal([n_input, n_hidden_vec4])),\n",
    "        #'b1': tf.Variable(tf.random_normal([n_input,n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        #'b2': tf.Variable(tf.random_normal([n_input,n_hidden_vec2])),\n",
    "        #'b3': tf.Variable(tf.random_normal([n_input,n_hidden_vec3])),\n",
    "        #'bf1': tf.Variable(tf.random_normal([1, 100])), \n",
    "        #'bf2': tf.Variable(tf.random_normal([1, 10])), \n",
    "        #'bf3': tf.Variable(tf.random_normal([1, 272])), \n",
    "        'boutg': tf.Variable(tf.random_normal([1, n_hidden_vec4]), dtype=np.float32), \n",
    "        'bout': tf.Variable(tf.random_normal([Y_whole.shape[1]]), dtype=np.float32), \n",
    "        #'b1_wea': tf.Variable(tf.random_normal([1, n_hidden_weather1])), \n",
    "        #'bout_wea': tf.Variable(tf.random_normal([1, n_input])), \n",
    "    }\n",
    "    \n",
    "    with tf.variable_scope('lstm'):\n",
    "        lstm = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(num_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "        rnn_input_seq = tf.unstack(X, lstm_steps, 1) # lstm_steps is the 2nd variable\n",
    "\n",
    "        for i in range(lstm_steps):\n",
    "            rnn_input_seq[i] = gcn(rnn_input_seq[i], weights, biases, batch_size,n_input, frequency, 1, n_output_vec)\n",
    "            #print (rnn_input_seq[i].shape)\n",
    "        outputs, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs[-1], [-1, num_hidden])\n",
    "        #print ('123here!!!!!!!!!!!')\n",
    "        pred = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        #print (pred)\n",
    "        #pred = tf.reshape(pred, [-1, Y_whole.shape[1]])\n",
    "        #print ('here!!!!!!!!!!!')\n",
    "        pred = scaler.inverse_transform(pred)\n",
    "        Y_true_tr = scaler.inverse_transform(Y)\n",
    "\n",
    "        cost = tf.reduce_mean(tf.pow(pred - Y_true_tr, 2)) \n",
    "        #print (cost)\n",
    "        \n",
    "    with tf.variable_scope('lstm', reuse=True):\n",
    "        rnn_input_seq_val = tf.unstack(X, lstm_steps, 1)\n",
    "\n",
    "        for i in range(lstm_steps):\n",
    "            rnn_input_seq_val[i] = gcn(rnn_input_seq_val[i], weights, biases, batch_size,n_input,frequency, 2, n_output_vec)\n",
    "        outputs_val, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq_val, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs_val[-1], [-1, num_hidden])\n",
    "\n",
    "        pred_val = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        pred_val = scaler.inverse_transform(pred_val)\n",
    "        Y_true_val = scaler.inverse_transform(Y)\n",
    "        \n",
    "        cost_val =  tf.reduce_mean(tf.pow(pred_val - Y_true_val, 2)) \n",
    "        #print ('234here!!!!!!!!!!!')\n",
    "    with tf.variable_scope('lstm', reuse=True):\n",
    "        rnn_input_seq_test = tf.unstack(X, lstm_steps, 1)\n",
    "        \n",
    "        for i in range(lstm_steps):\n",
    "            rnn_input_seq_test[i] = gcn(rnn_input_seq_test[i], weights, biases, batch_size,n_input,frequency, 3, n_output_vec)\n",
    "        outputs_test, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq_test, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs_test[-1], [-1, num_hidden])\n",
    "\n",
    "        pred_tes = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        pred_tes = scaler.inverse_transform(pred_tes)\n",
    "        Y_true_tes = scaler.inverse_transform(Y)\n",
    "        \n",
    "        cost_tes = tf.reduce_mean(tf.pow(pred_tes - Y_true_tes, 2)) \n",
    "        \n",
    "        #print ('345here!!!!!!!!!!!')\n",
    "    #rmse\n",
    "    #cost_tes = tf.reduce_mean(tf.pow(pred_tes-Y, 2))\n",
    "    # cross-entropy for classification\n",
    "    # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y_train))\n",
    "    # ratio = tf.abs(tf.reduce_sum(pred)-tf.reduce_sum(Y))/tf.reduce_sum(Y)\n",
    "    #zero = 0\n",
    "    #ratio = tf.reduce_mean(tf.divide(tf.where(tf.not_equal(Y, zero), np.abs(pred-Y), tf.zeros(Y.get_shape(), tf.float32)), tf.where(tf.not_equal(Y, zero), Y, tf.ones(Y.get_shape(), tf.float32))))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay).minimize(cost)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #total_val_cost = []\n",
    "    #total_val_ratio = []\n",
    "\n",
    "    # learning start from \n",
    "\n",
    "    #index = daily_bike[(daily_bike['year'] == 2016) & (daily_bike['monthofyear'] == 1) & (daily_bike['dayofmonth'] == 1)].index.tolist()[0]\n",
    "    #A_hat = normalize_adj(corr_matrix_trips)\n",
    "    #print(A_hat)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(num_train/batch_size) #int(num_train/batch_size)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                #print (Y_training[i*batch_size:(i+1)*batch_size,].size())\n",
    "                #num = batch_size\n",
    "                _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                      Y: Y_training[i*batch_size:(i+1)*batch_size,],  \n",
    "                                                              keep_prob: keep})\n",
    "                #print (preds)\n",
    "                #print (trueval)\n",
    "                #print (\"Epoch:\", '%04d' % (epoch+1), \"batch: \", i, \"batch cost=\", \\\n",
    "                #    \"{:.9f}\".format(c))\n",
    "                #print ('here!!!!!!!!!!!!!!!!')\n",
    "                avg_cost += c * batch_size #/ total_batch \n",
    "                #Display logs per epoch step\n",
    "                \n",
    "            # rest part of training dataset\n",
    "            #num = num_train - total_batch*batch_size \n",
    "            if total_batch * batch_size != num_train:\n",
    "                _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[total_batch*batch_size:num_train,], \n",
    "                                          Y: Y_training[total_batch*batch_size:num_train,],\n",
    "                                                  keep_prob: keep})\n",
    "                avg_cost += c * (num_train - total_batch*batch_size)\n",
    "            \n",
    "            avg_cost = np.sqrt(avg_cost / num_train)\n",
    "            \n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Epoch:\", '%04d' % (epoch+1), \"Training RMSE=\", \\\n",
    "                    \"{:.9f}\".format(avg_cost)) #np.sqrt(avg_cost)\n",
    "                \n",
    "            # also use batch to save memory\n",
    "            # validation\n",
    "            c_val = 0.\n",
    "            total_bat_val = int(num_val/batch_size)\n",
    "            for i in range(total_bat_val):\n",
    "                #num = batch_size\n",
    "                c_val_b = sess.run([cost_val], feed_dict={X: X_val[i*batch_size:(i+1)*batch_size,], \n",
    "                                                          Y: Y_val[i*batch_size:(i+1)*batch_size,],   keep_prob:1})\n",
    "                c_val += c_val_b[0]*batch_size\n",
    "            \n",
    "            if total_bat_val * batch_size != num_val:\n",
    "                #num = num_val - total_bat_val*batch_size\n",
    "                c_val_b = sess.run([cost_val], feed_dict={X: X_val[total_bat_val*batch_size:num_val,], \n",
    "                                                          Y: Y_val[total_bat_val*batch_size:num_val,],  keep_prob:1})\n",
    "                c_val += c_val_b[0] * (num_val - total_bat_val*batch_size)\n",
    "                \n",
    "            c_val = np.sqrt(c_val / num_val)\n",
    "            \n",
    "            print(\"Validation RMSE: \", c_val)\n",
    "            \n",
    "            # test\n",
    "            c_tes = 0.\n",
    "            total_bat_test = int(num_test/batch_size)\n",
    "            \n",
    "            pre_test_tem = [] # save the prediction results\n",
    "            Y_tes_true = []\n",
    "            \n",
    "            for i in range(total_bat_test):\n",
    "                #num = batch_size\n",
    "                c_tes_b, pred_tes1, Y_tes_batch = sess.run([cost_tes, pred_tes, Y_true_tes], feed_dict={X: X_test[i*batch_size:(i+1)*batch_size,],\n",
    "                                                                               Y: Y_test[i*batch_size:(i+1)*batch_size,],  keep_prob: 1})\n",
    "                c_tes += c_tes_b*batch_size\n",
    "\n",
    "                #print (cost_h)\n",
    "                #print (pred_tes1.shape)\n",
    "                pre_test_tem.append(pred_tes1)\n",
    "                Y_tes_true.append(Y_tes_batch)\n",
    "                \n",
    "            if total_bat_test * batch_size != num_test:\n",
    "                #num = num_test - total_bat_test*batch_size\n",
    "                c_tes_b, pred_tes1, Y_tes_batch = sess.run([cost_tes, pred_tes, Y_true_tes], feed_dict={X: X_test[total_bat_test*batch_size:num_test,],\n",
    "                                                                               Y: Y_test[total_bat_test*batch_size:num_test,],  keep_prob: 1})\n",
    "                c_tes += c_tes_b * (num_test - total_bat_test*batch_size) \n",
    "                 \n",
    "                \n",
    "                #print (pred_tes1.shape)\n",
    "                pre_test_tem.append(pred_tes1)\n",
    "                Y_tes_true.append(Y_tes_batch)\n",
    "            \n",
    "            #print (c_tes_h.shape)\n",
    "            pre_test_tem = np.concatenate(pre_test_tem, axis = 0)\n",
    "            #print (pre_test_tem.shape)\n",
    "            Y_tes_true = np.concatenate(Y_tes_true, axis = 0)\n",
    "            \n",
    "            c_tes = np.sqrt(c_tes / num_test)\n",
    "            #c_tes_h = c_tes_h / num_test\n",
    "            \n",
    "            print(\"Test RMSE: \", c_tes)\n",
    "            #print(\"predic step: \", cost_by_hor)\n",
    "\n",
    "            if c_val < best_val:\n",
    "                best_val = c_val\n",
    "                #saver.save(sess, './bikesharing_graph_2_th_point1')\n",
    "                test_error = c_tes\n",
    "                \n",
    "                traing_error = avg_cost#np.sqrt(avg_cost)\n",
    "                early_stop_k = 0 # reset to 0\n",
    "                #print (pred_tes1)\n",
    "                predic_res = pre_test_tem\n",
    "                Y_true = Y_tes_true\n",
    "                #predic_step = cost_by_hor\n",
    "\n",
    "            # early stopping\n",
    "            if c_val >= best_val:\n",
    "                early_stop_k += 1\n",
    "\n",
    "            # threshold\n",
    "            if early_stop_k == early_stop_th:\n",
    "              #  print (\"early stopping...\")\n",
    "                break\n",
    "            \n",
    "\n",
    "        print(\"epoch is \", epoch)\n",
    "        print(\"training RMSE is \", traing_error)\n",
    "        print(\"Optimization Finished! the lowest validation RMSE is \", best_val)#(np.sqrt(best_val)))\n",
    "        print(\"The test RMSE is \", test_error)#(np.sqrt(test_error)))\n",
    "    \n",
    "    #test_Y = Y_test\n",
    "    #test_error = np.sqrt(test_error)\n",
    "    return test_error, predic_res, Y_true#, A1#, predic_step#, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training RMSE= 8.299416202\n",
      "Validation RMSE:  7.580002447948211\n",
      "Test RMSE:  6.705666653733698\n",
      "Epoch: 0002 Training RMSE= 7.436435918\n",
      "Validation RMSE:  7.386007596484858\n",
      "Test RMSE:  6.475329752298409\n",
      "Epoch: 0003 Training RMSE= 7.236388247\n",
      "Validation RMSE:  7.32281848593852\n",
      "Test RMSE:  6.327300142758965\n",
      "Epoch: 0004 Training RMSE= 7.065989458\n",
      "Validation RMSE:  6.975596532983168\n",
      "Test RMSE:  6.020240816134595\n",
      "Epoch: 0005 Training RMSE= 6.850644120\n",
      "Validation RMSE:  6.7757945464039055\n",
      "Test RMSE:  5.8079438759900555\n",
      "Epoch: 0006 Training RMSE= 6.658961171\n",
      "Validation RMSE:  6.593882844138136\n",
      "Test RMSE:  5.605094870616014\n",
      "Epoch: 0007 Training RMSE= 6.443181370\n",
      "Validation RMSE:  6.416954141427694\n",
      "Test RMSE:  5.422436072261587\n",
      "Epoch: 0008 Training RMSE= 6.304944727\n",
      "Validation RMSE:  6.262239069271115\n",
      "Test RMSE:  5.256020000927754\n",
      "Epoch: 0009 Training RMSE= 6.153715404\n",
      "Validation RMSE:  6.107630409850258\n",
      "Test RMSE:  5.0965355587585845\n",
      "Epoch: 0010 Training RMSE= 5.971322389\n",
      "Validation RMSE:  6.022000667129829\n",
      "Test RMSE:  5.033630329279217\n",
      "Epoch: 0011 Training RMSE= 5.817744045\n",
      "Validation RMSE:  5.923959732480323\n",
      "Test RMSE:  4.964489291644143\n",
      "Epoch: 0012 Training RMSE= 5.638553101\n",
      "Validation RMSE:  5.627352498111389\n",
      "Test RMSE:  4.6464518433274264\n",
      "Epoch: 0013 Training RMSE= 5.476887866\n",
      "Validation RMSE:  5.432868431929042\n",
      "Test RMSE:  4.480158348605857\n",
      "Epoch: 0014 Training RMSE= 5.315855932\n",
      "Validation RMSE:  5.293438812441263\n",
      "Test RMSE:  4.366790835359913\n",
      "Epoch: 0015 Training RMSE= 5.128050469\n",
      "Validation RMSE:  5.0735612756065125\n",
      "Test RMSE:  4.181571036930833\n",
      "Epoch: 0016 Training RMSE= 4.921896062\n",
      "Validation RMSE:  4.882191195823229\n",
      "Test RMSE:  4.046769512672205\n",
      "Epoch: 0017 Training RMSE= 4.733609748\n",
      "Validation RMSE:  4.730713438703473\n",
      "Test RMSE:  3.8798481853023916\n",
      "Epoch: 0018 Training RMSE= 4.584525109\n",
      "Validation RMSE:  4.586281862603103\n",
      "Test RMSE:  3.8136004751395203\n",
      "Epoch: 0019 Training RMSE= 4.450262452\n",
      "Validation RMSE:  4.492948994985288\n",
      "Test RMSE:  3.680522848179878\n",
      "Epoch: 0020 Training RMSE= 4.356380170\n",
      "Validation RMSE:  4.334203287366112\n",
      "Test RMSE:  3.5503619358759977\n",
      "Epoch: 0021 Training RMSE= 4.221796995\n",
      "Validation RMSE:  4.21730239898206\n",
      "Test RMSE:  3.438979992013261\n",
      "Epoch: 0022 Training RMSE= 4.134262647\n",
      "Validation RMSE:  4.160992433569972\n",
      "Test RMSE:  3.4008168978217226\n",
      "Epoch: 0023 Training RMSE= 4.064932852\n",
      "Validation RMSE:  4.112798736760246\n",
      "Test RMSE:  3.3302101802651767\n",
      "Epoch: 0024 Training RMSE= 3.996080908\n",
      "Validation RMSE:  4.04506955321579\n",
      "Test RMSE:  3.319574072012406\n",
      "Epoch: 0025 Training RMSE= 3.958228961\n",
      "Validation RMSE:  3.9973770721028417\n",
      "Test RMSE:  3.202745759424754\n",
      "Epoch: 0026 Training RMSE= 3.923421395\n",
      "Validation RMSE:  4.010753125365533\n",
      "Test RMSE:  3.2168359342981945\n",
      "Epoch: 0027 Training RMSE= 3.877628007\n",
      "Validation RMSE:  3.999228552098156\n",
      "Test RMSE:  3.1756621428766745\n",
      "Epoch: 0028 Training RMSE= 3.864597435\n",
      "Validation RMSE:  3.909758708611205\n",
      "Test RMSE:  3.113457171017974\n",
      "Epoch: 0029 Training RMSE= 3.830384795\n",
      "Validation RMSE:  3.8873895574256836\n",
      "Test RMSE:  3.148757615392546\n",
      "Epoch: 0030 Training RMSE= 3.811488733\n",
      "Validation RMSE:  3.9175227663489935\n",
      "Test RMSE:  3.102559042467929\n",
      "Epoch: 0031 Training RMSE= 3.801066740\n",
      "Validation RMSE:  3.825508749544376\n",
      "Test RMSE:  3.02885775208727\n",
      "Epoch: 0032 Training RMSE= 3.772968571\n",
      "Validation RMSE:  3.823075463568216\n",
      "Test RMSE:  3.043893617222243\n",
      "Epoch: 0033 Training RMSE= 3.772400863\n",
      "Validation RMSE:  3.7978843948463505\n",
      "Test RMSE:  3.0091519630627164\n",
      "Epoch: 0034 Training RMSE= 3.753583144\n",
      "Validation RMSE:  3.766163261217448\n",
      "Test RMSE:  2.9745658726056607\n",
      "Epoch: 0035 Training RMSE= 3.732348591\n",
      "Validation RMSE:  3.839388163520892\n",
      "Test RMSE:  3.0313794609818157\n",
      "Epoch: 0036 Training RMSE= 3.716442678\n",
      "Validation RMSE:  3.794328094378492\n",
      "Test RMSE:  2.977559560953547\n",
      "Epoch: 0037 Training RMSE= 3.702958531\n",
      "Validation RMSE:  3.7116309873193285\n",
      "Test RMSE:  2.9339785672687597\n",
      "Epoch: 0038 Training RMSE= 3.676890188\n",
      "Validation RMSE:  3.6854830737876125\n",
      "Test RMSE:  2.9240374187614004\n",
      "Epoch: 0039 Training RMSE= 3.662725417\n",
      "Validation RMSE:  3.678350007103373\n",
      "Test RMSE:  2.923251618225908\n",
      "Epoch: 0040 Training RMSE= 3.636293013\n",
      "Validation RMSE:  3.6235653570530952\n",
      "Test RMSE:  2.861513869991012\n",
      "Epoch: 0041 Training RMSE= 3.617313714\n",
      "Validation RMSE:  3.7172213706373145\n",
      "Test RMSE:  2.9028257275281115\n",
      "Epoch: 0042 Training RMSE= 3.606555419\n",
      "Validation RMSE:  3.6055506638038364\n",
      "Test RMSE:  2.833694832310668\n",
      "Epoch: 0043 Training RMSE= 3.584244874\n",
      "Validation RMSE:  3.5757091145722715\n",
      "Test RMSE:  2.8206046843351835\n",
      "Epoch: 0044 Training RMSE= 3.562470714\n",
      "Validation RMSE:  3.5714880846378336\n",
      "Test RMSE:  2.8156399471381572\n",
      "Epoch: 0045 Training RMSE= 3.551300561\n",
      "Validation RMSE:  3.5293553088385154\n",
      "Test RMSE:  2.7773624424902237\n",
      "Epoch: 0046 Training RMSE= 3.533953538\n",
      "Validation RMSE:  3.5369746131438875\n",
      "Test RMSE:  2.7800958360303745\n",
      "Epoch: 0047 Training RMSE= 3.520130272\n",
      "Validation RMSE:  3.5493715549350844\n",
      "Test RMSE:  2.8034256032900298\n",
      "Epoch: 0048 Training RMSE= 3.507735142\n",
      "Validation RMSE:  3.486425950844335\n",
      "Test RMSE:  2.760687326561546\n",
      "Epoch: 0049 Training RMSE= 3.495325267\n",
      "Validation RMSE:  3.52861993458758\n",
      "Test RMSE:  2.764800816375338\n",
      "Epoch: 0050 Training RMSE= 3.482971829\n",
      "Validation RMSE:  3.4947135545137473\n",
      "Test RMSE:  2.766096602925778\n",
      "Epoch: 0051 Training RMSE= 3.473944608\n",
      "Validation RMSE:  3.474323726275026\n",
      "Test RMSE:  2.7391447047308763\n",
      "Epoch: 0052 Training RMSE= 3.460210337\n",
      "Validation RMSE:  3.455327570025803\n",
      "Test RMSE:  2.7171896918596725\n",
      "Epoch: 0053 Training RMSE= 3.442545964\n",
      "Validation RMSE:  3.464795771355868\n",
      "Test RMSE:  2.7172829846478854\n",
      "Epoch: 0054 Training RMSE= 3.448349951\n",
      "Validation RMSE:  3.4241555683522127\n",
      "Test RMSE:  2.6853483147476314\n",
      "Epoch: 0055 Training RMSE= 3.426122776\n",
      "Validation RMSE:  3.4417655315623685\n",
      "Test RMSE:  2.6993787969238823\n",
      "Epoch: 0056 Training RMSE= 3.426320878\n",
      "Validation RMSE:  3.4126690043704513\n",
      "Test RMSE:  2.6762038739437433\n",
      "Epoch: 0057 Training RMSE= 3.412677287\n",
      "Validation RMSE:  3.4232307953197982\n",
      "Test RMSE:  2.6818053659623664\n",
      "Epoch: 0058 Training RMSE= 3.397813081\n",
      "Validation RMSE:  3.4096697168565617\n",
      "Test RMSE:  2.6595761062840224\n",
      "Epoch: 0059 Training RMSE= 3.393615698\n",
      "Validation RMSE:  3.395290891154623\n",
      "Test RMSE:  2.646605969552972\n",
      "Epoch: 0060 Training RMSE= 3.380485439\n",
      "Validation RMSE:  3.3889325044427454\n",
      "Test RMSE:  2.6514476672580685\n",
      "Epoch: 0061 Training RMSE= 3.376440447\n",
      "Validation RMSE:  3.3718505575657045\n",
      "Test RMSE:  2.6314042397278428\n",
      "Epoch: 0062 Training RMSE= 3.373606896\n",
      "Validation RMSE:  3.3965605540230075\n",
      "Test RMSE:  2.67748025012457\n",
      "Epoch: 0063 Training RMSE= 3.361477291\n",
      "Validation RMSE:  3.351271200970267\n",
      "Test RMSE:  2.6256030162098893\n",
      "Epoch: 0064 Training RMSE= 3.353950379\n",
      "Validation RMSE:  3.3591531547218483\n",
      "Test RMSE:  2.630821856462709\n",
      "Epoch: 0065 Training RMSE= 3.343879922\n",
      "Validation RMSE:  3.350507005358296\n",
      "Test RMSE:  2.613089150979156\n",
      "Epoch: 0066 Training RMSE= 3.341173138\n",
      "Validation RMSE:  3.3628915168831033\n",
      "Test RMSE:  2.622989724929178\n",
      "Epoch: 0067 Training RMSE= 3.338304596\n",
      "Validation RMSE:  3.3214367747022404\n",
      "Test RMSE:  2.6072038080701496\n",
      "Epoch: 0068 Training RMSE= 3.322740223\n",
      "Validation RMSE:  3.350097478330869\n",
      "Test RMSE:  2.6073375902590636\n",
      "Epoch: 0069 Training RMSE= 3.323559322\n",
      "Validation RMSE:  3.333165019474805\n",
      "Test RMSE:  2.597446978177487\n",
      "Epoch: 0070 Training RMSE= 3.311061396\n",
      "Validation RMSE:  3.3485553117284708\n",
      "Test RMSE:  2.594765521795398\n",
      "Epoch: 0071 Training RMSE= 3.312455962\n",
      "Validation RMSE:  3.3451469747473634\n",
      "Test RMSE:  2.619342542996539\n",
      "Epoch: 0072 Training RMSE= 3.300613421\n",
      "Validation RMSE:  3.3864290846131913\n",
      "Test RMSE:  2.643854818227681\n",
      "Epoch: 0073 Training RMSE= 3.301449438\n",
      "Validation RMSE:  3.3439154182944026\n",
      "Test RMSE:  2.5973496564471974\n",
      "Epoch: 0074 Training RMSE= 3.287752176\n",
      "Validation RMSE:  3.296797765035041\n",
      "Test RMSE:  2.5582736681422347\n",
      "Epoch: 0075 Training RMSE= 3.280253933\n",
      "Validation RMSE:  3.328812828087011\n",
      "Test RMSE:  2.58604905732039\n",
      "Epoch: 0076 Training RMSE= 3.277308780\n",
      "Validation RMSE:  3.304903785355414\n",
      "Test RMSE:  2.5798771077466336\n",
      "Epoch: 0077 Training RMSE= 3.272812041\n",
      "Validation RMSE:  3.3179004839114037\n",
      "Test RMSE:  2.5852774155028806\n",
      "Epoch: 0078 Training RMSE= 3.265317930\n",
      "Validation RMSE:  3.2853127894713934\n",
      "Test RMSE:  2.5559532926020014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0079 Training RMSE= 3.267081535\n",
      "Validation RMSE:  3.280807483443037\n",
      "Test RMSE:  2.555761548968199\n",
      "Epoch: 0080 Training RMSE= 3.252055479\n",
      "Validation RMSE:  3.276403189857848\n",
      "Test RMSE:  2.5579767546115777\n",
      "Epoch: 0081 Training RMSE= 3.244679663\n",
      "Validation RMSE:  3.297369029558239\n",
      "Test RMSE:  2.557757082124998\n",
      "Epoch: 0082 Training RMSE= 3.240966133\n",
      "Validation RMSE:  3.2730828505069893\n",
      "Test RMSE:  2.5606135193090442\n",
      "Epoch: 0083 Training RMSE= 3.233300927\n",
      "Validation RMSE:  3.2765199082734777\n",
      "Test RMSE:  2.5488293176407053\n",
      "Epoch: 0084 Training RMSE= 3.224243435\n",
      "Validation RMSE:  3.312825420877293\n",
      "Test RMSE:  2.59509885713591\n",
      "Epoch: 0085 Training RMSE= 3.226860531\n",
      "Validation RMSE:  3.2542164667269593\n",
      "Test RMSE:  2.531860089659545\n",
      "Epoch: 0086 Training RMSE= 3.214934627\n",
      "Validation RMSE:  3.260148653871237\n",
      "Test RMSE:  2.5455775086084236\n",
      "Epoch: 0087 Training RMSE= 3.209155481\n",
      "Validation RMSE:  3.24586772476082\n",
      "Test RMSE:  2.5264548104761446\n",
      "Epoch: 0088 Training RMSE= 3.206157068\n",
      "Validation RMSE:  3.241066943559242\n",
      "Test RMSE:  2.5220848449983158\n",
      "Epoch: 0089 Training RMSE= 3.205286259\n",
      "Validation RMSE:  3.2709760857468573\n",
      "Test RMSE:  2.5496256899527197\n",
      "Epoch: 0090 Training RMSE= 3.192444702\n",
      "Validation RMSE:  3.2562288419069065\n",
      "Test RMSE:  2.542005177661416\n",
      "Epoch: 0091 Training RMSE= 3.192219520\n",
      "Validation RMSE:  3.2595929922352256\n",
      "Test RMSE:  2.5317698286366204\n",
      "Epoch: 0092 Training RMSE= 3.176061144\n",
      "Validation RMSE:  3.2437795016613813\n",
      "Test RMSE:  2.514704541811932\n",
      "Epoch: 0093 Training RMSE= 3.174685345\n",
      "Validation RMSE:  3.223199213699951\n",
      "Test RMSE:  2.494589171557022\n",
      "Epoch: 0094 Training RMSE= 3.175105152\n",
      "Validation RMSE:  3.244034996816428\n",
      "Test RMSE:  2.5158891243027606\n",
      "Epoch: 0095 Training RMSE= 3.164190762\n",
      "Validation RMSE:  3.228577453427414\n",
      "Test RMSE:  2.5120503398048664\n",
      "Epoch: 0096 Training RMSE= 3.151589452\n",
      "Validation RMSE:  3.2349856440595564\n",
      "Test RMSE:  2.5180219757675473\n",
      "Epoch: 0097 Training RMSE= 3.154141101\n",
      "Validation RMSE:  3.217102203053592\n",
      "Test RMSE:  2.5148451170809474\n",
      "Epoch: 0098 Training RMSE= 3.146855554\n",
      "Validation RMSE:  3.265779044428574\n",
      "Test RMSE:  2.5528061965377042\n",
      "Epoch: 0099 Training RMSE= 3.139156769\n",
      "Validation RMSE:  3.3046390358547826\n",
      "Test RMSE:  2.6283406390288357\n",
      "Epoch: 0100 Training RMSE= 3.132042849\n",
      "Validation RMSE:  3.230813650334142\n",
      "Test RMSE:  2.518334629463285\n",
      "Epoch: 0101 Training RMSE= 3.127407535\n",
      "Validation RMSE:  3.1781163750965638\n",
      "Test RMSE:  2.4745516149317934\n",
      "Epoch: 0102 Training RMSE= 3.123692361\n",
      "Validation RMSE:  3.1647519149394787\n",
      "Test RMSE:  2.4718426274278853\n",
      "Epoch: 0103 Training RMSE= 3.118428351\n",
      "Validation RMSE:  3.1971021457792945\n",
      "Test RMSE:  2.473266140658311\n",
      "Epoch: 0104 Training RMSE= 3.109080948\n",
      "Validation RMSE:  3.1666803338776504\n",
      "Test RMSE:  2.44560525135048\n",
      "Epoch: 0105 Training RMSE= 3.100070385\n",
      "Validation RMSE:  3.163823738037643\n",
      "Test RMSE:  2.4735295349078905\n",
      "Epoch: 0106 Training RMSE= 3.100652970\n",
      "Validation RMSE:  3.1905861294552205\n",
      "Test RMSE:  2.4650123647337145\n",
      "Epoch: 0107 Training RMSE= 3.095270800\n",
      "Validation RMSE:  3.1967694381632428\n",
      "Test RMSE:  2.492698664845071\n",
      "Epoch: 0108 Training RMSE= 3.086892135\n",
      "Validation RMSE:  3.151107280878254\n",
      "Test RMSE:  2.4888813007921207\n",
      "Epoch: 0109 Training RMSE= 3.082292000\n",
      "Validation RMSE:  3.1697643191515628\n",
      "Test RMSE:  2.4597632376780565\n",
      "Epoch: 0110 Training RMSE= 3.080323850\n",
      "Validation RMSE:  3.1181349497605964\n",
      "Test RMSE:  2.435100327878589\n",
      "Epoch: 0111 Training RMSE= 3.075146348\n",
      "Validation RMSE:  3.1387591157003767\n",
      "Test RMSE:  2.439417378367559\n",
      "Epoch: 0112 Training RMSE= 3.060834362\n",
      "Validation RMSE:  3.1949559504796095\n",
      "Test RMSE:  2.4979820929054184\n",
      "Epoch: 0113 Training RMSE= 3.061952990\n",
      "Validation RMSE:  3.129791915454426\n",
      "Test RMSE:  2.4491582008190407\n",
      "Epoch: 0114 Training RMSE= 3.053369137\n",
      "Validation RMSE:  3.110467785830182\n",
      "Test RMSE:  2.4397269615244963\n",
      "Epoch: 0115 Training RMSE= 3.047279249\n",
      "Validation RMSE:  3.0898998076975728\n",
      "Test RMSE:  2.4150949881217927\n",
      "Epoch: 0116 Training RMSE= 3.041912749\n",
      "Validation RMSE:  3.1036412842016183\n",
      "Test RMSE:  2.4119188735423385\n",
      "Epoch: 0117 Training RMSE= 3.036081023\n",
      "Validation RMSE:  3.0997484989588737\n",
      "Test RMSE:  2.405935217344324\n",
      "Epoch: 0118 Training RMSE= 3.035833233\n",
      "Validation RMSE:  3.0964815623151627\n",
      "Test RMSE:  2.408353653880346\n",
      "Epoch: 0119 Training RMSE= 3.028101527\n",
      "Validation RMSE:  3.0915845748682393\n",
      "Test RMSE:  2.400080512603688\n",
      "Epoch: 0120 Training RMSE= 3.020606268\n",
      "Validation RMSE:  3.0801529182830567\n",
      "Test RMSE:  2.3851828620193234\n",
      "Epoch: 0121 Training RMSE= 3.017002491\n",
      "Validation RMSE:  3.104808770831664\n",
      "Test RMSE:  2.4236926487417665\n",
      "Epoch: 0122 Training RMSE= 3.007945906\n",
      "Validation RMSE:  3.097691808739396\n",
      "Test RMSE:  2.4138321504288633\n",
      "Epoch: 0123 Training RMSE= 3.015473108\n",
      "Validation RMSE:  3.0605805473690606\n",
      "Test RMSE:  2.3816397610775892\n",
      "Epoch: 0124 Training RMSE= 3.001568360\n",
      "Validation RMSE:  3.08786381342502\n",
      "Test RMSE:  2.393347866254194\n",
      "Epoch: 0125 Training RMSE= 2.996938093\n",
      "Validation RMSE:  3.0569362704591274\n",
      "Test RMSE:  2.378434533766542\n",
      "Epoch: 0126 Training RMSE= 3.000177593\n",
      "Validation RMSE:  3.112437671205821\n",
      "Test RMSE:  2.4625687052248058\n",
      "Epoch: 0127 Training RMSE= 2.991848604\n",
      "Validation RMSE:  3.0390081915624187\n",
      "Test RMSE:  2.356887845389019\n",
      "Epoch: 0128 Training RMSE= 2.983195626\n",
      "Validation RMSE:  3.0316501235370743\n",
      "Test RMSE:  2.350914114851372\n",
      "Epoch: 0129 Training RMSE= 2.978269645\n",
      "Validation RMSE:  3.0375673396491374\n",
      "Test RMSE:  2.3565968458047553\n",
      "Epoch: 0130 Training RMSE= 2.973549120\n",
      "Validation RMSE:  3.0890318024363745\n",
      "Test RMSE:  2.387237417550844\n",
      "Epoch: 0131 Training RMSE= 2.973438442\n",
      "Validation RMSE:  3.020209252358505\n",
      "Test RMSE:  2.342576330685815\n",
      "Epoch: 0132 Training RMSE= 2.962605854\n",
      "Validation RMSE:  3.0324156771302535\n",
      "Test RMSE:  2.364448196201806\n",
      "Epoch: 0133 Training RMSE= 2.962038972\n",
      "Validation RMSE:  3.0210538827471867\n",
      "Test RMSE:  2.347940132635374\n",
      "Epoch: 0134 Training RMSE= 2.963919289\n",
      "Validation RMSE:  3.029612698745606\n",
      "Test RMSE:  2.3723585849577766\n",
      "Epoch: 0135 Training RMSE= 2.957044324\n",
      "Validation RMSE:  3.0166252137773153\n",
      "Test RMSE:  2.3443397288668\n",
      "Epoch: 0136 Training RMSE= 2.948299977\n",
      "Validation RMSE:  3.0292484321385547\n",
      "Test RMSE:  2.3408311597742255\n",
      "Epoch: 0137 Training RMSE= 2.961408043\n",
      "Validation RMSE:  3.0442893182972672\n",
      "Test RMSE:  2.3805153451884475\n",
      "Epoch: 0138 Training RMSE= 2.942472206\n",
      "Validation RMSE:  2.996057482802059\n",
      "Test RMSE:  2.3249975614637486\n",
      "Epoch: 0139 Training RMSE= 2.939795845\n",
      "Validation RMSE:  2.9838792678772443\n",
      "Test RMSE:  2.3219224886446845\n",
      "Epoch: 0140 Training RMSE= 2.930493391\n",
      "Validation RMSE:  3.0063197010866296\n",
      "Test RMSE:  2.3261268028421616\n",
      "Epoch: 0141 Training RMSE= 2.922108668\n",
      "Validation RMSE:  3.0009802965065853\n",
      "Test RMSE:  2.3311419556562742\n",
      "Epoch: 0142 Training RMSE= 2.912829053\n",
      "Validation RMSE:  3.044429678099928\n",
      "Test RMSE:  2.3809828440670158\n",
      "Epoch: 0143 Training RMSE= 2.923550965\n",
      "Validation RMSE:  3.0194548761300095\n",
      "Test RMSE:  2.388019163796033\n",
      "Epoch: 0144 Training RMSE= 2.909922034\n",
      "Validation RMSE:  2.9818456426617335\n",
      "Test RMSE:  2.3049803466951997\n",
      "Epoch: 0145 Training RMSE= 2.914323330\n",
      "Validation RMSE:  3.001570429635086\n",
      "Test RMSE:  2.3209368446958334\n",
      "Epoch: 0146 Training RMSE= 2.900684440\n",
      "Validation RMSE:  2.9473166287548556\n",
      "Test RMSE:  2.287554664167118\n",
      "Epoch: 0147 Training RMSE= 2.892736503\n",
      "Validation RMSE:  2.9773963501166874\n",
      "Test RMSE:  2.3082870692793875\n",
      "Epoch: 0148 Training RMSE= 2.884667946\n",
      "Validation RMSE:  2.982807706546279\n",
      "Test RMSE:  2.336011828002314\n",
      "Epoch: 0149 Training RMSE= 2.883408271\n",
      "Validation RMSE:  3.052652193160387\n",
      "Test RMSE:  2.3559236609116594\n",
      "Epoch: 0150 Training RMSE= 2.883539868\n",
      "Validation RMSE:  2.9892897954686033\n",
      "Test RMSE:  2.329666267569464\n",
      "Epoch: 0151 Training RMSE= 2.871684653\n",
      "Validation RMSE:  2.9455466341812264\n",
      "Test RMSE:  2.280401463968195\n",
      "Epoch: 0152 Training RMSE= 2.864617069\n",
      "Validation RMSE:  2.9703591252279087\n",
      "Test RMSE:  2.309027642335306\n",
      "Epoch: 0153 Training RMSE= 2.866907704\n",
      "Validation RMSE:  2.963501778564898\n",
      "Test RMSE:  2.3096554762641257\n",
      "Epoch: 0154 Training RMSE= 2.861081353\n",
      "Validation RMSE:  2.980632272536688\n",
      "Test RMSE:  2.3266875162347254\n",
      "Epoch: 0155 Training RMSE= 2.852895466\n",
      "Validation RMSE:  2.9147201286936206\n",
      "Test RMSE:  2.2566540263872357\n",
      "Epoch: 0156 Training RMSE= 2.840050676\n",
      "Validation RMSE:  2.9364821111107235\n",
      "Test RMSE:  2.299720506686522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0157 Training RMSE= 2.848209634\n",
      "Validation RMSE:  2.975077541526865\n",
      "Test RMSE:  2.3569365525686448\n",
      "Epoch: 0158 Training RMSE= 2.832907001\n",
      "Validation RMSE:  2.919795494789302\n",
      "Test RMSE:  2.267463766626564\n",
      "Epoch: 0159 Training RMSE= 2.829492592\n",
      "Validation RMSE:  2.9322911810775727\n",
      "Test RMSE:  2.2957288386547203\n",
      "Epoch: 0160 Training RMSE= 2.825411014\n",
      "Validation RMSE:  2.9070914291226098\n",
      "Test RMSE:  2.2677124667067936\n",
      "Epoch: 0161 Training RMSE= 2.821934497\n",
      "Validation RMSE:  2.903186290738662\n",
      "Test RMSE:  2.2550390692453846\n",
      "Epoch: 0162 Training RMSE= 2.806874437\n",
      "Validation RMSE:  2.8809553123922345\n",
      "Test RMSE:  2.239606761070555\n",
      "Epoch: 0163 Training RMSE= 2.805826441\n",
      "Validation RMSE:  2.9186479424026603\n",
      "Test RMSE:  2.2851789554869915\n",
      "Epoch: 0164 Training RMSE= 2.806229030\n",
      "Validation RMSE:  2.887910362082297\n",
      "Test RMSE:  2.2484088808402793\n",
      "Epoch: 0165 Training RMSE= 2.792082407\n",
      "Validation RMSE:  2.8918868120330252\n",
      "Test RMSE:  2.2671395994104278\n",
      "Epoch: 0166 Training RMSE= 2.789680675\n",
      "Validation RMSE:  2.9343823263221096\n",
      "Test RMSE:  2.280442682883722\n",
      "Epoch: 0167 Training RMSE= 2.780011782\n",
      "Validation RMSE:  2.9195205667865274\n",
      "Test RMSE:  2.274559976132081\n",
      "Epoch: 0168 Training RMSE= 2.780344938\n",
      "Validation RMSE:  2.9003922895946928\n",
      "Test RMSE:  2.2924096174737607\n",
      "Epoch: 0169 Training RMSE= 2.776235380\n",
      "Validation RMSE:  2.862018530554398\n",
      "Test RMSE:  2.2278876794851175\n",
      "Epoch: 0170 Training RMSE= 2.769597734\n",
      "Validation RMSE:  2.871881805013371\n",
      "Test RMSE:  2.2405495858369746\n",
      "Epoch: 0171 Training RMSE= 2.762116069\n",
      "Validation RMSE:  2.851272296508949\n",
      "Test RMSE:  2.229643112735241\n",
      "Epoch: 0172 Training RMSE= 2.762761514\n",
      "Validation RMSE:  2.8995898236213367\n",
      "Test RMSE:  2.2600032645176626\n",
      "Epoch: 0173 Training RMSE= 2.751137273\n",
      "Validation RMSE:  2.858938840526152\n",
      "Test RMSE:  2.2436122155985427\n",
      "Epoch: 0174 Training RMSE= 2.744082606\n",
      "Validation RMSE:  2.893697847757364\n",
      "Test RMSE:  2.2549080300025857\n",
      "Epoch: 0175 Training RMSE= 2.740594293\n",
      "Validation RMSE:  2.868291886429366\n",
      "Test RMSE:  2.2448096145537844\n",
      "Epoch: 0176 Training RMSE= 2.747325184\n",
      "Validation RMSE:  2.829953672043336\n",
      "Test RMSE:  2.2141837426736615\n",
      "Epoch: 0177 Training RMSE= 2.732737977\n",
      "Validation RMSE:  2.8385294479433774\n",
      "Test RMSE:  2.225903718095688\n",
      "Epoch: 0178 Training RMSE= 2.728000249\n",
      "Validation RMSE:  2.8818262578994926\n",
      "Test RMSE:  2.265802823684735\n",
      "Epoch: 0179 Training RMSE= 2.723998185\n",
      "Validation RMSE:  2.8417995597066357\n",
      "Test RMSE:  2.259143648134677\n",
      "Epoch: 0180 Training RMSE= 2.721658151\n",
      "Validation RMSE:  2.827273372333947\n",
      "Test RMSE:  2.2080676455786685\n",
      "Epoch: 0181 Training RMSE= 2.711157055\n",
      "Validation RMSE:  2.8114279929228596\n",
      "Test RMSE:  2.2019196588132837\n",
      "Epoch: 0182 Training RMSE= 2.707657292\n",
      "Validation RMSE:  2.8624894529256624\n",
      "Test RMSE:  2.2655924038350865\n",
      "Epoch: 0183 Training RMSE= 2.709005812\n",
      "Validation RMSE:  2.811799915919719\n",
      "Test RMSE:  2.1975124508718733\n",
      "Epoch: 0184 Training RMSE= 2.695568648\n",
      "Validation RMSE:  2.869810728550979\n",
      "Test RMSE:  2.253418458089228\n",
      "Epoch: 0185 Training RMSE= 2.705132296\n",
      "Validation RMSE:  2.794801294909961\n",
      "Test RMSE:  2.1927022246746106\n",
      "Epoch: 0186 Training RMSE= 2.693980802\n",
      "Validation RMSE:  2.8136825830355665\n",
      "Test RMSE:  2.2104200155201053\n",
      "Epoch: 0187 Training RMSE= 2.686360000\n",
      "Validation RMSE:  2.794918313547314\n",
      "Test RMSE:  2.189557742529758\n",
      "Epoch: 0188 Training RMSE= 2.686210977\n",
      "Validation RMSE:  2.792682510326784\n",
      "Test RMSE:  2.181815691065089\n",
      "Epoch: 0189 Training RMSE= 2.681128883\n",
      "Validation RMSE:  2.8288344868488853\n",
      "Test RMSE:  2.2251496366482\n",
      "Epoch: 0190 Training RMSE= 2.676632780\n",
      "Validation RMSE:  2.7889857989538154\n",
      "Test RMSE:  2.1867308899439233\n",
      "Epoch: 0191 Training RMSE= 2.674126222\n",
      "Validation RMSE:  2.8302311089172227\n",
      "Test RMSE:  2.231814858452297\n",
      "Epoch: 0192 Training RMSE= 2.667127787\n",
      "Validation RMSE:  2.795837657506321\n",
      "Test RMSE:  2.2046750750282813\n",
      "Epoch: 0193 Training RMSE= 2.668379611\n",
      "Validation RMSE:  2.8090662658238394\n",
      "Test RMSE:  2.20193476348687\n",
      "Epoch: 0194 Training RMSE= 2.661749411\n",
      "Validation RMSE:  2.780051927012952\n",
      "Test RMSE:  2.173819076642641\n",
      "Epoch: 0195 Training RMSE= 2.657260081\n",
      "Validation RMSE:  2.8030203962424354\n",
      "Test RMSE:  2.1960059458829293\n",
      "Epoch: 0196 Training RMSE= 2.656018983\n",
      "Validation RMSE:  2.791756448203499\n",
      "Test RMSE:  2.212562381813238\n",
      "Epoch: 0197 Training RMSE= 2.646803919\n",
      "Validation RMSE:  2.7726621161791414\n",
      "Test RMSE:  2.1900750807298492\n",
      "Epoch: 0198 Training RMSE= 2.653806855\n",
      "Validation RMSE:  2.764789067029695\n",
      "Test RMSE:  2.1705328821900283\n",
      "Epoch: 0199 Training RMSE= 2.646350018\n",
      "Validation RMSE:  2.768371209955896\n",
      "Test RMSE:  2.1723565178472413\n",
      "Epoch: 0200 Training RMSE= 2.637411704\n",
      "Validation RMSE:  2.7542795131224325\n",
      "Test RMSE:  2.1656581215575628\n",
      "Epoch: 0201 Training RMSE= 2.642363564\n",
      "Validation RMSE:  2.7897158800409843\n",
      "Test RMSE:  2.199205171533001\n",
      "Epoch: 0202 Training RMSE= 2.635172762\n",
      "Validation RMSE:  2.7550829598662805\n",
      "Test RMSE:  2.159422613632007\n",
      "Epoch: 0203 Training RMSE= 2.630964997\n",
      "Validation RMSE:  2.767545819682881\n",
      "Test RMSE:  2.179086062130117\n",
      "Epoch: 0204 Training RMSE= 2.626894027\n",
      "Validation RMSE:  2.76392126676763\n",
      "Test RMSE:  2.17002987871827\n",
      "Epoch: 0205 Training RMSE= 2.624031154\n",
      "Validation RMSE:  2.759814068260415\n",
      "Test RMSE:  2.1700148129067487\n",
      "Epoch: 0206 Training RMSE= 2.623733678\n",
      "Validation RMSE:  2.7376863102677467\n",
      "Test RMSE:  2.1563646866525956\n",
      "Epoch: 0207 Training RMSE= 2.615218808\n",
      "Validation RMSE:  2.7304137173892906\n",
      "Test RMSE:  2.148620250905489\n",
      "Epoch: 0208 Training RMSE= 2.618293180\n",
      "Validation RMSE:  2.7333905436956174\n",
      "Test RMSE:  2.149581091568393\n",
      "Epoch: 0209 Training RMSE= 2.614160408\n",
      "Validation RMSE:  2.7357295141895404\n",
      "Test RMSE:  2.1510797932888948\n",
      "Epoch: 0210 Training RMSE= 2.606410507\n",
      "Validation RMSE:  2.7444555353046964\n",
      "Test RMSE:  2.1655207379056516\n",
      "Epoch: 0211 Training RMSE= 2.607385192\n",
      "Validation RMSE:  2.764975347564231\n",
      "Test RMSE:  2.192583702755848\n",
      "Epoch: 0212 Training RMSE= 2.606681900\n",
      "Validation RMSE:  2.815832473782599\n",
      "Test RMSE:  2.2330646273771455\n",
      "Epoch: 0213 Training RMSE= 2.596122828\n",
      "Validation RMSE:  2.749750819621175\n",
      "Test RMSE:  2.171923533933359\n",
      "Epoch: 0214 Training RMSE= 2.596049785\n",
      "Validation RMSE:  2.718702622801045\n",
      "Test RMSE:  2.1413786459434094\n",
      "Epoch: 0215 Training RMSE= 2.594023010\n",
      "Validation RMSE:  2.720244884581866\n",
      "Test RMSE:  2.135627174856381\n",
      "Epoch: 0216 Training RMSE= 2.590706122\n",
      "Validation RMSE:  2.759717159291127\n",
      "Test RMSE:  2.1817603151006097\n",
      "Epoch: 0217 Training RMSE= 2.584874228\n",
      "Validation RMSE:  2.731321100841514\n",
      "Test RMSE:  2.1556438616875573\n",
      "Epoch: 0218 Training RMSE= 2.581120876\n",
      "Validation RMSE:  2.7266796709914867\n",
      "Test RMSE:  2.1713218396193685\n",
      "Epoch: 0219 Training RMSE= 2.589868795\n",
      "Validation RMSE:  2.7429128269024003\n",
      "Test RMSE:  2.189681042149525\n",
      "Epoch: 0220 Training RMSE= 2.579029540\n",
      "Validation RMSE:  2.7199257029526036\n",
      "Test RMSE:  2.1452858158460795\n",
      "Epoch: 0221 Training RMSE= 2.572626975\n",
      "Validation RMSE:  2.6964299635942197\n",
      "Test RMSE:  2.1221788275930065\n",
      "Epoch: 0222 Training RMSE= 2.577074433\n",
      "Validation RMSE:  2.7245176457103017\n",
      "Test RMSE:  2.1600146669755227\n",
      "Epoch: 0223 Training RMSE= 2.569415987\n",
      "Validation RMSE:  2.7149779581611866\n",
      "Test RMSE:  2.145790314166\n",
      "Epoch: 0224 Training RMSE= 2.566653502\n",
      "Validation RMSE:  2.7178689910396967\n",
      "Test RMSE:  2.1544038222077875\n",
      "Epoch: 0225 Training RMSE= 2.561109419\n",
      "Validation RMSE:  2.7638795808308436\n",
      "Test RMSE:  2.1900710936115604\n",
      "Epoch: 0226 Training RMSE= 2.568758328\n",
      "Validation RMSE:  2.7144389557424025\n",
      "Test RMSE:  2.1600660060360624\n",
      "Epoch: 0227 Training RMSE= 2.565388970\n",
      "Validation RMSE:  2.676207437475247\n",
      "Test RMSE:  2.109256218285577\n",
      "Epoch: 0228 Training RMSE= 2.556418774\n",
      "Validation RMSE:  2.726629567892674\n",
      "Test RMSE:  2.1665523954359704\n",
      "Epoch: 0229 Training RMSE= 2.553718514\n",
      "Validation RMSE:  2.6722680856765595\n",
      "Test RMSE:  2.112836766345537\n",
      "Epoch: 0230 Training RMSE= 2.550348170\n",
      "Validation RMSE:  2.701730092610409\n",
      "Test RMSE:  2.146126145381156\n",
      "Epoch: 0231 Training RMSE= 2.550515107\n",
      "Validation RMSE:  2.675959649369096\n",
      "Test RMSE:  2.114990234375\n",
      "Epoch: 0232 Training RMSE= 2.544294243\n",
      "Validation RMSE:  2.7239116472628324\n",
      "Test RMSE:  2.188788742820544\n",
      "Epoch: 0233 Training RMSE= 2.547834211\n",
      "Validation RMSE:  2.6664840119113427\n",
      "Test RMSE:  2.10923937608793\n",
      "Epoch: 0234 Training RMSE= 2.536202244\n",
      "Validation RMSE:  2.662193067526702\n",
      "Test RMSE:  2.1029627222991865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0235 Training RMSE= 2.542600382\n",
      "Validation RMSE:  2.66673739657379\n",
      "Test RMSE:  2.1058978066451677\n",
      "Epoch: 0236 Training RMSE= 2.535182446\n",
      "Validation RMSE:  2.6725332776559063\n",
      "Test RMSE:  2.1165803834144725\n",
      "Epoch: 0237 Training RMSE= 2.530195895\n",
      "Validation RMSE:  2.6646858224453034\n",
      "Test RMSE:  2.102955253858771\n",
      "Epoch: 0238 Training RMSE= 2.534483516\n",
      "Validation RMSE:  2.659003883403454\n",
      "Test RMSE:  2.0940938496431167\n",
      "Epoch: 0239 Training RMSE= 2.531700436\n",
      "Validation RMSE:  2.6635709292271734\n",
      "Test RMSE:  2.1101707935361693\n",
      "Epoch: 0240 Training RMSE= 2.524339868\n",
      "Validation RMSE:  2.6588661101418913\n",
      "Test RMSE:  2.104709835259657\n",
      "Epoch: 0241 Training RMSE= 2.521839073\n",
      "Validation RMSE:  2.6482862356194015\n",
      "Test RMSE:  2.092481727224071\n",
      "Epoch: 0242 Training RMSE= 2.521201607\n",
      "Validation RMSE:  2.6594543875512007\n",
      "Test RMSE:  2.104037589341154\n",
      "Epoch: 0243 Training RMSE= 2.517071383\n",
      "Validation RMSE:  2.678529294454613\n",
      "Test RMSE:  2.1329998730526656\n",
      "Epoch: 0244 Training RMSE= 2.514132660\n",
      "Validation RMSE:  2.656682080300328\n",
      "Test RMSE:  2.1024082850116628\n",
      "Epoch: 0245 Training RMSE= 2.517946660\n",
      "Validation RMSE:  2.636567740178007\n",
      "Test RMSE:  2.0910584529294023\n",
      "Epoch: 0246 Training RMSE= 2.512693643\n",
      "Validation RMSE:  2.6329422417885784\n",
      "Test RMSE:  2.085606709515256\n",
      "Epoch: 0247 Training RMSE= 2.509374780\n",
      "Validation RMSE:  2.6706354935839625\n",
      "Test RMSE:  2.153226570978083\n",
      "Epoch: 0248 Training RMSE= 2.506982170\n",
      "Validation RMSE:  2.6364114539898167\n",
      "Test RMSE:  2.0858272137579443\n",
      "Epoch: 0249 Training RMSE= 2.508552480\n",
      "Validation RMSE:  2.6347442015278135\n",
      "Test RMSE:  2.0855586961695924\n",
      "Epoch: 0250 Training RMSE= 2.501312910\n",
      "Validation RMSE:  2.6460224719790877\n",
      "Test RMSE:  2.0956582440550404\n",
      "Epoch: 0251 Training RMSE= 2.501461810\n",
      "Validation RMSE:  2.6380097118167103\n",
      "Test RMSE:  2.089261139560977\n",
      "Epoch: 0252 Training RMSE= 2.494936280\n",
      "Validation RMSE:  2.641386362501328\n",
      "Test RMSE:  2.0890233220925474\n",
      "Epoch: 0253 Training RMSE= 2.497898573\n",
      "Validation RMSE:  2.6329196262921517\n",
      "Test RMSE:  2.0986292196875875\n",
      "Epoch: 0254 Training RMSE= 2.499979703\n",
      "Validation RMSE:  2.6388622246337645\n",
      "Test RMSE:  2.081315982333999\n",
      "Epoch: 0255 Training RMSE= 2.490854348\n",
      "Validation RMSE:  2.6436952878372466\n",
      "Test RMSE:  2.1023201976918378\n",
      "Epoch: 0256 Training RMSE= 2.487870036\n",
      "Validation RMSE:  2.6522359699716347\n",
      "Test RMSE:  2.104871718310275\n",
      "Epoch: 0257 Training RMSE= 2.490686301\n",
      "Validation RMSE:  2.631335129799176\n",
      "Test RMSE:  2.08350008456141\n",
      "Epoch: 0258 Training RMSE= 2.484003320\n",
      "Validation RMSE:  2.6491017816674747\n",
      "Test RMSE:  2.122573392652789\n",
      "Epoch: 0259 Training RMSE= 2.484162483\n",
      "Validation RMSE:  2.6704199325709004\n",
      "Test RMSE:  2.1090084605176154\n",
      "Epoch: 0260 Training RMSE= 2.484585922\n",
      "Validation RMSE:  2.613030186383517\n",
      "Test RMSE:  2.0667493279394744\n",
      "Epoch: 0261 Training RMSE= 2.478796376\n",
      "Validation RMSE:  2.644999745734911\n",
      "Test RMSE:  2.1050767976066864\n",
      "Epoch: 0262 Training RMSE= 2.480552838\n",
      "Validation RMSE:  2.62445993771593\n",
      "Test RMSE:  2.077747425397669\n",
      "Epoch: 0263 Training RMSE= 2.473495341\n",
      "Validation RMSE:  2.6206851646454967\n",
      "Test RMSE:  2.073125533183515\n",
      "Epoch: 0264 Training RMSE= 2.472855684\n",
      "Validation RMSE:  2.6498599213338903\n",
      "Test RMSE:  2.1171295608531744\n",
      "Epoch: 0265 Training RMSE= 2.473756697\n",
      "Validation RMSE:  2.611699764765807\n",
      "Test RMSE:  2.0704886739375454\n",
      "Epoch: 0266 Training RMSE= 2.467934979\n",
      "Validation RMSE:  2.6521386136039427\n",
      "Test RMSE:  2.124159338100278\n",
      "Epoch: 0267 Training RMSE= 2.467467291\n",
      "Validation RMSE:  2.6103670895418305\n",
      "Test RMSE:  2.0697287402277\n",
      "Epoch: 0268 Training RMSE= 2.469909804\n",
      "Validation RMSE:  2.6543789276173033\n",
      "Test RMSE:  2.115479374977285\n",
      "Epoch: 0269 Training RMSE= 2.464588886\n",
      "Validation RMSE:  2.6028252592769063\n",
      "Test RMSE:  2.0647188735594444\n",
      "Epoch: 0270 Training RMSE= 2.463940482\n",
      "Validation RMSE:  2.6406758771881957\n",
      "Test RMSE:  2.1260639220036532\n",
      "Epoch: 0271 Training RMSE= 2.461373444\n",
      "Validation RMSE:  2.6176705213599574\n",
      "Test RMSE:  2.0789633472490605\n",
      "Epoch: 0272 Training RMSE= 2.456223971\n",
      "Validation RMSE:  2.620614839511847\n",
      "Test RMSE:  2.074939925691483\n",
      "Epoch: 0273 Training RMSE= 2.460037481\n",
      "Validation RMSE:  2.6154333128699974\n",
      "Test RMSE:  2.0798513226145197\n",
      "Epoch: 0274 Training RMSE= 2.457032686\n",
      "Validation RMSE:  2.6078264358358427\n",
      "Test RMSE:  2.0653800363186616\n",
      "Epoch: 0275 Training RMSE= 2.453071530\n",
      "Validation RMSE:  2.6246802271485468\n",
      "Test RMSE:  2.0943960787202758\n",
      "Epoch: 0276 Training RMSE= 2.451842448\n",
      "Validation RMSE:  2.595800784005434\n",
      "Test RMSE:  2.06749640338774\n",
      "Epoch: 0277 Training RMSE= 2.448686204\n",
      "Validation RMSE:  2.6015570700648922\n",
      "Test RMSE:  2.0740073288463416\n",
      "Epoch: 0278 Training RMSE= 2.447600509\n",
      "Validation RMSE:  2.6182157859260498\n",
      "Test RMSE:  2.09124413746561\n",
      "Epoch: 0279 Training RMSE= 2.450495826\n",
      "Validation RMSE:  2.610054384519981\n",
      "Test RMSE:  2.0804306519558646\n",
      "Epoch: 0280 Training RMSE= 2.445225860\n",
      "Validation RMSE:  2.6121389860560167\n",
      "Test RMSE:  2.076669470082019\n",
      "Epoch: 0281 Training RMSE= 2.441096719\n",
      "Validation RMSE:  2.593932409651503\n",
      "Test RMSE:  2.0611573819970053\n",
      "Epoch: 0282 Training RMSE= 2.442516317\n",
      "Validation RMSE:  2.627476409639248\n",
      "Test RMSE:  2.108425421142355\n",
      "Epoch: 0283 Training RMSE= 2.439990241\n",
      "Validation RMSE:  2.607537267753266\n",
      "Test RMSE:  2.0638851514077636\n",
      "Epoch: 0284 Training RMSE= 2.438536600\n",
      "Validation RMSE:  2.610867901279071\n",
      "Test RMSE:  2.0850893789568903\n",
      "Epoch: 0285 Training RMSE= 2.437570762\n",
      "Validation RMSE:  2.599423603209748\n",
      "Test RMSE:  2.067265841959544\n",
      "Epoch: 0286 Training RMSE= 2.434307392\n",
      "Validation RMSE:  2.6031793146775213\n",
      "Test RMSE:  2.068949916780099\n",
      "Epoch: 0287 Training RMSE= 2.432075509\n",
      "Validation RMSE:  2.579177780209592\n",
      "Test RMSE:  2.043203788934875\n",
      "Epoch: 0288 Training RMSE= 2.431138394\n",
      "Validation RMSE:  2.58359416803634\n",
      "Test RMSE:  2.042794185496224\n",
      "Epoch: 0289 Training RMSE= 2.429486480\n",
      "Validation RMSE:  2.6295503957981787\n",
      "Test RMSE:  2.1066536904214037\n",
      "Epoch: 0290 Training RMSE= 2.425921548\n",
      "Validation RMSE:  2.6236847125431866\n",
      "Test RMSE:  2.094883057054879\n",
      "Epoch: 0291 Training RMSE= 2.427261139\n",
      "Validation RMSE:  2.602002951421684\n",
      "Test RMSE:  2.0634729084753425\n",
      "Epoch: 0292 Training RMSE= 2.428739494\n",
      "Validation RMSE:  2.5853048513016446\n",
      "Test RMSE:  2.0487851992056862\n",
      "Epoch: 0293 Training RMSE= 2.420534235\n",
      "Validation RMSE:  2.595735318663811\n",
      "Test RMSE:  2.0591635594048\n",
      "Epoch: 0294 Training RMSE= 2.423981020\n",
      "Validation RMSE:  2.5834999235849754\n",
      "Test RMSE:  2.055728402116271\n",
      "Epoch: 0295 Training RMSE= 2.418073856\n",
      "Validation RMSE:  2.608570134643523\n",
      "Test RMSE:  2.0650990755869074\n",
      "Epoch: 0296 Training RMSE= 2.417542090\n",
      "Validation RMSE:  2.590781915051581\n",
      "Test RMSE:  2.069681942480299\n",
      "Epoch: 0297 Training RMSE= 2.415156364\n",
      "Validation RMSE:  2.6139367255161\n",
      "Test RMSE:  2.087855981980084\n",
      "Epoch: 0298 Training RMSE= 2.416001565\n",
      "Validation RMSE:  2.5758008248647215\n",
      "Test RMSE:  2.060122448929982\n",
      "Epoch: 0299 Training RMSE= 2.415565327\n",
      "Validation RMSE:  2.5758826243080915\n",
      "Test RMSE:  2.054289964286001\n",
      "Epoch: 0300 Training RMSE= 2.408970430\n",
      "Validation RMSE:  2.569589507386945\n",
      "Test RMSE:  2.038448957603731\n",
      "Epoch: 0301 Training RMSE= 2.413487985\n",
      "Validation RMSE:  2.567110264104441\n",
      "Test RMSE:  2.0341983085582163\n",
      "Epoch: 0302 Training RMSE= 2.405859058\n",
      "Validation RMSE:  2.5840241882532573\n",
      "Test RMSE:  2.056726466576448\n",
      "Epoch: 0303 Training RMSE= 2.406616451\n",
      "Validation RMSE:  2.5778170314993623\n",
      "Test RMSE:  2.052234679577689\n",
      "Epoch: 0304 Training RMSE= 2.407355533\n",
      "Validation RMSE:  2.572639131184978\n",
      "Test RMSE:  2.051416324192972\n",
      "Epoch: 0305 Training RMSE= 2.400500429\n",
      "Validation RMSE:  2.609406750879877\n",
      "Test RMSE:  2.0761111547539106\n",
      "Epoch: 0306 Training RMSE= 2.401140845\n",
      "Validation RMSE:  2.5889724255342497\n",
      "Test RMSE:  2.0568408633111464\n",
      "Epoch: 0307 Training RMSE= 2.400035040\n",
      "Validation RMSE:  2.5735012785391618\n",
      "Test RMSE:  2.0444607948113585\n",
      "Epoch: 0308 Training RMSE= 2.401408825\n",
      "Validation RMSE:  2.5765311660850796\n",
      "Test RMSE:  2.0532148036738587\n",
      "Epoch: 0309 Training RMSE= 2.394066715\n",
      "Validation RMSE:  2.574948799216842\n",
      "Test RMSE:  2.0515649224620733\n",
      "Epoch: 0310 Training RMSE= 2.399128927\n",
      "Validation RMSE:  2.562701380190234\n",
      "Test RMSE:  2.043889554826557\n",
      "Epoch: 0311 Training RMSE= 2.393450744\n",
      "Validation RMSE:  2.568845591193513\n",
      "Test RMSE:  2.0507473897409296\n",
      "Epoch: 0312 Training RMSE= 2.394143770\n",
      "Validation RMSE:  2.5813836051594663\n",
      "Test RMSE:  2.062177748503147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0313 Training RMSE= 2.392896725\n",
      "Validation RMSE:  2.565612321792111\n",
      "Test RMSE:  2.0353607543585475\n",
      "Epoch: 0314 Training RMSE= 2.385560557\n",
      "Validation RMSE:  2.5556538005501843\n",
      "Test RMSE:  2.0314608977946502\n",
      "Epoch: 0315 Training RMSE= 2.393471585\n",
      "Validation RMSE:  2.5606563029704796\n",
      "Test RMSE:  2.039430007847393\n",
      "Epoch: 0316 Training RMSE= 2.385072123\n",
      "Validation RMSE:  2.565153097599349\n",
      "Test RMSE:  2.0461479235188875\n",
      "Epoch: 0317 Training RMSE= 2.389117769\n",
      "Validation RMSE:  2.5761127826660664\n",
      "Test RMSE:  2.0503712707524686\n",
      "Epoch: 0318 Training RMSE= 2.384132932\n",
      "Validation RMSE:  2.5599724469341245\n",
      "Test RMSE:  2.0447087509307935\n",
      "Epoch: 0319 Training RMSE= 2.382995373\n",
      "Validation RMSE:  2.5699104064138147\n",
      "Test RMSE:  2.056437874533373\n",
      "Epoch: 0320 Training RMSE= 2.384542676\n",
      "Validation RMSE:  2.5623113516241043\n",
      "Test RMSE:  2.0389883385980068\n",
      "Epoch: 0321 Training RMSE= 2.379616793\n",
      "Validation RMSE:  2.568239645832452\n",
      "Test RMSE:  2.0564171215889107\n",
      "Epoch: 0322 Training RMSE= 2.378339633\n",
      "Validation RMSE:  2.5573622207594537\n",
      "Test RMSE:  2.0313511016333803\n",
      "Epoch: 0323 Training RMSE= 2.377256373\n",
      "Validation RMSE:  2.571545893399601\n",
      "Test RMSE:  2.049298271587955\n",
      "Epoch: 0324 Training RMSE= 2.378552515\n",
      "Validation RMSE:  2.555408784261311\n",
      "Test RMSE:  2.0430316222456137\n",
      "Epoch: 0325 Training RMSE= 2.376490607\n",
      "Validation RMSE:  2.5596180736360292\n",
      "Test RMSE:  2.0406811745872626\n",
      "Epoch: 0326 Training RMSE= 2.372473919\n",
      "Validation RMSE:  2.5536726822914395\n",
      "Test RMSE:  2.033391498557132\n",
      "Epoch: 0327 Training RMSE= 2.373433759\n",
      "Validation RMSE:  2.556057553870138\n",
      "Test RMSE:  2.0287873084538885\n",
      "Epoch: 0328 Training RMSE= 2.372315777\n",
      "Validation RMSE:  2.557457638163171\n",
      "Test RMSE:  2.0363603716327145\n",
      "Epoch: 0329 Training RMSE= 2.365727511\n",
      "Validation RMSE:  2.5429901731558484\n",
      "Test RMSE:  2.025210101213152\n",
      "Epoch: 0330 Training RMSE= 2.370163100\n",
      "Validation RMSE:  2.5529959441756813\n",
      "Test RMSE:  2.0435616998516624\n",
      "Epoch: 0331 Training RMSE= 2.367399341\n",
      "Validation RMSE:  2.547495994452415\n",
      "Test RMSE:  2.0366264348679137\n",
      "Epoch: 0332 Training RMSE= 2.365837790\n",
      "Validation RMSE:  2.5583822381339405\n",
      "Test RMSE:  2.046599507902279\n",
      "Epoch: 0333 Training RMSE= 2.365213330\n",
      "Validation RMSE:  2.538291274098204\n",
      "Test RMSE:  2.0314629809888745\n",
      "Epoch: 0334 Training RMSE= 2.362992753\n",
      "Validation RMSE:  2.554618531208982\n",
      "Test RMSE:  2.0296066385538007\n",
      "Epoch: 0335 Training RMSE= 2.359001499\n",
      "Validation RMSE:  2.5489994152288413\n",
      "Test RMSE:  2.0370814759491833\n",
      "Epoch: 0336 Training RMSE= 2.361314648\n",
      "Validation RMSE:  2.5306997525288626\n",
      "Test RMSE:  2.0134176099293013\n",
      "Epoch: 0337 Training RMSE= 2.357341825\n",
      "Validation RMSE:  2.529742062541604\n",
      "Test RMSE:  2.013875675340764\n",
      "Epoch: 0338 Training RMSE= 2.359581657\n",
      "Validation RMSE:  2.53574137701994\n",
      "Test RMSE:  2.018003849044774\n",
      "Epoch: 0339 Training RMSE= 2.355387973\n",
      "Validation RMSE:  2.556410718682765\n",
      "Test RMSE:  2.0423392498488306\n",
      "Epoch: 0340 Training RMSE= 2.356874548\n",
      "Validation RMSE:  2.54467458394476\n",
      "Test RMSE:  2.0343307900495953\n",
      "Epoch: 0341 Training RMSE= 2.352067975\n",
      "Validation RMSE:  2.5367219201794486\n",
      "Test RMSE:  2.022178140169688\n",
      "Epoch: 0342 Training RMSE= 2.355161388\n",
      "Validation RMSE:  2.5426555153501025\n",
      "Test RMSE:  2.0213278152358836\n",
      "Epoch: 0343 Training RMSE= 2.348711692\n",
      "Validation RMSE:  2.529516639146522\n",
      "Test RMSE:  2.014609887054292\n",
      "Epoch: 0344 Training RMSE= 2.351146058\n",
      "Validation RMSE:  2.53047556862968\n",
      "Test RMSE:  2.0158625913654284\n",
      "Epoch: 0345 Training RMSE= 2.348860609\n",
      "Validation RMSE:  2.553188058921863\n",
      "Test RMSE:  2.040247927026732\n",
      "Epoch: 0346 Training RMSE= 2.347274820\n",
      "Validation RMSE:  2.538351998322113\n",
      "Test RMSE:  2.0329540001121047\n",
      "Epoch: 0347 Training RMSE= 2.346283825\n",
      "Validation RMSE:  2.52386135639748\n",
      "Test RMSE:  2.0115712824723535\n",
      "Epoch: 0348 Training RMSE= 2.344568308\n",
      "Validation RMSE:  2.532626896058031\n",
      "Test RMSE:  2.0258127947851143\n",
      "Epoch: 0349 Training RMSE= 2.344975914\n",
      "Validation RMSE:  2.5389312944991147\n",
      "Test RMSE:  2.0226073458444342\n",
      "Epoch: 0350 Training RMSE= 2.343041860\n",
      "Validation RMSE:  2.5353382439083894\n",
      "Test RMSE:  2.0238539366630595\n",
      "Epoch: 0351 Training RMSE= 2.341788320\n",
      "Validation RMSE:  2.567526307273038\n",
      "Test RMSE:  2.066227982733183\n",
      "Epoch: 0352 Training RMSE= 2.341016274\n",
      "Validation RMSE:  2.533659482862522\n",
      "Test RMSE:  2.02538840313075\n",
      "Epoch: 0353 Training RMSE= 2.340753659\n",
      "Validation RMSE:  2.5474572247553664\n",
      "Test RMSE:  2.030754835956831\n",
      "Epoch: 0354 Training RMSE= 2.337632580\n",
      "Validation RMSE:  2.519739494293503\n",
      "Test RMSE:  2.013405931229143\n",
      "Epoch: 0355 Training RMSE= 2.337821976\n",
      "Validation RMSE:  2.5251436164350616\n",
      "Test RMSE:  2.0123195602843356\n",
      "Epoch: 0356 Training RMSE= 2.336395359\n",
      "Validation RMSE:  2.5216767395063755\n",
      "Test RMSE:  2.0102901089484386\n",
      "Epoch: 0357 Training RMSE= 2.336787097\n",
      "Validation RMSE:  2.531447296930581\n",
      "Test RMSE:  2.0230407620470405\n",
      "Epoch: 0358 Training RMSE= 2.332051549\n",
      "Validation RMSE:  2.5704271702565467\n",
      "Test RMSE:  2.0640856976183746\n",
      "Epoch: 0359 Training RMSE= 2.333961983\n",
      "Validation RMSE:  2.5189114776797497\n",
      "Test RMSE:  2.0092892689310453\n",
      "Epoch: 0360 Training RMSE= 2.329205085\n",
      "Validation RMSE:  2.5124486690231134\n",
      "Test RMSE:  2.0127658230101915\n",
      "Epoch: 0361 Training RMSE= 2.332040022\n",
      "Validation RMSE:  2.530412794520896\n",
      "Test RMSE:  2.0249404922030427\n",
      "Epoch: 0362 Training RMSE= 2.329250785\n",
      "Validation RMSE:  2.538983880806561\n",
      "Test RMSE:  2.039204793474109\n",
      "Epoch: 0363 Training RMSE= 2.326469839\n",
      "Validation RMSE:  2.514814755763527\n",
      "Test RMSE:  2.015817041637457\n",
      "Epoch: 0364 Training RMSE= 2.328212296\n",
      "Validation RMSE:  2.5171472196949427\n",
      "Test RMSE:  2.01429734381282\n",
      "Epoch: 0365 Training RMSE= 2.324768489\n",
      "Validation RMSE:  2.5159846694385224\n",
      "Test RMSE:  2.0124650034023652\n",
      "Epoch: 0366 Training RMSE= 2.324164715\n",
      "Validation RMSE:  2.5325682939469876\n",
      "Test RMSE:  2.0216206085444197\n",
      "Epoch: 0367 Training RMSE= 2.324925022\n",
      "Validation RMSE:  2.518521033924172\n",
      "Test RMSE:  2.0228592624501185\n",
      "Epoch: 0368 Training RMSE= 2.324029069\n",
      "Validation RMSE:  2.510772574325064\n",
      "Test RMSE:  2.0096960176178547\n",
      "Epoch: 0369 Training RMSE= 2.320197725\n",
      "Validation RMSE:  2.5154520058236036\n",
      "Test RMSE:  2.0116001723813755\n",
      "Epoch: 0370 Training RMSE= 2.322017973\n",
      "Validation RMSE:  2.5082000242866727\n",
      "Test RMSE:  2.012797272092596\n",
      "Epoch: 0371 Training RMSE= 2.319868478\n",
      "Validation RMSE:  2.5215411545486703\n",
      "Test RMSE:  2.0153256423197523\n",
      "Epoch: 0372 Training RMSE= 2.318107634\n",
      "Validation RMSE:  2.51243405516516\n",
      "Test RMSE:  2.0106672927966565\n",
      "Epoch: 0373 Training RMSE= 2.316558094\n",
      "Validation RMSE:  2.5420615691990656\n",
      "Test RMSE:  2.037275049745363\n",
      "Epoch: 0374 Training RMSE= 2.315502712\n",
      "Validation RMSE:  2.5473317631882466\n",
      "Test RMSE:  2.033869755905393\n",
      "Epoch: 0375 Training RMSE= 2.316377917\n",
      "Validation RMSE:  2.524256216164894\n",
      "Test RMSE:  2.0189692971604942\n",
      "Epoch: 0376 Training RMSE= 2.311411197\n",
      "Validation RMSE:  2.504001728683399\n",
      "Test RMSE:  2.009116880885698\n",
      "Epoch: 0377 Training RMSE= 2.314343215\n",
      "Validation RMSE:  2.5116639553992868\n",
      "Test RMSE:  2.0043494346089714\n",
      "Epoch: 0378 Training RMSE= 2.309844188\n",
      "Validation RMSE:  2.5255517045269684\n",
      "Test RMSE:  2.0219160420705156\n",
      "Epoch: 0379 Training RMSE= 2.311391340\n",
      "Validation RMSE:  2.5188946296684622\n",
      "Test RMSE:  2.025170662835058\n",
      "Epoch: 0380 Training RMSE= 2.310247910\n",
      "Validation RMSE:  2.534308975243993\n",
      "Test RMSE:  2.040906591103964\n",
      "Epoch: 0381 Training RMSE= 2.309007371\n",
      "Validation RMSE:  2.5028937996186524\n",
      "Test RMSE:  2.0039376150431663\n",
      "Epoch: 0382 Training RMSE= 2.303360511\n",
      "Validation RMSE:  2.506729105867555\n",
      "Test RMSE:  2.0139510574464574\n",
      "Epoch: 0383 Training RMSE= 2.309367344\n",
      "Validation RMSE:  2.5160003998048035\n",
      "Test RMSE:  2.0210454639377775\n",
      "Epoch: 0384 Training RMSE= 2.307497924\n",
      "Validation RMSE:  2.4982092881364517\n",
      "Test RMSE:  1.9946295188426615\n",
      "Epoch: 0385 Training RMSE= 2.304801703\n",
      "Validation RMSE:  2.511439970928473\n",
      "Test RMSE:  2.0088362996542535\n",
      "Epoch: 0386 Training RMSE= 2.303240073\n",
      "Validation RMSE:  2.51549773753244\n",
      "Test RMSE:  2.0095392661338543\n",
      "Epoch: 0387 Training RMSE= 2.303586049\n",
      "Validation RMSE:  2.5099394860514948\n",
      "Test RMSE:  2.0031118770413503\n",
      "Epoch: 0388 Training RMSE= 2.299480162\n",
      "Validation RMSE:  2.523743790972678\n",
      "Test RMSE:  2.0212484765740046\n",
      "Epoch: 0389 Training RMSE= 2.300215421\n",
      "Validation RMSE:  2.526121147452201\n",
      "Test RMSE:  2.0274209244873482\n",
      "Epoch: 0390 Training RMSE= 2.300224846\n",
      "Validation RMSE:  2.5057180339529186\n",
      "Test RMSE:  2.011204833524642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0391 Training RMSE= 2.296658702\n",
      "Validation RMSE:  2.4980485452343664\n",
      "Test RMSE:  1.9997504942948572\n",
      "Epoch: 0392 Training RMSE= 2.300363715\n",
      "Validation RMSE:  2.5375872507738966\n",
      "Test RMSE:  2.0319431589429184\n",
      "Epoch: 0393 Training RMSE= 2.297568750\n",
      "Validation RMSE:  2.5075320743627847\n",
      "Test RMSE:  2.01095905959303\n",
      "Epoch: 0394 Training RMSE= 2.293176673\n",
      "Validation RMSE:  2.497876433641585\n",
      "Test RMSE:  2.005605352822013\n",
      "Epoch: 0395 Training RMSE= 2.295738435\n",
      "Validation RMSE:  2.5010909561157826\n",
      "Test RMSE:  2.0121033828577994\n",
      "Epoch: 0396 Training RMSE= 2.295165104\n",
      "Validation RMSE:  2.5128240926679375\n",
      "Test RMSE:  2.0070548798984227\n",
      "Epoch: 0397 Training RMSE= 2.291282424\n",
      "Validation RMSE:  2.494289599863346\n",
      "Test RMSE:  1.9988816233817381\n",
      "Epoch: 0398 Training RMSE= 2.290277142\n",
      "Validation RMSE:  2.503573533922721\n",
      "Test RMSE:  2.0117035948543607\n",
      "Epoch: 0399 Training RMSE= 2.292036350\n",
      "Validation RMSE:  2.49944344524388\n",
      "Test RMSE:  2.003519671594643\n",
      "Epoch: 0400 Training RMSE= 2.290476600\n",
      "Validation RMSE:  2.506077341425207\n",
      "Test RMSE:  2.0095000392755304\n",
      "Epoch: 0401 Training RMSE= 2.289630520\n",
      "Validation RMSE:  2.4927553588240308\n",
      "Test RMSE:  1.999824397007389\n",
      "Epoch: 0402 Training RMSE= 2.287021021\n",
      "Validation RMSE:  2.5047713524482935\n",
      "Test RMSE:  2.001528260342409\n",
      "Epoch: 0403 Training RMSE= 2.286693086\n",
      "Validation RMSE:  2.487704774680288\n",
      "Test RMSE:  1.9998264982545062\n",
      "Epoch: 0404 Training RMSE= 2.285587733\n",
      "Validation RMSE:  2.5209838228519996\n",
      "Test RMSE:  2.014231606805594\n",
      "Epoch: 0405 Training RMSE= 2.286411492\n",
      "Validation RMSE:  2.5218835539832276\n",
      "Test RMSE:  2.026569108209593\n",
      "Epoch: 0406 Training RMSE= 2.285861778\n",
      "Validation RMSE:  2.5161206959504687\n",
      "Test RMSE:  2.0111360168876065\n",
      "Epoch: 0407 Training RMSE= 2.282916811\n",
      "Validation RMSE:  2.490166134366768\n",
      "Test RMSE:  1.9915642006371654\n",
      "Epoch: 0408 Training RMSE= 2.284325520\n",
      "Validation RMSE:  2.477592663153186\n",
      "Test RMSE:  1.9900376182383932\n",
      "Epoch: 0409 Training RMSE= 2.279954150\n",
      "Validation RMSE:  2.5205151669772956\n",
      "Test RMSE:  2.0300139114062943\n",
      "Epoch: 0410 Training RMSE= 2.282737161\n",
      "Validation RMSE:  2.489296650574201\n",
      "Test RMSE:  2.0011003119953172\n",
      "Epoch: 0411 Training RMSE= 2.278607134\n",
      "Validation RMSE:  2.4879413174918006\n",
      "Test RMSE:  1.9964312661842742\n",
      "Epoch: 0412 Training RMSE= 2.280268753\n",
      "Validation RMSE:  2.482005686954005\n",
      "Test RMSE:  1.9916576653307714\n",
      "Epoch: 0413 Training RMSE= 2.276320833\n",
      "Validation RMSE:  2.5075709384076346\n",
      "Test RMSE:  2.008136708248899\n",
      "Epoch: 0414 Training RMSE= 2.277434733\n",
      "Validation RMSE:  2.4954458479343606\n",
      "Test RMSE:  2.008096771303025\n",
      "Epoch: 0415 Training RMSE= 2.276392002\n",
      "Validation RMSE:  2.4950610010797103\n",
      "Test RMSE:  2.0003254744817087\n",
      "Epoch: 0416 Training RMSE= 2.275804167\n",
      "Validation RMSE:  2.489394892393524\n",
      "Test RMSE:  2.0113593808262835\n",
      "Epoch: 0417 Training RMSE= 2.273882090\n",
      "Validation RMSE:  2.4994914014362766\n",
      "Test RMSE:  2.0087712741003\n",
      "Epoch: 0418 Training RMSE= 2.274784400\n",
      "Validation RMSE:  2.5101791575502888\n",
      "Test RMSE:  2.0239420523179263\n",
      "Epoch: 0419 Training RMSE= 2.271850375\n",
      "Validation RMSE:  2.496598910463611\n",
      "Test RMSE:  2.0007687819634983\n",
      "Epoch: 0420 Training RMSE= 2.274256799\n",
      "Validation RMSE:  2.4799180027649963\n",
      "Test RMSE:  1.9904665526085907\n",
      "Epoch: 0421 Training RMSE= 2.269107641\n",
      "Validation RMSE:  2.496405998364152\n",
      "Test RMSE:  1.9976698593374262\n",
      "Epoch: 0422 Training RMSE= 2.271151121\n",
      "Validation RMSE:  2.500384944843169\n",
      "Test RMSE:  2.0209664976022474\n",
      "Epoch: 0423 Training RMSE= 2.268934492\n",
      "Validation RMSE:  2.48242257130971\n",
      "Test RMSE:  1.9956870663217618\n",
      "Epoch: 0424 Training RMSE= 2.267514873\n",
      "Validation RMSE:  2.497729677249175\n",
      "Test RMSE:  2.012628160862164\n",
      "Epoch: 0425 Training RMSE= 2.267960525\n",
      "Validation RMSE:  2.4880399721743234\n",
      "Test RMSE:  1.9999612864085328\n",
      "Epoch: 0426 Training RMSE= 2.266862960\n",
      "Validation RMSE:  2.49722011506196\n",
      "Test RMSE:  2.0160823717101484\n",
      "Epoch: 0427 Training RMSE= 2.265548294\n",
      "Validation RMSE:  2.4985096303751546\n",
      "Test RMSE:  2.009867377342119\n",
      "Epoch: 0428 Training RMSE= 2.265905263\n",
      "Validation RMSE:  2.5037985077766454\n",
      "Test RMSE:  2.0249001066270496\n",
      "Epoch: 0429 Training RMSE= 2.262253277\n",
      "Validation RMSE:  2.4903357631014695\n",
      "Test RMSE:  1.999793295311373\n",
      "Epoch: 0430 Training RMSE= 2.266258988\n",
      "Validation RMSE:  2.4831688070093643\n",
      "Test RMSE:  1.9912199625406604\n",
      "Epoch: 0431 Training RMSE= 2.260175423\n",
      "Validation RMSE:  2.4863343579617965\n",
      "Test RMSE:  1.9926851735528162\n",
      "Epoch: 0432 Training RMSE= 2.260418285\n",
      "Validation RMSE:  2.5103400259599464\n",
      "Test RMSE:  2.0293266727436996\n",
      "Epoch: 0433 Training RMSE= 2.262946957\n",
      "Validation RMSE:  2.4824842539707617\n",
      "Test RMSE:  2.0005359080676173\n",
      "Epoch: 0434 Training RMSE= 2.260806457\n",
      "Validation RMSE:  2.488758562380797\n",
      "Test RMSE:  1.9962074113051824\n",
      "Epoch: 0435 Training RMSE= 2.258481541\n",
      "Validation RMSE:  2.4821650194352896\n",
      "Test RMSE:  2.0043273542811844\n",
      "Epoch: 0436 Training RMSE= 2.259171005\n",
      "Validation RMSE:  2.4931713063799363\n",
      "Test RMSE:  2.0028541881777757\n",
      "Epoch: 0437 Training RMSE= 2.257532661\n",
      "Validation RMSE:  2.4874100529879724\n",
      "Test RMSE:  1.9971279443758656\n",
      "Epoch: 0438 Training RMSE= 2.258585455\n",
      "Validation RMSE:  2.480984083094619\n",
      "Test RMSE:  1.9972323550119209\n",
      "Epoch: 0439 Training RMSE= 2.254483693\n",
      "Validation RMSE:  2.474962650364046\n",
      "Test RMSE:  1.9879179587033844\n",
      "Epoch: 0440 Training RMSE= 2.253740357\n",
      "Validation RMSE:  2.5127964347835317\n",
      "Test RMSE:  2.036891468422804\n",
      "Epoch: 0441 Training RMSE= 2.254715770\n",
      "Validation RMSE:  2.4799790266903665\n",
      "Test RMSE:  1.9936062865644548\n",
      "Epoch: 0442 Training RMSE= 2.255850957\n",
      "Validation RMSE:  2.4802266881381865\n",
      "Test RMSE:  2.0038928353081844\n",
      "Epoch: 0443 Training RMSE= 2.252452457\n",
      "Validation RMSE:  2.476913331029029\n",
      "Test RMSE:  1.9859128985017926\n",
      "Epoch: 0444 Training RMSE= 2.251846875\n",
      "Validation RMSE:  2.4768128373852063\n",
      "Test RMSE:  1.9936102629814147\n",
      "Epoch: 0445 Training RMSE= 2.253041133\n",
      "Validation RMSE:  2.499258575169844\n",
      "Test RMSE:  2.0197190689908493\n",
      "Epoch: 0446 Training RMSE= 2.250099479\n",
      "Validation RMSE:  2.476874419103261\n",
      "Test RMSE:  1.986554203277156\n",
      "Epoch: 0447 Training RMSE= 2.250735237\n",
      "Validation RMSE:  2.481638643013889\n",
      "Test RMSE:  2.0015875952500157\n",
      "Epoch: 0448 Training RMSE= 2.247891365\n",
      "Validation RMSE:  2.4893594319256387\n",
      "Test RMSE:  1.9920997899512796\n",
      "Epoch: 0449 Training RMSE= 2.248445270\n",
      "Validation RMSE:  2.4683224995993744\n",
      "Test RMSE:  1.991255508530176\n",
      "Epoch: 0450 Training RMSE= 2.249797311\n",
      "Validation RMSE:  2.4820373141245\n",
      "Test RMSE:  1.9963470268436991\n",
      "Epoch: 0451 Training RMSE= 2.247228086\n",
      "Validation RMSE:  2.486239783072828\n",
      "Test RMSE:  2.0050657312458653\n",
      "Epoch: 0452 Training RMSE= 2.244899442\n",
      "Validation RMSE:  2.484159928135358\n",
      "Test RMSE:  1.9961192060366546\n",
      "Epoch: 0453 Training RMSE= 2.246313930\n",
      "Validation RMSE:  2.4660020817904877\n",
      "Test RMSE:  1.9862900901911358\n",
      "Epoch: 0454 Training RMSE= 2.244510596\n",
      "Validation RMSE:  2.4621527199407525\n",
      "Test RMSE:  1.9855811440893145\n",
      "Epoch: 0455 Training RMSE= 2.242527296\n",
      "Validation RMSE:  2.4748714220961467\n",
      "Test RMSE:  1.9897691596630847\n",
      "Epoch: 0456 Training RMSE= 2.242709093\n",
      "Validation RMSE:  2.483019933083606\n",
      "Test RMSE:  1.9973242264851108\n",
      "Epoch: 0457 Training RMSE= 2.241496683\n",
      "Validation RMSE:  2.4880357558327506\n",
      "Test RMSE:  1.9950175958349823\n",
      "Epoch: 0458 Training RMSE= 2.242241479\n",
      "Validation RMSE:  2.4873569035225733\n",
      "Test RMSE:  2.007704112173928\n",
      "Epoch: 0459 Training RMSE= 2.239882762\n",
      "Validation RMSE:  2.48247841951454\n",
      "Test RMSE:  1.9851592459087442\n",
      "Epoch: 0460 Training RMSE= 2.241847684\n",
      "Validation RMSE:  2.4652771485615914\n",
      "Test RMSE:  1.9841867943369949\n",
      "Epoch: 0461 Training RMSE= 2.237950978\n",
      "Validation RMSE:  2.4698767203481333\n",
      "Test RMSE:  1.9859281304091139\n",
      "Epoch: 0462 Training RMSE= 2.239299996\n",
      "Validation RMSE:  2.469658575785204\n",
      "Test RMSE:  1.9838169244295238\n",
      "Epoch: 0463 Training RMSE= 2.237183770\n",
      "Validation RMSE:  2.462711457792477\n",
      "Test RMSE:  1.9815688078304203\n",
      "Epoch: 0464 Training RMSE= 2.236504837\n",
      "Validation RMSE:  2.4667816524081227\n",
      "Test RMSE:  1.9856710331725622\n",
      "Epoch: 0465 Training RMSE= 2.236455245\n",
      "Validation RMSE:  2.4755917400465157\n",
      "Test RMSE:  1.9912781229326117\n",
      "Epoch: 0466 Training RMSE= 2.235320957\n",
      "Validation RMSE:  2.4632701898938114\n",
      "Test RMSE:  1.9847076317449819\n",
      "Epoch: 0467 Training RMSE= 2.232820023\n",
      "Validation RMSE:  2.480551674725766\n",
      "Test RMSE:  1.9998409237779446\n",
      "Epoch: 0468 Training RMSE= 2.235737456\n",
      "Validation RMSE:  2.465590761282988\n",
      "Test RMSE:  1.982037647012951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0469 Training RMSE= 2.234780108\n",
      "Validation RMSE:  2.483760541827288\n",
      "Test RMSE:  2.004218138577719\n",
      "Epoch: 0470 Training RMSE= 2.231681545\n",
      "Validation RMSE:  2.4645115648161995\n",
      "Test RMSE:  1.985845306441497\n",
      "Epoch: 0471 Training RMSE= 2.230329343\n",
      "Validation RMSE:  2.4686950725770633\n",
      "Test RMSE:  1.9828707633089564\n",
      "Epoch: 0472 Training RMSE= 2.232476686\n",
      "Validation RMSE:  2.4595600208838695\n",
      "Test RMSE:  1.9801157499446114\n",
      "Epoch: 0473 Training RMSE= 2.230425766\n",
      "Validation RMSE:  2.48155534628661\n",
      "Test RMSE:  1.9939833531397446\n",
      "Epoch: 0474 Training RMSE= 2.229589002\n",
      "Validation RMSE:  2.484458945913403\n",
      "Test RMSE:  2.0122195759275536\n",
      "Epoch: 0475 Training RMSE= 2.229103272\n",
      "Validation RMSE:  2.48067320934688\n",
      "Test RMSE:  1.9937939618086093\n",
      "Epoch: 0476 Training RMSE= 2.228603397\n",
      "Validation RMSE:  2.469670522468074\n",
      "Test RMSE:  1.9845258099777474\n",
      "Epoch: 0477 Training RMSE= 2.228042833\n",
      "Validation RMSE:  2.4623241090647805\n",
      "Test RMSE:  1.9818253394779903\n",
      "Epoch: 0478 Training RMSE= 2.227470170\n",
      "Validation RMSE:  2.4885413065404833\n",
      "Test RMSE:  2.009406262476417\n",
      "Epoch: 0479 Training RMSE= 2.225133982\n",
      "Validation RMSE:  2.464533645782535\n",
      "Test RMSE:  1.9827823705766396\n",
      "Epoch: 0480 Training RMSE= 2.226302403\n",
      "Validation RMSE:  2.471663602718616\n",
      "Test RMSE:  1.9877739879401917\n",
      "Epoch: 0481 Training RMSE= 2.224791760\n",
      "Validation RMSE:  2.474358718329019\n",
      "Test RMSE:  1.991588263052469\n",
      "Epoch: 0482 Training RMSE= 2.222447884\n",
      "Validation RMSE:  2.479743767468319\n",
      "Test RMSE:  1.9947916998668167\n",
      "Epoch: 0483 Training RMSE= 2.225056848\n",
      "Validation RMSE:  2.4666385796167867\n",
      "Test RMSE:  1.9915279418590293\n",
      "Epoch: 0484 Training RMSE= 2.222849261\n",
      "Validation RMSE:  2.458834836744205\n",
      "Test RMSE:  1.9766916828384455\n",
      "Epoch: 0485 Training RMSE= 2.222630272\n",
      "Validation RMSE:  2.471847546566062\n",
      "Test RMSE:  1.9854309246302473\n",
      "Epoch: 0486 Training RMSE= 2.220319883\n",
      "Validation RMSE:  2.455090895250076\n",
      "Test RMSE:  1.9766969597356994\n",
      "Epoch: 0487 Training RMSE= 2.222996675\n",
      "Validation RMSE:  2.4652030187264917\n",
      "Test RMSE:  1.9798060254487477\n",
      "Epoch: 0488 Training RMSE= 2.220650453\n",
      "Validation RMSE:  2.473772902650142\n",
      "Test RMSE:  1.9977046789118051\n",
      "Epoch: 0489 Training RMSE= 2.218349380\n",
      "Validation RMSE:  2.4558654348214612\n",
      "Test RMSE:  1.9736534242152999\n",
      "Epoch: 0490 Training RMSE= 2.218440120\n",
      "Validation RMSE:  2.4646800813578995\n",
      "Test RMSE:  1.980595360796269\n",
      "Epoch: 0491 Training RMSE= 2.217665547\n",
      "Validation RMSE:  2.4641182824825387\n",
      "Test RMSE:  1.9829442280329281\n",
      "Epoch: 0492 Training RMSE= 2.217417158\n",
      "Validation RMSE:  2.4834358071513085\n",
      "Test RMSE:  1.9928611065346111\n",
      "Epoch: 0493 Training RMSE= 2.218024654\n",
      "Validation RMSE:  2.4619823841267356\n",
      "Test RMSE:  1.986152183520029\n",
      "Epoch: 0494 Training RMSE= 2.216513471\n",
      "Validation RMSE:  2.464868391249341\n",
      "Test RMSE:  1.985051017413101\n",
      "Epoch: 0495 Training RMSE= 2.214736842\n",
      "Validation RMSE:  2.4588554415180695\n",
      "Test RMSE:  1.9799169640758074\n",
      "Epoch: 0496 Training RMSE= 2.215106457\n",
      "Validation RMSE:  2.4648850040036425\n",
      "Test RMSE:  1.9759293096114094\n",
      "Epoch: 0497 Training RMSE= 2.214083083\n",
      "Validation RMSE:  2.4636336800702363\n",
      "Test RMSE:  1.990283355259924\n",
      "Epoch: 0498 Training RMSE= 2.214779581\n",
      "Validation RMSE:  2.4640702184001086\n",
      "Test RMSE:  1.979396582516806\n",
      "Epoch: 0499 Training RMSE= 2.213376970\n",
      "Validation RMSE:  2.4566370612974033\n",
      "Test RMSE:  1.9765367468562212\n",
      "Epoch: 0500 Training RMSE= 2.213126823\n",
      "Validation RMSE:  2.4665873265608864\n",
      "Test RMSE:  1.9874285085283905\n",
      "Epoch: 0501 Training RMSE= 2.212560499\n",
      "Validation RMSE:  2.4763357016508913\n",
      "Test RMSE:  1.9959465315963696\n",
      "Epoch: 0502 Training RMSE= 2.211227678\n",
      "Validation RMSE:  2.471609077665862\n",
      "Test RMSE:  1.9939653130865078\n",
      "Epoch: 0503 Training RMSE= 2.210988201\n",
      "Validation RMSE:  2.4706819676635496\n",
      "Test RMSE:  1.9918318636735617\n",
      "Epoch: 0504 Training RMSE= 2.210357485\n",
      "Validation RMSE:  2.452159254140632\n",
      "Test RMSE:  1.9745238122409816\n",
      "Epoch: 0505 Training RMSE= 2.209322606\n",
      "Validation RMSE:  2.4711474837021727\n",
      "Test RMSE:  1.9893835792991694\n",
      "Epoch: 0506 Training RMSE= 2.211873186\n",
      "Validation RMSE:  2.4486011170502886\n",
      "Test RMSE:  1.9722900201743536\n",
      "Epoch: 0507 Training RMSE= 2.208155377\n",
      "Validation RMSE:  2.480400841451698\n",
      "Test RMSE:  1.9991030618960248\n",
      "Epoch: 0508 Training RMSE= 2.206874249\n",
      "Validation RMSE:  2.4587704032900826\n",
      "Test RMSE:  1.9793084712843487\n",
      "Epoch: 0509 Training RMSE= 2.207145126\n",
      "Validation RMSE:  2.463171946637895\n",
      "Test RMSE:  1.9815590921206332\n",
      "Epoch: 0510 Training RMSE= 2.206328148\n",
      "Validation RMSE:  2.4591964133348787\n",
      "Test RMSE:  1.9825181909474605\n",
      "Epoch: 0511 Training RMSE= 2.207406519\n",
      "Validation RMSE:  2.4487833125763108\n",
      "Test RMSE:  1.9716907820217051\n",
      "Epoch: 0512 Training RMSE= 2.204284958\n",
      "Validation RMSE:  2.4595562646319724\n",
      "Test RMSE:  1.9778501214174022\n",
      "Epoch: 0513 Training RMSE= 2.203258335\n",
      "Validation RMSE:  2.4664406658933635\n",
      "Test RMSE:  1.9919134063656783\n",
      "Epoch: 0514 Training RMSE= 2.205432844\n",
      "Validation RMSE:  2.4751052660664237\n",
      "Test RMSE:  1.9950311747722875\n",
      "Epoch: 0515 Training RMSE= 2.202509652\n",
      "Validation RMSE:  2.4583229102436377\n",
      "Test RMSE:  1.9797095624963392\n",
      "Epoch: 0516 Training RMSE= 2.203996264\n",
      "Validation RMSE:  2.4670344077526236\n",
      "Test RMSE:  1.9830783752913979\n",
      "Epoch: 0517 Training RMSE= 2.202214812\n",
      "Validation RMSE:  2.464298653519268\n",
      "Test RMSE:  1.9776298294900543\n",
      "Epoch: 0518 Training RMSE= 2.200848752\n",
      "Validation RMSE:  2.472269422697278\n",
      "Test RMSE:  2.0038478314843133\n",
      "Epoch: 0519 Training RMSE= 2.202826814\n",
      "Validation RMSE:  2.4491953384764993\n",
      "Test RMSE:  1.9708038848736076\n",
      "Epoch: 0520 Training RMSE= 2.199672057\n",
      "Validation RMSE:  2.4619638391809433\n",
      "Test RMSE:  1.9761888957605482\n",
      "Epoch: 0521 Training RMSE= 2.198358211\n",
      "Validation RMSE:  2.471063109698482\n",
      "Test RMSE:  1.9887180560535829\n",
      "Epoch: 0522 Training RMSE= 2.200846404\n",
      "Validation RMSE:  2.4532033336791534\n",
      "Test RMSE:  1.973539641841125\n",
      "Epoch: 0523 Training RMSE= 2.198752245\n",
      "Validation RMSE:  2.4601642665447745\n",
      "Test RMSE:  1.9779792201606337\n",
      "Epoch: 0524 Training RMSE= 2.198230547\n",
      "Validation RMSE:  2.44904327980824\n",
      "Test RMSE:  1.9796514988059257\n",
      "Epoch: 0525 Training RMSE= 2.197909517\n",
      "Validation RMSE:  2.4500128813813675\n",
      "Test RMSE:  1.9703281677252382\n",
      "Epoch: 0526 Training RMSE= 2.197368639\n",
      "Validation RMSE:  2.4500381826811934\n",
      "Test RMSE:  1.9710771491175287\n",
      "Epoch: 0527 Training RMSE= 2.197148708\n",
      "Validation RMSE:  2.470208835537947\n",
      "Test RMSE:  1.9855591252211922\n",
      "Epoch: 0528 Training RMSE= 2.194989834\n",
      "Validation RMSE:  2.4593487409767567\n",
      "Test RMSE:  1.9794234276578737\n",
      "Epoch: 0529 Training RMSE= 2.195961836\n",
      "Validation RMSE:  2.4587610459964404\n",
      "Test RMSE:  1.976011448215032\n",
      "Epoch: 0530 Training RMSE= 2.195523599\n",
      "Validation RMSE:  2.4843290792575767\n",
      "Test RMSE:  2.0105872073666724\n",
      "Epoch: 0531 Training RMSE= 2.192618782\n",
      "Validation RMSE:  2.446271421936146\n",
      "Test RMSE:  1.972116196234517\n",
      "Epoch: 0532 Training RMSE= 2.193973736\n",
      "Validation RMSE:  2.443469538290175\n",
      "Test RMSE:  1.9634712385368966\n",
      "Epoch: 0533 Training RMSE= 2.191169916\n",
      "Validation RMSE:  2.4564111409218397\n",
      "Test RMSE:  1.9802729342413246\n",
      "Epoch: 0534 Training RMSE= 2.192101106\n",
      "Validation RMSE:  2.456754635521648\n",
      "Test RMSE:  1.9779472174761805\n",
      "Epoch: 0535 Training RMSE= 2.192018670\n",
      "Validation RMSE:  2.468735175752194\n",
      "Test RMSE:  1.98834432265812\n",
      "Epoch: 0536 Training RMSE= 2.190132799\n",
      "Validation RMSE:  2.4486511887013487\n",
      "Test RMSE:  1.9772205840433257\n",
      "Epoch: 0537 Training RMSE= 2.190658104\n",
      "Validation RMSE:  2.469969605131434\n",
      "Test RMSE:  1.983272817099836\n",
      "Epoch: 0538 Training RMSE= 2.190202292\n",
      "Validation RMSE:  2.4614813067547585\n",
      "Test RMSE:  1.978042319981953\n",
      "Epoch: 0539 Training RMSE= 2.189076515\n",
      "Validation RMSE:  2.45190921955019\n",
      "Test RMSE:  1.9765661940709751\n",
      "Epoch: 0540 Training RMSE= 2.189943499\n",
      "Validation RMSE:  2.472175587584547\n",
      "Test RMSE:  1.9890687653232377\n",
      "Epoch: 0541 Training RMSE= 2.187405560\n",
      "Validation RMSE:  2.4587208043795568\n",
      "Test RMSE:  1.9767234646236869\n",
      "Epoch: 0542 Training RMSE= 2.189820405\n",
      "Validation RMSE:  2.480183622538066\n",
      "Test RMSE:  1.9891627068460869\n",
      "Epoch: 0543 Training RMSE= 2.186510789\n",
      "Validation RMSE:  2.443493638896995\n",
      "Test RMSE:  1.963997190204444\n",
      "Epoch: 0544 Training RMSE= 2.187542073\n",
      "Validation RMSE:  2.458286686363118\n",
      "Test RMSE:  1.9793565173862602\n",
      "Epoch: 0545 Training RMSE= 2.187469368\n",
      "Validation RMSE:  2.4424094374470284\n",
      "Test RMSE:  1.9644652318666733\n",
      "Epoch: 0546 Training RMSE= 2.185241893\n",
      "Validation RMSE:  2.4661493002609705\n",
      "Test RMSE:  1.9791150404236741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0547 Training RMSE= 2.185657074\n",
      "Validation RMSE:  2.437771708676899\n",
      "Test RMSE:  1.9613727983113871\n",
      "Epoch: 0548 Training RMSE= 2.184418942\n",
      "Validation RMSE:  2.4740798732951763\n",
      "Test RMSE:  1.9892174216183536\n",
      "Epoch: 0549 Training RMSE= 2.182372517\n",
      "Validation RMSE:  2.4560647346184905\n",
      "Test RMSE:  1.9772136354535361\n",
      "Epoch: 0550 Training RMSE= 2.184174378\n",
      "Validation RMSE:  2.4477646936087227\n",
      "Test RMSE:  1.9714591595915625\n",
      "Epoch: 0551 Training RMSE= 2.183009596\n",
      "Validation RMSE:  2.4742284660762888\n",
      "Test RMSE:  1.9924086055351886\n",
      "Epoch: 0552 Training RMSE= 2.183077399\n",
      "Validation RMSE:  2.4564989783985474\n",
      "Test RMSE:  1.9731632449842205\n",
      "Epoch: 0553 Training RMSE= 2.183490583\n",
      "Validation RMSE:  2.445456454881243\n",
      "Test RMSE:  1.9669404114791385\n",
      "Epoch: 0554 Training RMSE= 2.182010903\n",
      "Validation RMSE:  2.44850291814118\n",
      "Test RMSE:  1.9719624725140803\n",
      "Epoch: 0555 Training RMSE= 2.181460113\n",
      "Validation RMSE:  2.450083943310802\n",
      "Test RMSE:  1.9756349490578033\n",
      "Epoch: 0556 Training RMSE= 2.178682217\n",
      "Validation RMSE:  2.4604555899448046\n",
      "Test RMSE:  1.9829617521538112\n",
      "Epoch: 0557 Training RMSE= 2.181523068\n",
      "Validation RMSE:  2.4678931137430347\n",
      "Test RMSE:  1.9951717539162575\n",
      "Epoch: 0558 Training RMSE= 2.178704426\n",
      "Validation RMSE:  2.453860178809173\n",
      "Test RMSE:  1.9772517242781043\n",
      "Epoch: 0559 Training RMSE= 2.178063168\n",
      "Validation RMSE:  2.4832445607757063\n",
      "Test RMSE:  2.0076460268346845\n",
      "Epoch: 0560 Training RMSE= 2.179377314\n",
      "Validation RMSE:  2.448016393266902\n",
      "Test RMSE:  1.9749057445488354\n",
      "Epoch: 0561 Training RMSE= 2.178415333\n",
      "Validation RMSE:  2.460505056987264\n",
      "Test RMSE:  1.9838168643386507\n",
      "Epoch: 0562 Training RMSE= 2.177456647\n",
      "Validation RMSE:  2.446279852387228\n",
      "Test RMSE:  1.9729836069558067\n",
      "Epoch: 0563 Training RMSE= 2.177640302\n",
      "Validation RMSE:  2.465008955314417\n",
      "Test RMSE:  1.9845724983755169\n",
      "Epoch: 0564 Training RMSE= 2.175047858\n",
      "Validation RMSE:  2.4574143394605716\n",
      "Test RMSE:  1.9799824555900944\n",
      "Epoch: 0565 Training RMSE= 2.177778747\n",
      "Validation RMSE:  2.450432824808741\n",
      "Test RMSE:  1.971890382056839\n",
      "Epoch: 0566 Training RMSE= 2.172658211\n",
      "Validation RMSE:  2.4530768906073224\n",
      "Test RMSE:  1.9760152790544512\n",
      "Epoch: 0567 Training RMSE= 2.174905441\n",
      "Validation RMSE:  2.471164319565109\n",
      "Test RMSE:  1.988795590528501\n",
      "Epoch: 0568 Training RMSE= 2.173634323\n",
      "Validation RMSE:  2.439301876269099\n",
      "Test RMSE:  1.962006144738564\n",
      "Epoch: 0569 Training RMSE= 2.172625699\n",
      "Validation RMSE:  2.448893621301761\n",
      "Test RMSE:  1.966203253372833\n",
      "Epoch: 0570 Training RMSE= 2.174560141\n",
      "Validation RMSE:  2.451479244338044\n",
      "Test RMSE:  1.9730239223367898\n",
      "Epoch: 0571 Training RMSE= 2.172798035\n",
      "Validation RMSE:  2.464863796726931\n",
      "Test RMSE:  1.994430012866199\n",
      "Epoch: 0572 Training RMSE= 2.172039594\n",
      "Validation RMSE:  2.47276961562391\n",
      "Test RMSE:  1.991495393693612\n",
      "Epoch: 0573 Training RMSE= 2.172503236\n",
      "Validation RMSE:  2.451850535756665\n",
      "Test RMSE:  1.9832018290393947\n",
      "Epoch: 0574 Training RMSE= 2.170464419\n",
      "Validation RMSE:  2.451467889792156\n",
      "Test RMSE:  1.9759845718020872\n",
      "Epoch: 0575 Training RMSE= 2.169863328\n",
      "Validation RMSE:  2.4532826852839817\n",
      "Test RMSE:  1.9675236621534125\n",
      "Epoch: 0576 Training RMSE= 2.171956756\n",
      "Validation RMSE:  2.4581495690493997\n",
      "Test RMSE:  1.9755567773843916\n",
      "Epoch: 0577 Training RMSE= 2.169241207\n",
      "Validation RMSE:  2.4611562238103017\n",
      "Test RMSE:  1.981738599464114\n",
      "Epoch: 0578 Training RMSE= 2.169931198\n",
      "Validation RMSE:  2.4624850538933\n",
      "Test RMSE:  1.9862617474083142\n",
      "Epoch: 0579 Training RMSE= 2.168781531\n",
      "Validation RMSE:  2.447994309404715\n",
      "Test RMSE:  1.969133702988463\n",
      "Epoch: 0580 Training RMSE= 2.168154410\n",
      "Validation RMSE:  2.442845864616498\n",
      "Test RMSE:  1.9661161121538828\n",
      "Epoch: 0581 Training RMSE= 2.167503802\n",
      "Validation RMSE:  2.462381163503558\n",
      "Test RMSE:  1.9812746082546626\n",
      "Epoch: 0582 Training RMSE= 2.167814807\n",
      "Validation RMSE:  2.4488992437007977\n",
      "Test RMSE:  1.9700778993947639\n",
      "Epoch: 0583 Training RMSE= 2.168763645\n",
      "Validation RMSE:  2.459489620499366\n",
      "Test RMSE:  1.976850089158341\n",
      "Epoch: 0584 Training RMSE= 2.167206897\n",
      "Validation RMSE:  2.441533614841837\n",
      "Test RMSE:  1.9671902305820412\n",
      "Epoch: 0585 Training RMSE= 2.165734226\n",
      "Validation RMSE:  2.456848331792858\n",
      "Test RMSE:  1.9762040367306837\n",
      "Epoch: 0586 Training RMSE= 2.165001940\n",
      "Validation RMSE:  2.4521151365315226\n",
      "Test RMSE:  1.967489898927151\n",
      "Epoch: 0587 Training RMSE= 2.165800340\n",
      "Validation RMSE:  2.448435754051146\n",
      "Test RMSE:  1.9791014878097848\n",
      "Epoch: 0588 Training RMSE= 2.163217262\n",
      "Validation RMSE:  2.4517352060980437\n",
      "Test RMSE:  1.9710155197869905\n",
      "Epoch: 0589 Training RMSE= 2.164999740\n",
      "Validation RMSE:  2.4590368288004\n",
      "Test RMSE:  1.974655497962195\n",
      "Epoch: 0590 Training RMSE= 2.162153599\n",
      "Validation RMSE:  2.4519855015203156\n",
      "Test RMSE:  1.9659927375927087\n",
      "Epoch: 0591 Training RMSE= 2.164125333\n",
      "Validation RMSE:  2.4625127443477597\n",
      "Test RMSE:  1.9800114150633394\n",
      "Epoch: 0592 Training RMSE= 2.162206279\n",
      "Validation RMSE:  2.461107980787393\n",
      "Test RMSE:  1.9743290823934119\n",
      "Epoch: 0593 Training RMSE= 2.161768083\n",
      "Validation RMSE:  2.4513615629806225\n",
      "Test RMSE:  1.9715635842710166\n",
      "Epoch: 0594 Training RMSE= 2.161728026\n",
      "Validation RMSE:  2.4440519843870905\n",
      "Test RMSE:  1.9638480513222245\n",
      "Epoch: 0595 Training RMSE= 2.161510770\n",
      "Validation RMSE:  2.4472723714944045\n",
      "Test RMSE:  1.9628986567242785\n",
      "Epoch: 0596 Training RMSE= 2.160709687\n",
      "Validation RMSE:  2.4637288082301043\n",
      "Test RMSE:  1.9876389730797317\n",
      "Epoch: 0597 Training RMSE= 2.160419295\n",
      "Validation RMSE:  2.4541299790218356\n",
      "Test RMSE:  1.9735702663290204\n",
      "Epoch: 0598 Training RMSE= 2.161233109\n",
      "Validation RMSE:  2.45627644260141\n",
      "Test RMSE:  1.9755235285949693\n",
      "Epoch: 0599 Training RMSE= 2.157699847\n",
      "Validation RMSE:  2.4355551467914114\n",
      "Test RMSE:  1.9612111968267607\n",
      "Epoch: 0600 Training RMSE= 2.159851784\n",
      "Validation RMSE:  2.455334464129456\n",
      "Test RMSE:  1.9707179905431877\n",
      "Epoch: 0601 Training RMSE= 2.157659324\n",
      "Validation RMSE:  2.451116505031678\n",
      "Test RMSE:  1.9690052048714632\n",
      "Epoch: 0602 Training RMSE= 2.157986605\n",
      "Validation RMSE:  2.45193909572242\n",
      "Test RMSE:  1.9707797955209956\n",
      "Epoch: 0603 Training RMSE= 2.156869531\n",
      "Validation RMSE:  2.440844588713408\n",
      "Test RMSE:  1.9591557284230599\n",
      "Epoch: 0604 Training RMSE= 2.156483632\n",
      "Validation RMSE:  2.457963242124837\n",
      "Test RMSE:  1.9783111195440595\n",
      "Epoch: 0605 Training RMSE= 2.159054654\n",
      "Validation RMSE:  2.452003028049435\n",
      "Test RMSE:  1.9750430935371348\n",
      "Epoch: 0606 Training RMSE= 2.154383260\n",
      "Validation RMSE:  2.4572103704954946\n",
      "Test RMSE:  1.9727843139172168\n",
      "Epoch: 0607 Training RMSE= 2.155975012\n",
      "Validation RMSE:  2.4389744725003877\n",
      "Test RMSE:  1.9607890707430449\n",
      "Epoch: 0608 Training RMSE= 2.155008351\n",
      "Validation RMSE:  2.4532736229065812\n",
      "Test RMSE:  1.968945478392591\n",
      "Epoch: 0609 Training RMSE= 2.154306777\n",
      "Validation RMSE:  2.453882137011299\n",
      "Test RMSE:  1.9764519008407015\n",
      "Epoch: 0610 Training RMSE= 2.156189724\n",
      "Validation RMSE:  2.4434489013468323\n",
      "Test RMSE:  1.9626789794995427\n",
      "Epoch: 0611 Training RMSE= 2.153946625\n",
      "Validation RMSE:  2.468202240450839\n",
      "Test RMSE:  1.9855360554293962\n",
      "Epoch: 0612 Training RMSE= 2.151975921\n",
      "Validation RMSE:  2.4521304987863113\n",
      "Test RMSE:  1.977383092687049\n",
      "Epoch: 0613 Training RMSE= 2.154073712\n",
      "Validation RMSE:  2.468827862054947\n",
      "Test RMSE:  1.978520490333472\n",
      "Epoch: 0614 Training RMSE= 2.152758480\n",
      "Validation RMSE:  2.4483080179123156\n",
      "Test RMSE:  1.9733499194937736\n",
      "Epoch: 0615 Training RMSE= 2.154922989\n",
      "Validation RMSE:  2.448615941481485\n",
      "Test RMSE:  1.9728346791161961\n",
      "Epoch: 0616 Training RMSE= 2.150866990\n",
      "Validation RMSE:  2.4432077581871403\n",
      "Test RMSE:  1.9635121286802992\n",
      "Epoch: 0617 Training RMSE= 2.150641456\n",
      "Validation RMSE:  2.4512003011688988\n",
      "Test RMSE:  1.9681244113566299\n",
      "Epoch: 0618 Training RMSE= 2.152758143\n",
      "Validation RMSE:  2.450095790819906\n",
      "Test RMSE:  1.9710867804163283\n",
      "Epoch: 0619 Training RMSE= 2.149058648\n",
      "Validation RMSE:  2.450196188253381\n",
      "Test RMSE:  1.9702770577857314\n",
      "Epoch: 0620 Training RMSE= 2.150181970\n",
      "Validation RMSE:  2.449452609589364\n",
      "Test RMSE:  1.9705813836262445\n",
      "Epoch: 0621 Training RMSE= 2.151211519\n",
      "Validation RMSE:  2.4482947497209\n",
      "Test RMSE:  1.9657851252507417\n",
      "Epoch: 0622 Training RMSE= 2.147681288\n",
      "Validation RMSE:  2.474005573557849\n",
      "Test RMSE:  2.0008317886636795\n",
      "Epoch: 0623 Training RMSE= 2.149230503\n",
      "Validation RMSE:  2.456554057202973\n",
      "Test RMSE:  1.9733454944852318\n",
      "Epoch: 0624 Training RMSE= 2.148248746\n",
      "Validation RMSE:  2.452236573501342\n",
      "Test RMSE:  1.9678624741168274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0625 Training RMSE= 2.148998795\n",
      "Validation RMSE:  2.4398470118510307\n",
      "Test RMSE:  1.9623710893703428\n",
      "Epoch: 0626 Training RMSE= 2.148545130\n",
      "Validation RMSE:  2.470373754447658\n",
      "Test RMSE:  1.9876659617918817\n",
      "Epoch: 0627 Training RMSE= 2.148208426\n",
      "Validation RMSE:  2.4614831470902088\n",
      "Test RMSE:  1.9859273800715875\n",
      "Epoch: 0628 Training RMSE= 2.145620118\n",
      "Validation RMSE:  2.4600676679608533\n",
      "Test RMSE:  1.970628614122904\n",
      "Epoch: 0629 Training RMSE= 2.147698723\n",
      "Validation RMSE:  2.4420257514795543\n",
      "Test RMSE:  1.9658030752230724\n",
      "Epoch: 0630 Training RMSE= 2.146322106\n",
      "Validation RMSE:  2.4449061339444995\n",
      "Test RMSE:  1.9627751558416908\n",
      "Epoch: 0631 Training RMSE= 2.144514073\n",
      "Validation RMSE:  2.4583882040161003\n",
      "Test RMSE:  1.9829558757291617\n",
      "Epoch: 0632 Training RMSE= 2.145131059\n",
      "Validation RMSE:  2.4638279727653063\n",
      "Test RMSE:  1.982457383312306\n",
      "Epoch: 0633 Training RMSE= 2.144612231\n",
      "Validation RMSE:  2.4638222876656006\n",
      "Test RMSE:  1.9797942990080781\n",
      "Epoch: 0634 Training RMSE= 2.143920684\n",
      "Validation RMSE:  2.4449171289000833\n",
      "Test RMSE:  1.9642060080990358\n",
      "Epoch: 0635 Training RMSE= 2.145120684\n",
      "Validation RMSE:  2.4421062473050226\n",
      "Test RMSE:  1.9643106818160552\n",
      "Epoch: 0636 Training RMSE= 2.144030837\n",
      "Validation RMSE:  2.454011768985934\n",
      "Test RMSE:  1.9669158505590782\n",
      "Epoch: 0637 Training RMSE= 2.142287229\n",
      "Validation RMSE:  2.466525270516241\n",
      "Test RMSE:  1.9755428684761362\n",
      "Epoch: 0638 Training RMSE= 2.142512209\n",
      "Validation RMSE:  2.449612867170061\n",
      "Test RMSE:  1.9674164478975606\n",
      "Epoch: 0639 Training RMSE= 2.142124497\n",
      "Validation RMSE:  2.436741613126086\n",
      "Test RMSE:  1.9574098354099514\n",
      "Epoch: 0640 Training RMSE= 2.142596235\n",
      "Validation RMSE:  2.4672283601669007\n",
      "Test RMSE:  1.9784419054492375\n",
      "Epoch: 0641 Training RMSE= 2.140792158\n",
      "Validation RMSE:  2.4403045121505635\n",
      "Test RMSE:  1.969225704910599\n",
      "Epoch: 0642 Training RMSE= 2.140867192\n",
      "Validation RMSE:  2.4608218211279245\n",
      "Test RMSE:  1.9844100979081516\n",
      "Epoch: 0643 Training RMSE= 2.141437065\n",
      "Validation RMSE:  2.4478501141747118\n",
      "Test RMSE:  1.9649414735916055\n",
      "Epoch: 0644 Training RMSE= 2.140297487\n",
      "Validation RMSE:  2.4852534486481734\n",
      "Test RMSE:  2.0044829670845377\n",
      "Epoch: 0645 Training RMSE= 2.140554079\n",
      "Validation RMSE:  2.4386559215331176\n",
      "Test RMSE:  1.9592585880431561\n",
      "Epoch: 0646 Training RMSE= 2.139656193\n",
      "Validation RMSE:  2.4636975266642493\n",
      "Test RMSE:  1.9742230981764557\n",
      "Epoch: 0647 Training RMSE= 2.138886195\n",
      "Validation RMSE:  2.441997560228071\n",
      "Test RMSE:  1.963162576995281\n",
      "Epoch: 0648 Training RMSE= 2.139162031\n",
      "Validation RMSE:  2.4624408066117223\n",
      "Test RMSE:  1.985025524532609\n",
      "Epoch: 0649 Training RMSE= 2.138764025\n",
      "Validation RMSE:  2.456820431930337\n",
      "Test RMSE:  1.9734761866313588\n",
      "Epoch: 0650 Training RMSE= 2.137660409\n",
      "Validation RMSE:  2.476904740136962\n",
      "Test RMSE:  1.9962824155627603\n",
      "Epoch: 0651 Training RMSE= 2.138754979\n",
      "Validation RMSE:  2.452609232726299\n",
      "Test RMSE:  1.9692877685927137\n",
      "Epoch: 0652 Training RMSE= 2.136034472\n",
      "Validation RMSE:  2.4587523674383824\n",
      "Test RMSE:  1.97139209994695\n",
      "Epoch: 0653 Training RMSE= 2.137521312\n",
      "Validation RMSE:  2.4465944628711247\n",
      "Test RMSE:  1.9675545923349134\n",
      "Epoch: 0654 Training RMSE= 2.136855966\n",
      "Validation RMSE:  2.459096674214462\n",
      "Test RMSE:  1.9795065203215072\n",
      "Epoch: 0655 Training RMSE= 2.136042059\n",
      "Validation RMSE:  2.4384842132704994\n",
      "Test RMSE:  1.9572863079469367\n",
      "Epoch: 0656 Training RMSE= 2.136773971\n",
      "Validation RMSE:  2.449381261663276\n",
      "Test RMSE:  1.9647994595762994\n",
      "Epoch: 0657 Training RMSE= 2.134890752\n",
      "Validation RMSE:  2.4441195859730573\n",
      "Test RMSE:  1.9646839964216352\n",
      "Epoch: 0658 Training RMSE= 2.134021874\n",
      "Validation RMSE:  2.4599036088527537\n",
      "Test RMSE:  1.9787562563827998\n",
      "Epoch: 0659 Training RMSE= 2.134298163\n",
      "Validation RMSE:  2.4447320121136933\n",
      "Test RMSE:  1.9659666034446466\n",
      "Epoch: 0660 Training RMSE= 2.133036748\n",
      "Validation RMSE:  2.451388576700865\n",
      "Test RMSE:  1.9670522878171655\n",
      "Epoch: 0661 Training RMSE= 2.135719886\n",
      "Validation RMSE:  2.456734352792552\n",
      "Test RMSE:  1.9715059457970319\n",
      "Epoch: 0662 Training RMSE= 2.132675421\n",
      "Validation RMSE:  2.460805156721707\n",
      "Test RMSE:  1.9786895795984907\n",
      "Epoch: 0663 Training RMSE= 2.132434403\n",
      "Validation RMSE:  2.445642613562727\n",
      "Test RMSE:  1.9650750905654526\n",
      "Epoch: 0664 Training RMSE= 2.133105270\n",
      "Validation RMSE:  2.4497984667041126\n",
      "Test RMSE:  1.9626727842088956\n",
      "Epoch: 0665 Training RMSE= 2.131999401\n",
      "Validation RMSE:  2.459094589708523\n",
      "Test RMSE:  1.9752330757181802\n",
      "Epoch: 0666 Training RMSE= 2.131676365\n",
      "Validation RMSE:  2.4524578961132546\n",
      "Test RMSE:  1.9717466164462316\n",
      "Epoch: 0667 Training RMSE= 2.132521663\n",
      "Validation RMSE:  2.4425609085937965\n",
      "Test RMSE:  1.9630014872202823\n",
      "Epoch: 0668 Training RMSE= 2.130075259\n",
      "Validation RMSE:  2.445526552429371\n",
      "Test RMSE:  1.9647908137264376\n",
      "Epoch: 0669 Training RMSE= 2.129964717\n",
      "Validation RMSE:  2.4519789624641852\n",
      "Test RMSE:  1.967503046811373\n",
      "Epoch: 0670 Training RMSE= 2.131753165\n",
      "Validation RMSE:  2.438991481570625\n",
      "Test RMSE:  1.9601386521402036\n",
      "Epoch: 0671 Training RMSE= 2.129361252\n",
      "Validation RMSE:  2.448409194636612\n",
      "Test RMSE:  1.9685884136918177\n",
      "Epoch: 0672 Training RMSE= 2.131244253\n",
      "Validation RMSE:  2.449732822330348\n",
      "Test RMSE:  1.9691948161115647\n",
      "Epoch: 0673 Training RMSE= 2.128199194\n",
      "Validation RMSE:  2.4411702522655534\n",
      "Test RMSE:  1.9605685491099187\n",
      "Epoch: 0674 Training RMSE= 2.129693567\n",
      "Validation RMSE:  2.4684621425798645\n",
      "Test RMSE:  1.9852111286382912\n",
      "Epoch: 0675 Training RMSE= 2.128177318\n",
      "Validation RMSE:  2.440365745070051\n",
      "Test RMSE:  1.9610557368526278\n",
      "Epoch: 0676 Training RMSE= 2.128711083\n",
      "Validation RMSE:  2.4578691033443594\n",
      "Test RMSE:  1.9803957351979207\n",
      "Epoch: 0677 Training RMSE= 2.127524733\n",
      "Validation RMSE:  2.4583453377468314\n",
      "Test RMSE:  1.9727797516791126\n",
      "Epoch: 0678 Training RMSE= 2.127547698\n",
      "Validation RMSE:  2.4633987955248857\n",
      "Test RMSE:  1.9732698902464958\n",
      "Epoch: 0679 Training RMSE= 2.127845872\n",
      "Validation RMSE:  2.4401284253608515\n",
      "Test RMSE:  1.9569756657973887\n",
      "Epoch: 0680 Training RMSE= 2.126550160\n",
      "Validation RMSE:  2.455960232869374\n",
      "Test RMSE:  1.9707294685410184\n",
      "Epoch: 0681 Training RMSE= 2.125841429\n",
      "Validation RMSE:  2.428381185383046\n",
      "Test RMSE:  1.9496259092081911\n",
      "Epoch: 0682 Training RMSE= 2.127437817\n",
      "Validation RMSE:  2.4595481704954816\n",
      "Test RMSE:  1.974192121512586\n",
      "Epoch: 0683 Training RMSE= 2.124480925\n",
      "Validation RMSE:  2.4402580063458648\n",
      "Test RMSE:  1.9604883629488297\n",
      "Epoch: 0684 Training RMSE= 2.125563109\n",
      "Validation RMSE:  2.455788715135432\n",
      "Test RMSE:  1.9739781096045692\n",
      "Epoch: 0685 Training RMSE= 2.125544778\n",
      "Validation RMSE:  2.446945522651325\n",
      "Test RMSE:  1.9647093436833623\n",
      "Epoch: 0686 Training RMSE= 2.123506899\n",
      "Validation RMSE:  2.4464029434102814\n",
      "Test RMSE:  1.9638516934322827\n",
      "Epoch: 0687 Training RMSE= 2.124317135\n",
      "Validation RMSE:  2.461875155609428\n",
      "Test RMSE:  1.981358871717766\n",
      "Epoch: 0688 Training RMSE= 2.123414101\n",
      "Validation RMSE:  2.4480077740103257\n",
      "Test RMSE:  1.963608932017528\n",
      "Epoch: 0689 Training RMSE= 2.125359098\n",
      "Validation RMSE:  2.450096958538441\n",
      "Test RMSE:  1.9641265621981456\n",
      "Epoch: 0690 Training RMSE= 2.120553219\n",
      "Validation RMSE:  2.442878071974967\n",
      "Test RMSE:  1.9626880901855577\n",
      "Epoch: 0691 Training RMSE= 2.122263729\n",
      "Validation RMSE:  2.450774919807545\n",
      "Test RMSE:  1.9665270016938392\n",
      "Epoch: 0692 Training RMSE= 2.122439693\n",
      "Validation RMSE:  2.4559451372654255\n",
      "Test RMSE:  1.9685217861329376\n",
      "Epoch: 0693 Training RMSE= 2.122489877\n",
      "Validation RMSE:  2.4425472187409425\n",
      "Test RMSE:  1.964491128089311\n",
      "Epoch: 0694 Training RMSE= 2.122865873\n",
      "Validation RMSE:  2.4592414218636254\n",
      "Test RMSE:  1.9812591751078725\n",
      "Epoch: 0695 Training RMSE= 2.121959048\n",
      "Validation RMSE:  2.4472641880186448\n",
      "Test RMSE:  1.9693545215671489\n",
      "Epoch: 0696 Training RMSE= 2.119902468\n",
      "Validation RMSE:  2.462352914822778\n",
      "Test RMSE:  1.9766543069257931\n",
      "Epoch: 0697 Training RMSE= 2.121846423\n",
      "Validation RMSE:  2.4351593908510583\n",
      "Test RMSE:  1.9547399786743382\n",
      "Epoch: 0698 Training RMSE= 2.118449645\n",
      "Validation RMSE:  2.454218675313827\n",
      "Test RMSE:  1.9709767963489349\n",
      "Epoch: 0699 Training RMSE= 2.121021558\n",
      "Validation RMSE:  2.4604331816771214\n",
      "Test RMSE:  1.9805703823639733\n",
      "Epoch: 0700 Training RMSE= 2.120189737\n",
      "Validation RMSE:  2.4592940640734144\n",
      "Test RMSE:  1.9746320140231328\n",
      "Epoch: 0701 Training RMSE= 2.120090012\n",
      "Validation RMSE:  2.450556363947129\n",
      "Test RMSE:  1.9633137868362662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0702 Training RMSE= 2.119818032\n",
      "Validation RMSE:  2.4360120829187024\n",
      "Test RMSE:  1.9552503237502719\n",
      "Epoch: 0703 Training RMSE= 2.117812407\n",
      "Validation RMSE:  2.4694850648195508\n",
      "Test RMSE:  1.9894976089810223\n",
      "Epoch: 0704 Training RMSE= 2.117901418\n",
      "Validation RMSE:  2.437635833672342\n",
      "Test RMSE:  1.9581927052861183\n",
      "Epoch: 0705 Training RMSE= 2.118815494\n",
      "Validation RMSE:  2.4527139258530886\n",
      "Test RMSE:  1.9774860742322178\n",
      "Epoch: 0706 Training RMSE= 2.116596258\n",
      "Validation RMSE:  2.458579711098192\n",
      "Test RMSE:  1.9726943966038228\n",
      "Epoch: 0707 Training RMSE= 2.115545682\n",
      "Validation RMSE:  2.4606636500502437\n",
      "Test RMSE:  1.981393240916101\n",
      "Epoch: 0708 Training RMSE= 2.117217528\n",
      "Validation RMSE:  2.4612095515936416\n",
      "Test RMSE:  1.9773868304397353\n",
      "Epoch: 0709 Training RMSE= 2.118248553\n",
      "Validation RMSE:  2.448599534798637\n",
      "Test RMSE:  1.9627496621298386\n",
      "Epoch: 0710 Training RMSE= 2.115710277\n",
      "Validation RMSE:  2.449657930121954\n",
      "Test RMSE:  1.9656067839780624\n",
      "Epoch: 0711 Training RMSE= 2.115340144\n",
      "Validation RMSE:  2.4420042724603244\n",
      "Test RMSE:  1.9660885244610957\n",
      "Epoch: 0712 Training RMSE= 2.115797658\n",
      "Validation RMSE:  2.443320172648702\n",
      "Test RMSE:  1.961487742274615\n",
      "Epoch: 0713 Training RMSE= 2.115619450\n",
      "Validation RMSE:  2.446887986500699\n",
      "Test RMSE:  1.9611756381542884\n",
      "Epoch: 0714 Training RMSE= 2.115685288\n",
      "Validation RMSE:  2.4660632083724647\n",
      "Test RMSE:  1.9941540455156415\n",
      "Epoch: 0715 Training RMSE= 2.114087158\n",
      "Validation RMSE:  2.4448302405854414\n",
      "Test RMSE:  1.9614934703038518\n",
      "Epoch: 0716 Training RMSE= 2.114453291\n",
      "Validation RMSE:  2.4445959144200486\n",
      "Test RMSE:  1.9645547521567837\n",
      "Epoch: 0717 Training RMSE= 2.115596201\n",
      "Validation RMSE:  2.433458425515894\n",
      "Test RMSE:  1.9539455216162616\n",
      "Epoch: 0718 Training RMSE= 2.112698021\n",
      "Validation RMSE:  2.4541471259262635\n",
      "Test RMSE:  1.9666128517589865\n",
      "Epoch: 0719 Training RMSE= 2.114061473\n",
      "Validation RMSE:  2.4371310468353045\n",
      "Test RMSE:  1.958427068482589\n",
      "Epoch: 0720 Training RMSE= 2.112026684\n",
      "Validation RMSE:  2.44938449815922\n",
      "Test RMSE:  1.9593356151364176\n",
      "Epoch: 0721 Training RMSE= 2.113732515\n",
      "Validation RMSE:  2.432197329361507\n",
      "Test RMSE:  1.9511857743479377\n",
      "Epoch: 0722 Training RMSE= 2.112185261\n",
      "Validation RMSE:  2.4507260589659894\n",
      "Test RMSE:  1.9675036981443672\n",
      "Epoch: 0723 Training RMSE= 2.111002059\n",
      "Validation RMSE:  2.454255857827675\n",
      "Test RMSE:  1.9745889845643319\n",
      "Epoch: 0724 Training RMSE= 2.113440578\n",
      "Validation RMSE:  2.4436718979474548\n",
      "Test RMSE:  1.9589940657523346\n",
      "Epoch: 0725 Training RMSE= 2.111429545\n",
      "Validation RMSE:  2.4523710076165286\n",
      "Test RMSE:  1.9728413258970439\n",
      "Epoch: 0726 Training RMSE= 2.110516594\n",
      "Validation RMSE:  2.427778174572216\n",
      "Test RMSE:  1.9469778140919543\n",
      "Epoch: 0727 Training RMSE= 2.111047054\n",
      "Validation RMSE:  2.449119285985305\n",
      "Test RMSE:  1.9625098926756503\n",
      "Epoch: 0728 Training RMSE= 2.109653277\n",
      "Validation RMSE:  2.4647048209766607\n",
      "Test RMSE:  1.9820887995188716\n",
      "Epoch: 0729 Training RMSE= 2.110085364\n",
      "Validation RMSE:  2.4448206348975754\n",
      "Test RMSE:  1.9599145295522518\n",
      "Epoch: 0730 Training RMSE= 2.108388707\n",
      "Validation RMSE:  2.455933754827457\n",
      "Test RMSE:  1.9706701421482262\n",
      "Epoch: 0731 Training RMSE= 2.110102535\n",
      "Validation RMSE:  2.4441480697755003\n",
      "Test RMSE:  1.9573907731325277\n",
      "Epoch: 0732 Training RMSE= 2.108959112\n",
      "Validation RMSE:  2.4363366064939624\n",
      "Test RMSE:  1.9552792075116516\n",
      "Epoch: 0733 Training RMSE= 2.108389060\n",
      "Validation RMSE:  2.4456464399279763\n",
      "Test RMSE:  1.9655582956418742\n",
      "Epoch: 0734 Training RMSE= 2.108840635\n",
      "Validation RMSE:  2.4435077625227075\n",
      "Test RMSE:  1.9616282336986806\n",
      "Epoch: 0735 Training RMSE= 2.108262451\n",
      "Validation RMSE:  2.4539101188602452\n",
      "Test RMSE:  1.9664038347813684\n",
      "Epoch: 0736 Training RMSE= 2.108047901\n",
      "Validation RMSE:  2.442156598560085\n",
      "Test RMSE:  1.961814091719189\n",
      "Epoch: 0737 Training RMSE= 2.108109472\n",
      "Validation RMSE:  2.436179977397157\n",
      "Test RMSE:  1.9531511228721794\n",
      "Epoch: 0738 Training RMSE= 2.106070871\n",
      "Validation RMSE:  2.4442039390095225\n",
      "Test RMSE:  1.9595680318169149\n",
      "Epoch: 0739 Training RMSE= 2.109356997\n",
      "Validation RMSE:  2.4505346434984765\n",
      "Test RMSE:  1.9678881136133746\n",
      "Epoch: 0740 Training RMSE= 2.104927664\n",
      "Validation RMSE:  2.456044082163671\n",
      "Test RMSE:  1.968903656670157\n",
      "Epoch: 0741 Training RMSE= 2.107026091\n",
      "Validation RMSE:  2.4408958450844014\n",
      "Test RMSE:  1.956780849739481\n",
      "Epoch: 0742 Training RMSE= 2.104327216\n",
      "Validation RMSE:  2.4505960342808506\n",
      "Test RMSE:  1.9745018964697008\n",
      "Epoch: 0743 Training RMSE= 2.105882698\n",
      "Validation RMSE:  2.4346032413928955\n",
      "Test RMSE:  1.9552160284927083\n",
      "Epoch: 0744 Training RMSE= 2.104379514\n",
      "Validation RMSE:  2.4503627216654396\n",
      "Test RMSE:  1.9652396953998292\n",
      "Epoch: 0745 Training RMSE= 2.105379647\n",
      "Validation RMSE:  2.4485948854070974\n",
      "Test RMSE:  1.9649347849208645\n",
      "Epoch: 0746 Training RMSE= 2.104092289\n",
      "Validation RMSE:  2.4427922091422767\n",
      "Test RMSE:  1.9624335824454788\n",
      "Epoch: 0747 Training RMSE= 2.103855343\n",
      "Validation RMSE:  2.451960949583697\n",
      "Test RMSE:  1.9722667800618878\n",
      "Epoch: 0748 Training RMSE= 2.105621395\n",
      "Validation RMSE:  2.439837606411936\n",
      "Test RMSE:  1.9579406270132393\n",
      "Epoch: 0749 Training RMSE= 2.103474441\n",
      "Validation RMSE:  2.457926843193919\n",
      "Test RMSE:  1.977033385626344\n",
      "Epoch: 0750 Training RMSE= 2.103423226\n",
      "Validation RMSE:  2.457349917271326\n",
      "Test RMSE:  1.9690409248597611\n",
      "Epoch: 0751 Training RMSE= 2.103822841\n",
      "Validation RMSE:  2.4310140637454265\n",
      "Test RMSE:  1.9520920425215331\n",
      "Epoch: 0752 Training RMSE= 2.102553268\n",
      "Validation RMSE:  2.4406038232097083\n",
      "Test RMSE:  1.955490251789458\n",
      "Epoch: 0753 Training RMSE= 2.104727368\n",
      "Validation RMSE:  2.4482377321973448\n",
      "Test RMSE:  1.9620189951989155\n",
      "Epoch: 0754 Training RMSE= 2.102294545\n",
      "Validation RMSE:  2.444866639461213\n",
      "Test RMSE:  1.9629980560813913\n",
      "Epoch: 0755 Training RMSE= 2.099849105\n",
      "Validation RMSE:  2.4422297195231364\n",
      "Test RMSE:  1.9564606678053464\n",
      "Epoch: 0756 Training RMSE= 2.102070040\n",
      "Validation RMSE:  2.4472910034130293\n",
      "Test RMSE:  1.9606289867862812\n",
      "Epoch: 0757 Training RMSE= 2.102942610\n",
      "Validation RMSE:  2.453305304637553\n",
      "Test RMSE:  1.963620284620555\n",
      "Epoch: 0758 Training RMSE= 2.099923631\n",
      "Validation RMSE:  2.437517386154378\n",
      "Test RMSE:  1.9578922836962176\n",
      "Epoch: 0759 Training RMSE= 2.100663880\n",
      "Validation RMSE:  2.4431583556647514\n",
      "Test RMSE:  1.9583307394727478\n",
      "Epoch: 0760 Training RMSE= 2.100589158\n",
      "Validation RMSE:  2.450940343856864\n",
      "Test RMSE:  1.9646079071654716\n",
      "Epoch: 0761 Training RMSE= 2.100168028\n",
      "Validation RMSE:  2.4338772334622\n",
      "Test RMSE:  1.9519848814666092\n",
      "Epoch: 0762 Training RMSE= 2.100592583\n",
      "Validation RMSE:  2.434099172457197\n",
      "Test RMSE:  1.953143295202399\n",
      "Epoch: 0763 Training RMSE= 2.098764362\n",
      "Validation RMSE:  2.467783412754683\n",
      "Test RMSE:  1.9814932315873495\n",
      "Epoch: 0764 Training RMSE= 2.101175592\n",
      "Validation RMSE:  2.4306594278453075\n",
      "Test RMSE:  1.9508139555988633\n",
      "Epoch: 0765 Training RMSE= 2.096659614\n",
      "Validation RMSE:  2.4820826048756826\n",
      "Test RMSE:  1.9990689673594206\n",
      "Epoch: 0766 Training RMSE= 2.099043585\n",
      "Validation RMSE:  2.450758722145213\n",
      "Test RMSE:  1.9721014923505165\n",
      "Epoch: 0767 Training RMSE= 2.098497264\n",
      "Validation RMSE:  2.4503526754920557\n",
      "Test RMSE:  1.9630335513787853\n",
      "Epoch: 0768 Training RMSE= 2.097958608\n",
      "Validation RMSE:  2.449795668703315\n",
      "Test RMSE:  1.9668987895543901\n",
      "Epoch: 0769 Training RMSE= 2.098832654\n",
      "Validation RMSE:  2.4416161042621014\n",
      "Test RMSE:  1.9599424626459003\n",
      "Epoch: 0770 Training RMSE= 2.097491190\n",
      "Validation RMSE:  2.4429032275785425\n",
      "Test RMSE:  1.962385243504821\n",
      "Epoch: 0771 Training RMSE= 2.097397093\n",
      "Validation RMSE:  2.438797874195861\n",
      "Test RMSE:  1.9531537778641614\n",
      "Epoch: 0772 Training RMSE= 2.097605398\n",
      "Validation RMSE:  2.449271023571201\n",
      "Test RMSE:  1.9719916404407656\n",
      "Epoch: 0773 Training RMSE= 2.096081989\n",
      "Validation RMSE:  2.435606441060015\n",
      "Test RMSE:  1.9553528858444547\n",
      "Epoch: 0774 Training RMSE= 2.096290698\n",
      "Validation RMSE:  2.447561040707441\n",
      "Test RMSE:  1.9689122239140215\n",
      "Epoch: 0775 Training RMSE= 2.095709479\n",
      "Validation RMSE:  2.437019692073337\n",
      "Test RMSE:  1.9600060671109087\n",
      "Epoch: 0776 Training RMSE= 2.096269421\n",
      "Validation RMSE:  2.449710048333801\n",
      "Test RMSE:  1.9634738643957452\n",
      "Epoch: 0777 Training RMSE= 2.095128798\n",
      "Validation RMSE:  2.441791497924723\n",
      "Test RMSE:  1.9701495721113742\n",
      "Epoch: 0778 Training RMSE= 2.094171282\n",
      "Validation RMSE:  2.452645321739217\n",
      "Test RMSE:  1.977275448406623\n",
      "Epoch: 0779 Training RMSE= 2.095977481\n",
      "Validation RMSE:  2.4472462622142155\n",
      "Test RMSE:  1.9575053876069468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0780 Training RMSE= 2.095901622\n",
      "Validation RMSE:  2.4384906418546284\n",
      "Test RMSE:  1.9546141631276148\n",
      "Epoch: 0781 Training RMSE= 2.092973115\n",
      "Validation RMSE:  2.4305968959259627\n",
      "Test RMSE:  1.952083538860146\n",
      "Epoch: 0782 Training RMSE= 2.094949487\n",
      "Validation RMSE:  2.4402657492330384\n",
      "Test RMSE:  1.958204865469122\n",
      "Epoch: 0783 Training RMSE= 2.093390366\n",
      "Validation RMSE:  2.4451322641463205\n",
      "Test RMSE:  1.9666368709209086\n",
      "Epoch: 0784 Training RMSE= 2.093907749\n",
      "Validation RMSE:  2.429447297123825\n",
      "Test RMSE:  1.9487964577853687\n",
      "Epoch: 0785 Training RMSE= 2.092802277\n",
      "Validation RMSE:  2.44575284460441\n",
      "Test RMSE:  1.9656738438928323\n",
      "Epoch: 0786 Training RMSE= 2.092594617\n",
      "Validation RMSE:  2.451916244980628\n",
      "Test RMSE:  1.9648624214513801\n",
      "Epoch: 0787 Training RMSE= 2.093305529\n",
      "Validation RMSE:  2.439523957005275\n",
      "Test RMSE:  1.956164931620112\n",
      "Epoch: 0788 Training RMSE= 2.091435198\n",
      "Validation RMSE:  2.435971808054296\n",
      "Test RMSE:  1.960647652774558\n",
      "Epoch: 0789 Training RMSE= 2.091857661\n",
      "Validation RMSE:  2.448340299486915\n",
      "Test RMSE:  1.970656365103546\n",
      "Epoch: 0790 Training RMSE= 2.091839850\n",
      "Validation RMSE:  2.4439977700758657\n",
      "Test RMSE:  1.958158765952832\n",
      "Epoch: 0791 Training RMSE= 2.090653239\n",
      "Validation RMSE:  2.45680500195726\n",
      "Test RMSE:  1.9727794495437663\n",
      "Epoch: 0792 Training RMSE= 2.093453745\n",
      "Validation RMSE:  2.438400493469589\n",
      "Test RMSE:  1.9627600783022645\n",
      "Epoch: 0793 Training RMSE= 2.090399760\n",
      "Validation RMSE:  2.4588912449320905\n",
      "Test RMSE:  1.9725741227682836\n",
      "Epoch: 0794 Training RMSE= 2.091527670\n",
      "Validation RMSE:  2.4433592286327146\n",
      "Test RMSE:  1.966412367449381\n",
      "Epoch: 0795 Training RMSE= 2.088783523\n",
      "Validation RMSE:  2.4528948682003606\n",
      "Test RMSE:  1.973655190923738\n",
      "Epoch: 0796 Training RMSE= 2.091183724\n",
      "Validation RMSE:  2.4439857710644195\n",
      "Test RMSE:  1.9603627186999637\n",
      "Epoch: 0797 Training RMSE= 2.089173166\n",
      "Validation RMSE:  2.4496066624330304\n",
      "Test RMSE:  1.9683608169989462\n",
      "Epoch: 0798 Training RMSE= 2.089239520\n",
      "Validation RMSE:  2.454688891842045\n",
      "Test RMSE:  1.9682298003296053\n",
      "Epoch: 0799 Training RMSE= 2.089759551\n",
      "Validation RMSE:  2.4276691406520583\n",
      "Test RMSE:  1.9495672552732983\n",
      "Epoch: 0800 Training RMSE= 2.088694730\n",
      "Validation RMSE:  2.472628649034773\n",
      "Test RMSE:  1.988399224818947\n",
      "Epoch: 0801 Training RMSE= 2.089124895\n",
      "Validation RMSE:  2.4311499655965347\n",
      "Test RMSE:  1.951734643067799\n",
      "Epoch: 0802 Training RMSE= 2.087275901\n",
      "Validation RMSE:  2.4296555103023714\n",
      "Test RMSE:  1.9491514603308553\n",
      "Epoch: 0803 Training RMSE= 2.089485828\n",
      "Validation RMSE:  2.4551074527758034\n",
      "Test RMSE:  1.9711916481530072\n",
      "Epoch: 0804 Training RMSE= 2.087072008\n",
      "Validation RMSE:  2.444592500908619\n",
      "Test RMSE:  1.9654984947456957\n",
      "Epoch: 0805 Training RMSE= 2.087998610\n",
      "Validation RMSE:  2.439828469664955\n",
      "Test RMSE:  1.9582295660839697\n",
      "Epoch: 0806 Training RMSE= 2.088223979\n",
      "Validation RMSE:  2.4405525851282874\n",
      "Test RMSE:  1.9628148913600465\n",
      "Epoch: 0807 Training RMSE= 2.087009537\n",
      "Validation RMSE:  2.433323889598636\n",
      "Test RMSE:  1.9548581939985674\n",
      "Epoch: 0808 Training RMSE= 2.086588517\n",
      "Validation RMSE:  2.464332273588396\n",
      "Test RMSE:  1.9809766641028979\n",
      "Epoch: 0809 Training RMSE= 2.085909191\n",
      "Validation RMSE:  2.4355228181215294\n",
      "Test RMSE:  1.954037506563496\n",
      "Epoch: 0810 Training RMSE= 2.085891306\n",
      "Validation RMSE:  2.4344134228554744\n",
      "Test RMSE:  1.9585038239415196\n",
      "Epoch: 0811 Training RMSE= 2.085597358\n",
      "Validation RMSE:  2.4543092626865874\n",
      "Test RMSE:  1.967137312002704\n",
      "Epoch: 0812 Training RMSE= 2.086134629\n",
      "Validation RMSE:  2.4439143854838346\n",
      "Test RMSE:  1.962125577865431\n",
      "Epoch: 0813 Training RMSE= 2.085583215\n",
      "Validation RMSE:  2.438805670598955\n",
      "Test RMSE:  1.9569754982809673\n",
      "Epoch: 0814 Training RMSE= 2.085903985\n",
      "Validation RMSE:  2.4349615377173124\n",
      "Test RMSE:  1.9536288717725667\n",
      "Epoch: 0815 Training RMSE= 2.084778173\n",
      "Validation RMSE:  2.4454261095266907\n",
      "Test RMSE:  1.9652914974226512\n",
      "Epoch: 0816 Training RMSE= 2.084651728\n",
      "Validation RMSE:  2.433565926431033\n",
      "Test RMSE:  1.955527803609124\n",
      "Epoch: 0817 Training RMSE= 2.084678756\n",
      "Validation RMSE:  2.436893289814887\n",
      "Test RMSE:  1.9568841235503414\n",
      "Epoch: 0818 Training RMSE= 2.083758046\n",
      "Validation RMSE:  2.4380286939386\n",
      "Test RMSE:  1.9624972428466478\n",
      "Epoch: 0819 Training RMSE= 2.083587838\n",
      "Validation RMSE:  2.4574031336137474\n",
      "Test RMSE:  1.9703815904887156\n",
      "Epoch: 0820 Training RMSE= 2.084138402\n",
      "Validation RMSE:  2.4350417285531734\n",
      "Test RMSE:  1.9579605363194537\n",
      "Epoch: 0821 Training RMSE= 2.082426676\n",
      "Validation RMSE:  2.440115014984758\n",
      "Test RMSE:  1.9596720560055656\n",
      "Epoch: 0822 Training RMSE= 2.082884030\n",
      "Validation RMSE:  2.438418215419098\n",
      "Test RMSE:  1.9612687275699305\n",
      "Epoch: 0823 Training RMSE= 2.082435033\n",
      "Validation RMSE:  2.4574040310535787\n",
      "Test RMSE:  1.9831801143601404\n",
      "Epoch: 0824 Training RMSE= 2.084727215\n",
      "Validation RMSE:  2.4367331252143405\n",
      "Test RMSE:  1.9568576088506884\n",
      "Epoch: 0825 Training RMSE= 2.081890192\n",
      "Validation RMSE:  2.4258892876835327\n",
      "Test RMSE:  1.9479890357733132\n",
      "Epoch: 0826 Training RMSE= 2.081709144\n",
      "Validation RMSE:  2.471148400270729\n",
      "Test RMSE:  1.9900564127263152\n",
      "Epoch: 0827 Training RMSE= 2.081769160\n",
      "Validation RMSE:  2.4419239685444456\n",
      "Test RMSE:  1.9639952327161527\n",
      "Epoch: 0828 Training RMSE= 2.081723486\n",
      "Validation RMSE:  2.433333895848701\n",
      "Test RMSE:  1.9611454431685356\n",
      "Epoch: 0829 Training RMSE= 2.081084033\n",
      "Validation RMSE:  2.4337238013724107\n",
      "Test RMSE:  1.957121491017229\n",
      "Epoch: 0830 Training RMSE= 2.080116250\n",
      "Validation RMSE:  2.4607702047008213\n",
      "Test RMSE:  1.977060157337223\n",
      "Epoch: 0831 Training RMSE= 2.080957576\n",
      "Validation RMSE:  2.4399997169072347\n",
      "Test RMSE:  1.9577388131537272\n",
      "Epoch: 0832 Training RMSE= 2.081405122\n",
      "Validation RMSE:  2.4219693565446154\n",
      "Test RMSE:  1.9483047972444791\n",
      "Epoch: 0833 Training RMSE= 2.079272308\n",
      "Validation RMSE:  2.433151572332495\n",
      "Test RMSE:  1.9527786252921473\n",
      "Epoch: 0834 Training RMSE= 2.080316498\n",
      "Validation RMSE:  2.445885149413916\n",
      "Test RMSE:  1.9660371527202798\n",
      "Epoch: 0835 Training RMSE= 2.079303069\n",
      "Validation RMSE:  2.4499351269054506\n",
      "Test RMSE:  1.9743340184232385\n",
      "Epoch: 0836 Training RMSE= 2.079874089\n",
      "Validation RMSE:  2.4283883893333154\n",
      "Test RMSE:  1.9535563641079656\n",
      "Epoch: 0837 Training RMSE= 2.078494000\n",
      "Validation RMSE:  2.445732446265374\n",
      "Test RMSE:  1.9588281297225383\n",
      "Epoch: 0838 Training RMSE= 2.080130274\n",
      "Validation RMSE:  2.43479309409265\n",
      "Test RMSE:  1.9559189153697372\n",
      "Epoch: 0839 Training RMSE= 2.078457586\n",
      "Validation RMSE:  2.4541008824424546\n",
      "Test RMSE:  1.972091745111077\n",
      "Epoch: 0840 Training RMSE= 2.079328381\n",
      "Validation RMSE:  2.4390458560353725\n",
      "Test RMSE:  1.958157944096211\n",
      "Epoch: 0841 Training RMSE= 2.078066195\n",
      "Validation RMSE:  2.4272958324111666\n",
      "Test RMSE:  1.949693182502704\n",
      "Epoch: 0842 Training RMSE= 2.077406525\n",
      "Validation RMSE:  2.4523301993412208\n",
      "Test RMSE:  1.9790444453464138\n",
      "Epoch: 0843 Training RMSE= 2.078714689\n",
      "Validation RMSE:  2.429214382411843\n",
      "Test RMSE:  1.9580710536418429\n",
      "Epoch: 0844 Training RMSE= 2.076809457\n",
      "Validation RMSE:  2.4628907460054617\n",
      "Test RMSE:  1.9795793872628786\n",
      "Epoch: 0845 Training RMSE= 2.077252743\n",
      "Validation RMSE:  2.4305339945487754\n",
      "Test RMSE:  1.9516629048915783\n",
      "Epoch: 0846 Training RMSE= 2.076887766\n",
      "Validation RMSE:  2.447918195250555\n",
      "Test RMSE:  1.9721543532843868\n",
      "Epoch: 0847 Training RMSE= 2.075794979\n",
      "Validation RMSE:  2.4399019291195523\n",
      "Test RMSE:  1.9619710106034993\n",
      "Epoch: 0848 Training RMSE= 2.076883843\n",
      "Validation RMSE:  2.4525029310934143\n",
      "Test RMSE:  1.9708235130172926\n",
      "Epoch: 0849 Training RMSE= 2.076531512\n",
      "Validation RMSE:  2.4349834949944498\n",
      "Test RMSE:  1.9557090754859034\n",
      "Epoch: 0850 Training RMSE= 2.076419641\n",
      "Validation RMSE:  2.424810931120511\n",
      "Test RMSE:  1.9485510865656113\n",
      "Epoch: 0851 Training RMSE= 2.075328972\n",
      "Validation RMSE:  2.439937644224261\n",
      "Test RMSE:  1.9601386825485851\n",
      "Epoch: 0852 Training RMSE= 2.075038629\n",
      "Validation RMSE:  2.455590145400637\n",
      "Test RMSE:  1.969786477111511\n",
      "Epoch: 0853 Training RMSE= 2.074369683\n",
      "Validation RMSE:  2.4319086991805854\n",
      "Test RMSE:  1.9549243877989133\n",
      "Epoch: 0854 Training RMSE= 2.074287224\n",
      "Validation RMSE:  2.4340641308033493\n",
      "Test RMSE:  1.9584747290462403\n",
      "Epoch: 0855 Training RMSE= 2.075449757\n",
      "Validation RMSE:  2.4473261966500814\n",
      "Test RMSE:  1.9654413759129847\n",
      "Epoch: 0856 Training RMSE= 2.076637800\n",
      "Validation RMSE:  2.4544769497906325\n",
      "Test RMSE:  1.9708785857018425\n",
      "Epoch: 0857 Training RMSE= 2.073378893\n",
      "Validation RMSE:  2.4232578021582802\n",
      "Test RMSE:  1.952017065515924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0858 Training RMSE= 2.074743198\n",
      "Validation RMSE:  2.456264018244453\n",
      "Test RMSE:  1.988025071703662\n",
      "Epoch: 0859 Training RMSE= 2.071848072\n",
      "Validation RMSE:  2.437985860868078\n",
      "Test RMSE:  1.9656359855696626\n",
      "Epoch: 0860 Training RMSE= 2.074609284\n",
      "Validation RMSE:  2.454192275629889\n",
      "Test RMSE:  1.9711033062243963\n",
      "Epoch: 0861 Training RMSE= 2.073220680\n",
      "Validation RMSE:  2.4455990608179214\n",
      "Test RMSE:  1.9684394712630529\n",
      "Epoch: 0862 Training RMSE= 2.073462506\n",
      "Validation RMSE:  2.431562957136099\n",
      "Test RMSE:  1.9577313235077318\n",
      "Epoch: 0863 Training RMSE= 2.072680010\n",
      "Validation RMSE:  2.4388743463018456\n",
      "Test RMSE:  1.9634601431061147\n",
      "Epoch: 0864 Training RMSE= 2.072248163\n",
      "Validation RMSE:  2.4392858589924726\n",
      "Test RMSE:  1.9568997641610497\n",
      "Epoch: 0865 Training RMSE= 2.071963329\n",
      "Validation RMSE:  2.449623646342515\n",
      "Test RMSE:  1.9713745485677403\n",
      "Epoch: 0866 Training RMSE= 2.072207949\n",
      "Validation RMSE:  2.432437897778434\n",
      "Test RMSE:  1.9576278355877943\n",
      "Epoch: 0867 Training RMSE= 2.072625854\n",
      "Validation RMSE:  2.4241592709728037\n",
      "Test RMSE:  1.9522135477051834\n",
      "Epoch: 0868 Training RMSE= 2.070792221\n",
      "Validation RMSE:  2.4345378728205676\n",
      "Test RMSE:  1.960918289637601\n",
      "Epoch: 0869 Training RMSE= 2.071133456\n",
      "Validation RMSE:  2.44365362870032\n",
      "Test RMSE:  1.9641886656182932\n",
      "Epoch: 0870 Training RMSE= 2.069639360\n",
      "Validation RMSE:  2.433317104431363\n",
      "Test RMSE:  1.9536879376727294\n",
      "Epoch: 0871 Training RMSE= 2.072241821\n",
      "Validation RMSE:  2.4393651135513323\n",
      "Test RMSE:  1.9622299524858569\n",
      "Epoch: 0872 Training RMSE= 2.071305203\n",
      "Validation RMSE:  2.42608150522082\n",
      "Test RMSE:  1.9518116464389157\n",
      "Epoch: 0873 Training RMSE= 2.069820389\n",
      "Validation RMSE:  2.456475975959589\n",
      "Test RMSE:  1.9784959375996756\n",
      "Epoch: 0874 Training RMSE= 2.071116355\n",
      "Validation RMSE:  2.424370396868591\n",
      "Test RMSE:  1.948818234441314\n",
      "Epoch: 0875 Training RMSE= 2.067978472\n",
      "Validation RMSE:  2.4440495943978098\n",
      "Test RMSE:  1.964466809619353\n",
      "Epoch: 0876 Training RMSE= 2.069686052\n",
      "Validation RMSE:  2.4264112382403087\n",
      "Test RMSE:  1.95354541068586\n",
      "Epoch: 0877 Training RMSE= 2.070677737\n",
      "Validation RMSE:  2.4504019816187292\n",
      "Test RMSE:  1.9675404450938199\n",
      "Epoch: 0878 Training RMSE= 2.069333836\n",
      "Validation RMSE:  2.4291638855784634\n",
      "Test RMSE:  1.9556931815326826\n",
      "Epoch: 0879 Training RMSE= 2.068205790\n",
      "Validation RMSE:  2.4321011149379603\n",
      "Test RMSE:  1.955877500784911\n",
      "Epoch: 0880 Training RMSE= 2.068822778\n",
      "Validation RMSE:  2.456167484673746\n",
      "Test RMSE:  1.9723221297621492\n",
      "Epoch: 0881 Training RMSE= 2.068902470\n",
      "Validation RMSE:  2.4223230132139513\n",
      "Test RMSE:  1.951057117589031\n",
      "Epoch: 0882 Training RMSE= 2.067243321\n",
      "Validation RMSE:  2.4432883614421175\n",
      "Test RMSE:  1.9670085622594333\n",
      "Epoch: 0883 Training RMSE= 2.070069137\n",
      "Validation RMSE:  2.437622996430424\n",
      "Test RMSE:  1.9638553810618349\n",
      "Epoch: 0884 Training RMSE= 2.066648553\n",
      "Validation RMSE:  2.4637482834049864\n",
      "Test RMSE:  1.9936140749526332\n",
      "Epoch: 0885 Training RMSE= 2.068336673\n",
      "Validation RMSE:  2.451906594124652\n",
      "Test RMSE:  1.9714379051359734\n",
      "Epoch: 0886 Training RMSE= 2.066997468\n",
      "Validation RMSE:  2.4292169096796146\n",
      "Test RMSE:  1.9513396533699285\n",
      "Epoch: 0887 Training RMSE= 2.067167218\n",
      "Validation RMSE:  2.4510608176798954\n",
      "Test RMSE:  1.9678976090775353\n",
      "Epoch: 0888 Training RMSE= 2.068295887\n",
      "Validation RMSE:  2.434695048221976\n",
      "Test RMSE:  1.9591189002630467\n",
      "Epoch: 0889 Training RMSE= 2.066026776\n",
      "Validation RMSE:  2.433574523359982\n",
      "Test RMSE:  1.9612725872054468\n",
      "Epoch: 0890 Training RMSE= 2.066509335\n",
      "Validation RMSE:  2.4550457377941566\n",
      "Test RMSE:  1.9798075006571614\n",
      "Epoch: 0891 Training RMSE= 2.066149035\n",
      "Validation RMSE:  2.4322315524485134\n",
      "Test RMSE:  1.955625124084332\n",
      "Epoch: 0892 Training RMSE= 2.066813595\n",
      "Validation RMSE:  2.4329628409775172\n",
      "Test RMSE:  1.9613447032502092\n",
      "Epoch: 0893 Training RMSE= 2.065557087\n",
      "Validation RMSE:  2.4503948058997715\n",
      "Test RMSE:  1.9790294315208283\n",
      "Epoch: 0894 Training RMSE= 2.066491869\n",
      "Validation RMSE:  2.4222766789228873\n",
      "Test RMSE:  1.9514501944022957\n",
      "Epoch: 0895 Training RMSE= 2.065844565\n",
      "Validation RMSE:  2.438149145691783\n",
      "Test RMSE:  1.9624813734980784\n",
      "Epoch: 0896 Training RMSE= 2.065742374\n",
      "Validation RMSE:  2.429554055526598\n",
      "Test RMSE:  1.9579688774670807\n",
      "Epoch: 0897 Training RMSE= 2.064965514\n",
      "Validation RMSE:  2.4627326109956424\n",
      "Test RMSE:  1.9793911923660723\n",
      "Epoch: 0898 Training RMSE= 2.063863909\n",
      "Validation RMSE:  2.4299910745142843\n",
      "Test RMSE:  1.9540165811692216\n",
      "Epoch: 0899 Training RMSE= 2.065481711\n",
      "Validation RMSE:  2.4431771165427523\n",
      "Test RMSE:  1.9695435090460869\n",
      "Epoch: 0900 Training RMSE= 2.065497606\n",
      "Validation RMSE:  2.4279259431957567\n",
      "Test RMSE:  1.9560293652118586\n",
      "Epoch: 0901 Training RMSE= 2.063296439\n",
      "Validation RMSE:  2.4529389960847965\n",
      "Test RMSE:  1.9740096482289453\n",
      "Epoch: 0902 Training RMSE= 2.064207298\n",
      "Validation RMSE:  2.4240901169132583\n",
      "Test RMSE:  1.9517824517844657\n",
      "Epoch: 0903 Training RMSE= 2.063842585\n",
      "Validation RMSE:  2.4347178402609986\n",
      "Test RMSE:  1.962190797465531\n",
      "Epoch: 0904 Training RMSE= 2.062325136\n",
      "Validation RMSE:  2.451073560224858\n",
      "Test RMSE:  1.9700020033443741\n",
      "Epoch: 0905 Training RMSE= 2.063864167\n",
      "Validation RMSE:  2.4273893889184337\n",
      "Test RMSE:  1.9564252360531424\n",
      "Epoch: 0906 Training RMSE= 2.063742435\n",
      "Validation RMSE:  2.4284783080524153\n",
      "Test RMSE:  1.9547992396872869\n",
      "Epoch: 0907 Training RMSE= 2.061541054\n",
      "Validation RMSE:  2.4553344398538854\n",
      "Test RMSE:  1.9860557887087016\n",
      "Epoch: 0908 Training RMSE= 2.062372617\n",
      "Validation RMSE:  2.453335431057441\n",
      "Test RMSE:  1.9835846596111766\n",
      "Epoch: 0909 Training RMSE= 2.062480308\n",
      "Validation RMSE:  2.448162623825577\n",
      "Test RMSE:  1.9715284088278275\n",
      "Epoch: 0910 Training RMSE= 2.062164580\n",
      "Validation RMSE:  2.4193721143685125\n",
      "Test RMSE:  1.9492033994142859\n",
      "Epoch: 0911 Training RMSE= 2.062335217\n",
      "Validation RMSE:  2.4372986563641486\n",
      "Test RMSE:  1.961555474769953\n",
      "Epoch: 0912 Training RMSE= 2.061605056\n",
      "Validation RMSE:  2.4325864247046556\n",
      "Test RMSE:  1.9597796331083304\n",
      "Epoch: 0913 Training RMSE= 2.060349858\n",
      "Validation RMSE:  2.423007884704287\n",
      "Test RMSE:  1.9515713730450779\n",
      "Epoch: 0914 Training RMSE= 2.060975711\n",
      "Validation RMSE:  2.4383809625098336\n",
      "Test RMSE:  1.9630112795969668\n",
      "Epoch: 0915 Training RMSE= 2.062433115\n",
      "Validation RMSE:  2.435551182208501\n",
      "Test RMSE:  1.9612923563218208\n",
      "Epoch: 0916 Training RMSE= 2.060246097\n",
      "Validation RMSE:  2.444104417237326\n",
      "Test RMSE:  1.9692778106891387\n",
      "Epoch: 0917 Training RMSE= 2.060762075\n",
      "Validation RMSE:  2.4409634608230126\n",
      "Test RMSE:  1.9646269752641536\n",
      "Epoch: 0918 Training RMSE= 2.060867619\n",
      "Validation RMSE:  2.424835487566482\n",
      "Test RMSE:  1.9571803906046812\n",
      "Epoch: 0919 Training RMSE= 2.060870346\n",
      "Validation RMSE:  2.43857379638502\n",
      "Test RMSE:  1.9602367015154496\n",
      "Epoch: 0920 Training RMSE= 2.059616457\n",
      "Validation RMSE:  2.450585843128273\n",
      "Test RMSE:  1.9794309556746943\n",
      "Epoch: 0921 Training RMSE= 2.060204500\n",
      "Validation RMSE:  2.4362607642337695\n",
      "Test RMSE:  1.966427841340789\n",
      "Epoch: 0922 Training RMSE= 2.059166283\n",
      "Validation RMSE:  2.4349520766890356\n",
      "Test RMSE:  1.9574343633548346\n",
      "Epoch: 0923 Training RMSE= 2.059012583\n",
      "Validation RMSE:  2.4377003856355826\n",
      "Test RMSE:  1.9613614934784787\n",
      "Epoch: 0924 Training RMSE= 2.061012594\n",
      "Validation RMSE:  2.4444466419884407\n",
      "Test RMSE:  1.9729214933739834\n",
      "Epoch: 0925 Training RMSE= 2.058454607\n",
      "Validation RMSE:  2.4289922547474077\n",
      "Test RMSE:  1.9613610680265054\n",
      "Epoch: 0926 Training RMSE= 2.059592510\n",
      "Validation RMSE:  2.4420893819651934\n",
      "Test RMSE:  1.9690742679759499\n",
      "Epoch: 0927 Training RMSE= 2.058068844\n",
      "Validation RMSE:  2.44501357033831\n",
      "Test RMSE:  1.9738050986795903\n",
      "Epoch: 0928 Training RMSE= 2.059005711\n",
      "Validation RMSE:  2.4251931747693765\n",
      "Test RMSE:  1.9554576676862805\n",
      "Epoch: 0929 Training RMSE= 2.057154165\n",
      "Validation RMSE:  2.4357840170964784\n",
      "Test RMSE:  1.9593877861907745\n",
      "Epoch: 0930 Training RMSE= 2.057544271\n",
      "Validation RMSE:  2.4364921736001475\n",
      "Test RMSE:  1.9624406896728246\n",
      "Epoch: 0931 Training RMSE= 2.058414916\n",
      "Validation RMSE:  2.4370509981212636\n",
      "Test RMSE:  1.9617638537570508\n",
      "Epoch: 0932 Training RMSE= 2.057718775\n",
      "Validation RMSE:  2.4348639637430485\n",
      "Test RMSE:  1.9657737093469223\n",
      "Epoch: 0933 Training RMSE= 2.057378540\n",
      "Validation RMSE:  2.4291412131893675\n",
      "Test RMSE:  1.9583414835117727\n",
      "Epoch: 0934 Training RMSE= 2.056599301\n",
      "Validation RMSE:  2.44419416015679\n",
      "Test RMSE:  1.9715876338153004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0935 Training RMSE= 2.056743855\n",
      "Validation RMSE:  2.4260491117300664\n",
      "Test RMSE:  1.9547354353066835\n",
      "Epoch: 0936 Training RMSE= 2.056409529\n",
      "Validation RMSE:  2.4341519176432898\n",
      "Test RMSE:  1.9640039731138952\n",
      "Epoch: 0937 Training RMSE= 2.056763706\n",
      "Validation RMSE:  2.4397949028194543\n",
      "Test RMSE:  1.9613334742300104\n",
      "Epoch: 0938 Training RMSE= 2.054757585\n",
      "Validation RMSE:  2.43332919279382\n",
      "Test RMSE:  1.963164520133064\n",
      "Epoch: 0939 Training RMSE= 2.057870151\n",
      "Validation RMSE:  2.4273796773804857\n",
      "Test RMSE:  1.9553417138637472\n",
      "Epoch: 0940 Training RMSE= 2.055584611\n",
      "Validation RMSE:  2.44544136755926\n",
      "Test RMSE:  1.9806694365476365\n",
      "Epoch: 0941 Training RMSE= 2.055409208\n",
      "Validation RMSE:  2.4327008980674463\n",
      "Test RMSE:  1.9581267893880734\n",
      "Epoch: 0942 Training RMSE= 2.055539013\n",
      "Validation RMSE:  2.419777620340196\n",
      "Test RMSE:  1.9508525751325434\n",
      "Epoch: 0943 Training RMSE= 2.055580695\n",
      "Validation RMSE:  2.4389595405895172\n",
      "Test RMSE:  1.96718703399483\n",
      "Epoch: 0944 Training RMSE= 2.054407775\n",
      "Validation RMSE:  2.4503594134869853\n",
      "Test RMSE:  1.9767785990797795\n",
      "Epoch: 0945 Training RMSE= 2.055644920\n",
      "Validation RMSE:  2.4204360132488896\n",
      "Test RMSE:  1.948700677371097\n",
      "Epoch: 0946 Training RMSE= 2.054863230\n",
      "Validation RMSE:  2.424334575358338\n",
      "Test RMSE:  1.9564349547145596\n",
      "Epoch: 0947 Training RMSE= 2.054752691\n",
      "Validation RMSE:  2.4480474126001686\n",
      "Test RMSE:  1.9742033075861622\n",
      "Epoch: 0948 Training RMSE= 2.053963826\n",
      "Validation RMSE:  2.4357268045952294\n",
      "Test RMSE:  1.9690528818297277\n",
      "Epoch: 0949 Training RMSE= 2.054015060\n",
      "Validation RMSE:  2.4406297349090167\n",
      "Test RMSE:  1.9704942242814323\n",
      "Epoch: 0950 Training RMSE= 2.054654953\n",
      "Validation RMSE:  2.4326479131533825\n",
      "Test RMSE:  1.9618063897625284\n",
      "Epoch: 0951 Training RMSE= 2.052033742\n",
      "Validation RMSE:  2.446674528228419\n",
      "Test RMSE:  1.9681265010204145\n",
      "Epoch: 0952 Training RMSE= 2.054581788\n",
      "Validation RMSE:  2.441101372565351\n",
      "Test RMSE:  1.9699157109600016\n",
      "Epoch: 0953 Training RMSE= 2.052404309\n",
      "Validation RMSE:  2.4363181354513017\n",
      "Test RMSE:  1.9664112610842146\n",
      "Epoch: 0954 Training RMSE= 2.052632754\n",
      "Validation RMSE:  2.4435726472153294\n",
      "Test RMSE:  1.9794200099287877\n",
      "Epoch: 0955 Training RMSE= 2.052850607\n",
      "Validation RMSE:  2.431780965041406\n",
      "Test RMSE:  1.9580880850226274\n",
      "Epoch: 0956 Training RMSE= 2.053048204\n",
      "Validation RMSE:  2.427063533936867\n",
      "Test RMSE:  1.9606079189345686\n",
      "Epoch: 0957 Training RMSE= 2.052150172\n",
      "Validation RMSE:  2.4372316604019733\n",
      "Test RMSE:  1.963882195838855\n",
      "Epoch: 0958 Training RMSE= 2.052225313\n",
      "Validation RMSE:  2.432639288447752\n",
      "Test RMSE:  1.9600667806118648\n",
      "Epoch: 0959 Training RMSE= 2.051601054\n",
      "Validation RMSE:  2.433886355815729\n",
      "Test RMSE:  1.9652598188887977\n",
      "Epoch: 0960 Training RMSE= 2.051544093\n",
      "Validation RMSE:  2.430926298445945\n",
      "Test RMSE:  1.9616112786330109\n",
      "Epoch: 0961 Training RMSE= 2.050629533\n",
      "Validation RMSE:  2.4296891436310633\n",
      "Test RMSE:  1.9579593490729246\n",
      "Epoch: 0962 Training RMSE= 2.050865281\n",
      "Validation RMSE:  2.43405149511469\n",
      "Test RMSE:  1.9662766133112164\n",
      "Epoch: 0963 Training RMSE= 2.051992298\n",
      "Validation RMSE:  2.4425286726503446\n",
      "Test RMSE:  1.9709375580400101\n",
      "Epoch: 0964 Training RMSE= 2.051184965\n",
      "Validation RMSE:  2.4313721167680806\n",
      "Test RMSE:  1.9667824950161301\n",
      "Epoch: 0965 Training RMSE= 2.050369926\n",
      "Validation RMSE:  2.4373117031825164\n",
      "Test RMSE:  1.964471573211101\n",
      "Epoch: 0966 Training RMSE= 2.051334880\n",
      "Validation RMSE:  2.4380053460967708\n",
      "Test RMSE:  1.9663898308196925\n",
      "Epoch: 0967 Training RMSE= 2.049264060\n",
      "Validation RMSE:  2.4177878193497264\n",
      "Test RMSE:  1.9512196058634554\n",
      "Epoch: 0968 Training RMSE= 2.049743902\n",
      "Validation RMSE:  2.4360978666496713\n",
      "Test RMSE:  1.9680752884760861\n",
      "Epoch: 0969 Training RMSE= 2.050730496\n",
      "Validation RMSE:  2.440486276862715\n",
      "Test RMSE:  1.9697218570076185\n",
      "Epoch: 0970 Training RMSE= 2.048697294\n",
      "Validation RMSE:  2.4205794283878648\n",
      "Test RMSE:  1.9536395959051989\n",
      "Epoch: 0971 Training RMSE= 2.050470014\n",
      "Validation RMSE:  2.4264386648400267\n",
      "Test RMSE:  1.951706256565355\n",
      "Epoch: 0972 Training RMSE= 2.048575280\n",
      "Validation RMSE:  2.4700973793478878\n",
      "Test RMSE:  1.989229826620304\n",
      "Epoch: 0973 Training RMSE= 2.049554575\n",
      "Validation RMSE:  2.4281016261523476\n",
      "Test RMSE:  1.9643839000338863\n",
      "Epoch: 0974 Training RMSE= 2.048728030\n",
      "Validation RMSE:  2.43486775808757\n",
      "Test RMSE:  1.9637061707032681\n",
      "Epoch: 0975 Training RMSE= 2.049501509\n",
      "Validation RMSE:  2.431443968637766\n",
      "Test RMSE:  1.9565325194789756\n",
      "Epoch: 0976 Training RMSE= 2.049154262\n",
      "Validation RMSE:  2.4364115412213123\n",
      "Test RMSE:  1.9657431756475667\n",
      "Epoch: 0977 Training RMSE= 2.049043762\n",
      "Validation RMSE:  2.4364584996317307\n",
      "Test RMSE:  1.9651352986500903\n",
      "Epoch: 0978 Training RMSE= 2.047876890\n",
      "Validation RMSE:  2.42558258275111\n",
      "Test RMSE:  1.9550905179632267\n",
      "Epoch: 0979 Training RMSE= 2.047971299\n",
      "Validation RMSE:  2.4470353318809965\n",
      "Test RMSE:  1.9705879018953258\n",
      "Epoch: 0980 Training RMSE= 2.049075972\n",
      "Validation RMSE:  2.450103064724216\n",
      "Test RMSE:  1.9776010310506755\n",
      "Epoch: 0981 Training RMSE= 2.047869949\n",
      "Validation RMSE:  2.4196884005698975\n",
      "Test RMSE:  1.953442006134172\n",
      "Epoch: 0982 Training RMSE= 2.046053691\n",
      "Validation RMSE:  2.440451827603308\n",
      "Test RMSE:  1.9671953814699363\n",
      "Epoch: 0983 Training RMSE= 2.048983063\n",
      "Validation RMSE:  2.4279758888717566\n",
      "Test RMSE:  1.9555408185601655\n",
      "Epoch: 0984 Training RMSE= 2.046403631\n",
      "Validation RMSE:  2.4448821447788687\n",
      "Test RMSE:  1.9747186587133645\n",
      "Epoch: 0985 Training RMSE= 2.047124977\n",
      "Validation RMSE:  2.426854496647482\n",
      "Test RMSE:  1.9573357168224481\n",
      "Epoch: 0986 Training RMSE= 2.047364297\n",
      "Validation RMSE:  2.44383870522368\n",
      "Test RMSE:  1.9702404375080207\n",
      "Epoch: 0987 Training RMSE= 2.046038095\n",
      "Validation RMSE:  2.421306740327863\n",
      "Test RMSE:  1.9532620801504499\n",
      "Epoch: 0988 Training RMSE= 2.047052433\n",
      "Validation RMSE:  2.4375365939205533\n",
      "Test RMSE:  1.964284221846769\n",
      "Epoch: 0989 Training RMSE= 2.047202824\n",
      "Validation RMSE:  2.4349273407609426\n",
      "Test RMSE:  1.964710617863829\n",
      "Epoch: 0990 Training RMSE= 2.045713439\n",
      "Validation RMSE:  2.420582777266231\n",
      "Test RMSE:  1.9519777972402854\n",
      "Epoch: 0991 Training RMSE= 2.046240385\n",
      "Validation RMSE:  2.439335914159895\n",
      "Test RMSE:  1.970183259505203\n",
      "Epoch: 0992 Training RMSE= 2.044776084\n",
      "Validation RMSE:  2.434968159191303\n",
      "Test RMSE:  1.9610904922683206\n",
      "Epoch: 0993 Training RMSE= 2.045947217\n",
      "Validation RMSE:  2.4286961142805086\n",
      "Test RMSE:  1.9586618144591113\n",
      "Epoch: 0994 Training RMSE= 2.045403348\n",
      "Validation RMSE:  2.4283077333244623\n",
      "Test RMSE:  1.9574142203165146\n",
      "Epoch: 0995 Training RMSE= 2.044618443\n",
      "Validation RMSE:  2.4488203585585664\n",
      "Test RMSE:  1.979384010494382\n",
      "Epoch: 0996 Training RMSE= 2.044757407\n",
      "Validation RMSE:  2.428296847258068\n",
      "Test RMSE:  1.9593220626031622\n",
      "Epoch: 0997 Training RMSE= 2.045273115\n",
      "Validation RMSE:  2.444644458913134\n",
      "Test RMSE:  1.9772593208534137\n",
      "Epoch: 0998 Training RMSE= 2.043764372\n",
      "Validation RMSE:  2.4332752662310275\n",
      "Test RMSE:  1.9606462999523904\n",
      "Epoch: 0999 Training RMSE= 2.044224693\n",
      "Validation RMSE:  2.4409938616552376\n",
      "Test RMSE:  1.9675503360557323\n",
      "Epoch: 1000 Training RMSE= 2.044052688\n",
      "Validation RMSE:  2.428899974698328\n",
      "Test RMSE:  1.9567047273139038\n",
      "epoch is  999\n",
      "training RMSE is  2.0492640596093383\n",
      "Optimization Finished! the lowest validation RMSE is  2.4177878193497264\n",
      "The test RMSE is  1.9512196058634554\n",
      "0:26:11.838354\n"
     ]
    }
   ],
   "source": [
    "# change hidden number\n",
    "# change batch_size\n",
    "# set size\n",
    "#from bayes_opt import BayesianOptimization\n",
    "import datetime\n",
    "#freq_max = 12\n",
    "#time_step = 12\n",
    "learning_rate = 0.002\n",
    "decay = 0.9 \n",
    "batch_size = 500\n",
    "num_hidden = 100 # number of hiddent units in LSTM Cell\n",
    "early_stop_th = 150\n",
    "training_epochs = 1000\n",
    "keep = 1#0.2\n",
    "#time_step_max = 10\n",
    "\n",
    "sn = 272 # station num\n",
    "#test = 2000 \n",
    "reg1 = 0.05#0.05\n",
    "reg2 = 0.05#0.1\n",
    "reg3 = 0.05\n",
    "frequency = 24\n",
    "horizon = 1\n",
    "lstm_steps = 3  # number of lstm cells\n",
    "n_hidden_vec1 = 10\n",
    "n_hidden_vec2 = 5\n",
    "n_hidden_vec3 = 10\n",
    "n_hidden_vec4 = 10\n",
    "#num = 0\n",
    "#All_pred = np.empty([2000, 207])\n",
    "#All_Y = np.empty([2000, 207])\n",
    "\n",
    "#24*90\n",
    "#step = 0\n",
    "#gap = 100\n",
    "#training = 0.7\n",
    "#validation = 0.1\n",
    "#test = 0.2\n",
    "\n",
    "#gcn_corr_eval(7, 0.01, 0.5, 100, 0.4, 10, 5, 5, 0.2, 50, 500)\n",
    "\n",
    "\n",
    "\n",
    "rep = 1 # repeating times\n",
    "\n",
    "#total_sn = 0\n",
    "#num_iter = 50\n",
    "#init_points = 200\n",
    "\n",
    "\n",
    "# stdbscan\n",
    "#spatial_threshold = 300\n",
    "#temporal_threshold = 300\n",
    "#min_neighbors = 1 # number of neighbor\n",
    "\n",
    "#frequency2 = skip1 + freq_max + training\n",
    "\n",
    "#while step < 2000:\n",
    "\n",
    "#hourly_bike_cluster = hourly_bike\n",
    "best = -10000\n",
    "pre_best = []\n",
    "test_Y_best = []\n",
    "test_error_best = 1000\n",
    "A1_best = []\n",
    "# A2_best = []\n",
    "for i in range(rep):\n",
    "    a = datetime.datetime.now()\n",
    "    test_error, predic_res, Y_true = gcn_corr_final(frequency, horizon, learning_rate, decay, batch_size, n_hidden_vec1,\n",
    "                                                                n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\n",
    "    #val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\n",
    "    #print (\"finished A running: \", i)\n",
    "    b = datetime.datetime.now()\n",
    "    print(b-a)\n",
    "    \n",
    "\n",
    "    #total_sn = total_sn + sn\n",
    "\n",
    "    #total_error = np.sqrt(np.mean((All_pred[0:(step+gap),0:total_sn] - All_Y[0:(step+gap),0:total_sn])**2))\n",
    "\n",
    "    #print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the cluster now is: \", c)\n",
    "    #print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the val error of this cluster now is: \", best)\n",
    "    #print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the test error by this cluster now is: \", total_error)\n",
    "    #step = step + gap\n",
    "    #skip1 = skip1 + gap\n",
    "    \n",
    "    #np.savetxt(\"prediction_2.csv\", All_pred, delimiter = ',')\n",
    "    #np.savetxt(\"prediction_Y_2.csv\", All_Y, delimiter = ',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../../data/nyc_bike/lstm_gcnn_prediction_1.95.csv\", predic_res, delimiter = ',')\n",
    "np.savetxt(\"../../data/nyc_bike/lstm_gcnn_prediction_1.95_Y.csv\", Y_true, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# learning rate to 0.005, not decreasing, bouncing\n",
    "# change it to 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_whole = []\n",
    "Y_whole = []\n",
    "\n",
    "x_offsets = np.sort(\n",
    "    # np.concatenate(([-week_size + 1, -day_size + 1], np.arange(-11, 1, 1)))\n",
    "    np.concatenate((np.arange(-frequency+1, 1, 1),))\n",
    ")\n",
    "# Predict the next one hour\n",
    "y_offsets = np.sort(np.arange(1, 1+ horizon, 1))\n",
    "\n",
    "min_t = abs(min(x_offsets))\n",
    "max_t = abs(raw_data.shape[0] - abs(max(y_offsets)))  # Exclusive\n",
    "for t in range(min_t, max_t):\n",
    "    x_t = raw_data.iloc[t + x_offsets, 0:sn].values.flatten('F')\n",
    "    y_t = raw_data.iloc[t + y_offsets, 0:sn].values.flatten('F')\n",
    "    X_whole.append(x_t)\n",
    "    Y_whole.append(y_t)\n",
    "\n",
    "X_whole = np.stack(X_whole, axis=0)\n",
    "time_step = int(X_whole.shape[1] / sn)\n",
    "#X_whole = np.reshape(X_whole, [X_whole.shape[0], sn, time_step])\n",
    "Y_whole = np.stack(Y_whole, axis=0)\n",
    "\n",
    "i = lstm_steps\n",
    "X_whole_lstm = []\n",
    "Y_whole_lstm = []\n",
    "\n",
    "while i < X_whole.shape[0]:\n",
    "    X_whole_lstm.append(X_whole[i-lstm_steps:i,:])\n",
    "    Y_whole_lstm.append(Y_whole[i])\n",
    "    i = i + 1\n",
    "\n",
    "X_whole_lstm = np.stack(X_whole_lstm, axis = 0) # (34239, 10, 207, 12)\n",
    "Y_whole_lstm = np.stack(Y_whole_lstm, axis = 0) # (34239, 2484)\n",
    "#print (Y_whole_lstm.shape)\n",
    "'''\n",
    "time_step = int(time_step) #\n",
    "\n",
    "#i = time_step\n",
    "#X_whole = np.zeros(shape = (raw_data.shape[0] - time_step, sn*time_step), dtype = np.float)\n",
    "#Y_whole = np.zeros(shape = (raw_data.shape[0] - time_step, sn), dtype = np.float)\n",
    "\n",
    "while i < raw_data.shape[0]:\n",
    "    X_whole[i - time_step, ] = raw_data.iloc[(i - time_step):i, 0:sn].values.flatten('F') # 'F' flatten by column, default:flatten by row 0, 1, 2...7\n",
    "    Y_whole[i - time_step, ] = raw_data.iloc[i, 0:sn]\n",
    "    i = i + 1\n",
    "'''\n",
    "\n",
    "\n",
    "n_input = sn # station number\n",
    "n_input_vec = n_input * frequency # 207 * frequency\n",
    "n_A_vec = n_input * n_input\n",
    "n_output_vec = Y_whole_lstm.shape[1] # each row represent a result\n",
    "#print (n_output_vec)\n",
    "\n",
    "\n",
    "num_samples = X_whole_lstm.shape[0]\n",
    "num_train = 20000 # Note here actually we use the first 20000 to train the model. The paper mentioned \"22304\" need to be corrected.\n",
    "num_val = 2000\n",
    "num_test = 2000\n",
    "#skip = skip1 + freq_max - time_step#time_step_max - time_step # to make sure the testing datasets are the same although the frequency could be different\n",
    "\n",
    "X_training = X_whole_lstm[:num_train, :]\n",
    "Y_training = Y_whole_lstm[:num_train, :]\n",
    "\n",
    "# shuffle\n",
    "perm = np.arange(X_whole_lstm.shape[0])\n",
    "np.random.shuffle(perm)\n",
    "X_training = X_whole_lstm[perm]\n",
    "Y_training = Y_whole_lstm[perm]\n",
    "\n",
    "#print (type(X_training))\n",
    "#X_training = random.Random(6).shuffle(X_training)\n",
    "#Y_training = random.Random(6).shuffle(Y_training)\n",
    "\n",
    "X_val = X_whole_lstm[num_train:num_train+num_val, :]\n",
    "Y_val = Y_whole_lstm[num_train:num_train+num_val, :]\n",
    "#A_val = A_whole[0+training:0+training+validation, :]\n",
    "\n",
    "X_test = X_whole_lstm[num_train+num_val:num_train+num_val+num_test, :]\n",
    "Y_test = Y_whole_lstm[num_train+num_val:num_train+num_val+num_test, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3, 6528)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 2, ..., 0, 2, 2],\n",
       "       [4, 0, 2, ..., 0, 0, 1],\n",
       "       [2, 2, 0, ..., 0, 1, 1],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 272)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  2,  3, ...,  5,  4, 10],\n",
       "       [ 6,  3,  7, ..., 13,  9,  2],\n",
       "       [ 3,  0,  0, ...,  3,  6, 23],\n",
       "       ...,\n",
       "       [ 1,  0,  2, ...,  0,  0,  0],\n",
       "       [ 0,  0,  1, ...,  0,  1,  5],\n",
       "       [ 0,  1,  1, ...,  2,  0,  2]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(X_test[0, 2, :], (272, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  0,  2,  5,  3,  4,  3,  9,  8,  4,  5,  8,  3,  5,  7,  8,  7,\n",
       "        5,  3,  1,  5,  4,  6,  3,  0,  2,  0,  2,  1,  8, 14,  0,  1,  2,\n",
       "        0,  3,  0, 14, 14,  9,  8,  9,  1,  1,  1,  5,  9,  3,  4,  2,  4,\n",
       "        5,  1,  6,  4,  0,  3,  8,  3,  2,  9,  2,  9,  1,  1,  2,  1,  0,\n",
       "        3,  1,  6,  0,  0,  0,  5,  3,  0,  5,  5,  2,  3,  0,  4,  0,  2,\n",
       "        2, 11,  5, 13,  1,  3,  0,  3,  4,  5,  2,  5,  5,  3,  5,  3,  3,\n",
       "        5,  3,  2,  7,  2,  3,  2,  2,  2,  2, 16,  7,  1,  2,  1,  1,  5,\n",
       "        2,  5,  0,  3,  4,  2,  0,  1,  3,  2,  3,  5,  4,  0,  3,  7,  8,\n",
       "        4,  2,  3,  4,  1,  7,  7,  3,  4, 10,  5,  2,  0,  4,  1,  1,  2,\n",
       "        0,  0,  0,  5,  1, 15, 17,  3,  4,  0,  5,  2,  7,  0,  6,  5,  4,\n",
       "        0,  1,  0,  6,  1,  1,  4,  4,  2,  0,  3,  2,  6,  1, 19,  7,  3,\n",
       "        1,  0,  1,  0,  2,  1,  0,  3,  0,  7,  1,  2,  1,  1,  1,  2,  4,\n",
       "        3,  0,  1,  0,  5,  3,  9,  7,  0,  5,  2,  4,  7,  7, 11, 19,  7,\n",
       "        2,  9,  3,  1,  7, 11,  5,  7,  4,  6,  2,  3,  3,  1,  8,  4,  5,\n",
       "       11,  2,  4,  4,  3,  3,  0,  1,  2,  2,  3,  1,  7,  3,  0,  2, 10,\n",
       "        1,  1,  7,  1,  1,  1,  3,  7,  5,  3,  1,  7,  1,  7,  0,  2,  2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 3, ..., 1, 0, 1],\n",
       "       [0, 0, 3, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 1, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_training[0, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
