{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"../../data/nyc_bike/NYCBikeHourly272.pickle\"\n",
    "# open the file for writing\n",
    "fileObject = open(file_Name,'rb') \n",
    "hourly_bike = pickle.load(fileObject) \n",
    "hourly_bike = pd.DataFrame(hourly_bike)\n",
    "# results from bikedemandhourly_lstm_weather.ipynb\n",
    "# new station id for the selected 272 stations\n",
    "station_list = [0, 2, 3, 7, 8, 9, 11, 12, 27, 32, 33, 34, 38, 39, 40, 42, 43, 44, 46, 48, 52, 53, 54, 58, 59, 61, 62, 64, 65, 67, 68, 69, 74, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 104, 108, 111, 112, 115, 120, 121, 122, 124, 127, 130, 131, 133, 134, 135, 136, 138, 141, 142, 143, 145, 146, 150, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181, 182, 185, 186, 187, 188, 189, 195, 197, 198, 199, 200, 201, 204, 205, 206, 207, 208, 209, 210, 212, 214, 216, 219, 220, 221, 226, 242, 245, 249, 251, 252, 255, 257, 258, 263, 267, 273, 274, 275, 276, 280, 281, 282, 284, 290, 292, 295, 298, 299, 305, 306, 307, 308, 311, 316, 318, 320, 321, 322, 323, 327, 335, 340, 341, 342, 344, 345, 347, 348, 349, 353, 359, 361, 369, 371, 372, 393, 396, 397, 403, 404, 408, 410, 412, 414, 415, 416, 422, 429, 432, 434, 435, 438, 443, 446, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 467, 472, 475, 476, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 506, 508, 509, 510, 511, 512, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 534, 535, 537, 539, 540, 541, 542, 544, 545, 546, 547, 548, 549, 550, 553, 554]\n",
    "new_old_stationID = pd.read_csv('../../data/nyc_bike/bike_stations.csv')\n",
    "#new_old_stationID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_old_stationID = new_old_stationID[new_old_stationID['station_id_new'].isin(station_list)]\n",
    "#new_old_stationID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_old_stationID = new_old_stationID.drop_duplicates('station_id_new', keep ='last')\n",
    "new_old_stationID = new_old_stationID.reset_index()\n",
    "#new_old_stationID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_list = list(range(272))\n",
    "#station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_bike_gps = hourly_bike.transpose()\n",
    "hourly_bike_gps['lat'] = new_old_stationID['lat']\n",
    "hourly_bike_gps['lon'] = new_old_stationID['lon']\n",
    "hourly_bike_gps['index1'] = range(272)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26297</th>\n",
       "      <th>26298</th>\n",
       "      <th>26299</th>\n",
       "      <th>26300</th>\n",
       "      <th>26301</th>\n",
       "      <th>26302</th>\n",
       "      <th>26303</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>index1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>40.732219</td>\n",
       "      <td>-73.981656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>40.741444</td>\n",
       "      <td>-73.975361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.750020</td>\n",
       "      <td>-73.969053</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.750664</td>\n",
       "      <td>-74.001768</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>40.751396</td>\n",
       "      <td>-74.005226</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8   9  ...  26297  26298  26299  26300  26301  \\\n",
       "0  2  1  0  0  0  0  2  4  3   2  ...     23     21     24     12      8   \n",
       "1  1  0  0  0  0  0  1  1  4  13  ...      3     12     18     11      8   \n",
       "2  0  0  0  0  0  0  0  2  4   0  ...     17      8      7      5      1   \n",
       "3  3  1  0  1  1  0  0  2  7   4  ...      0      0      0      0      0   \n",
       "4  0  2  0  0  0  0  3  4  4   3  ...     19     41     18     24      8   \n",
       "\n",
       "   26302  26303        lat        lon  index1  \n",
       "0      5      6  40.732219 -73.981656       0  \n",
       "1      4      1  40.741444 -73.975361       1  \n",
       "2      0      1  40.750020 -73.969053       2  \n",
       "3      0      0  40.750664 -74.001768       3  \n",
       "4      5      6  40.751396 -74.005226       4  \n",
       "\n",
       "[5 rows x 26307 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_bike_gps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    #adj[np.isnan(adj)] = 0.\n",
    "    adj = tf.abs(adj)\n",
    "    rowsum = tf.reduce_sum(adj, 1)# sum by row\n",
    "\n",
    "    d_inv_sqrt = tf.pow(rowsum, -0.5)\n",
    "   \n",
    "    #d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    \n",
    "    d_mat_inv_sqrt = tf.diag(d_inv_sqrt)\n",
    "\n",
    "    return tf.matmul(tf.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def gcn(x, weights, biases, batch_size, n_input, frequency,flag):\n",
    "    # Hidden layer with RELU activation\n",
    "    \n",
    "    output_list = tf.Variable(tf.zeros([sn,1]),dtype=tf.float32) #'Tensor' object does not support item assignment, cant build Ypre\n",
    "    \n",
    "    if flag == 1:\n",
    "        rownum = batch_size\n",
    "    elif flag == 2:\n",
    "        rownum = validation\n",
    "    elif flag == 3:\n",
    "        rownum = test\n",
    "    \n",
    "    for i in range(rownum):\n",
    "        Xtem = tf.reshape(x[i,:], [frequency, n_input])\n",
    "        Xtem = tf.transpose(Xtem)\n",
    "        #Atem = tf.convert_to_tensor(A_whole_final, dtype=np.float32)\n",
    "        #Atem1 = tf.diag(tf.ones([n_input])) # Atem not palying any roles\n",
    "        #Ytem = tf.reshape(Y[i,:], [n_input, 1])\n",
    "        Atem = tf.diag(tf.ones([n_input]))\n",
    "        Atem1 = 0.5*(weights['A1'] + tf.transpose(weights['A1']))#+ Atem \n",
    "        Atem1 = normalize_adj(Atem1)\n",
    "        #th = tf.constant(0.01, dtype=tf.float32)\n",
    "        #where = tf.subtract(Atem1, th)\n",
    "        #Atem1 = tf.nn.relu(where)\n",
    "        \n",
    "        Z1 = tf.matmul(Atem1, Xtem)\n",
    "        #layer_1 = tf.matmul(Z1, weights['h1']) \n",
    "        layer_1 = tf.add(tf.matmul(Z1, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        \n",
    "        #Atem2 = 0.5*(weights['A2'] + tf.transpose(weights['A2']))#+ Atem \n",
    "        #Atem2 = normalize_adj(Atem2)\n",
    "        \n",
    "        #Z2 = tf.matmul(Atem2, layer_1)\n",
    "        #layer_2 = tf.add(tf.matmul(Z2, weights['h2']), biases['b2'])\n",
    "        #layer_2 = tf.nn.relu(layer_2)\n",
    "        \n",
    "        #Atem3 = weights['A3']+ Atem \n",
    "        #Z3 = tf.matmul(Atem3, layer_2)\n",
    "        #layer_3 = tf.add(tf.matmul(Z3, weights['h3']), biases['b3'])\n",
    "        #layer_3 = tf.nn.relu(layer_3)\n",
    "        \n",
    "        # flattern\n",
    "        #layer_3 = tf.reshape(layer_3, [1, 272*n_hidden_vec3])\n",
    "        \n",
    "        #F1 = tf.add(tf.matmul(layer_3, weights['f1']), biases['bf1'])\n",
    "        #F1 = tf.nn.relu(F1)\n",
    "        \n",
    "        #F2 = tf.add(tf.matmul(F1, weights['f2']), biases['bf2'])\n",
    "        #F2 = tf.nn.relu(F2)\n",
    "        \n",
    "        #F3 = tf.add(tf.matmul(F1, weights['f3']), biases['bf3'])\n",
    "        #out_layer = tf.reshape(F3, [272, 1])\n",
    "\n",
    "        # Output layer with linear activation\n",
    "        Z4 = layer_1#tf.matmul(Atem1, layer_1)#tf.matmul(Atem, layer_3)\n",
    "        out_layer = tf.add(tf.matmul(Z4, weights['out']), biases['bout'])\n",
    "        \n",
    "        # weather layer 1\n",
    "        #x_wea_tem = tf.reshape(x_wea[i,:], [1, 9*frequency2]) # 1 by 126\n",
    "        #layer_1_wea = tf.add(tf.matmul(x_wea_tem, weights['h1_wea']), biases['b1_wea'])\n",
    "        #layer_1_wea = tf.nn.relu(layer_1_wea)\n",
    "        \n",
    "        #out_layer_wea = tf.add(tf.matmul(layer_1_wea, weights['out_wea']), biases['bout_wea'])\n",
    "        #out_layer = tf.add(out_layer, tf.reshape(out_layer_wea, [272, 1]))\n",
    "        \n",
    "        #print (out_layer.get_shape())\n",
    "        if i ==0:\n",
    "            output_list = out_layer\n",
    "        else:\n",
    "            output_list = tf.concat([output_list, out_layer], 1)\n",
    "        \n",
    "        #print (tf.reduce_mean(tf.pow(output_list-out_layer, 2)))\n",
    "    \n",
    "    #print (output_list.get_shape())\n",
    "    \n",
    "    output_list = tf.transpose(output_list)\n",
    "    \n",
    "    #print (output_list.get_shape())\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_corr_final(frequency, learning_rate, decay,batch_size, n_hidden_vec1,n_hidden_vec2,n_hidden_vec3,keep, early_stop_th,training_epochs, reg1, reg2):\n",
    "    # set size\n",
    "    #sn = 3 # station number\n",
    "\n",
    "    frequency = int(frequency) #\n",
    "\n",
    "    i = frequency\n",
    "    X_whole = np.zeros(shape = (hourly_bike_cluster.shape[0] - frequency, sn*frequency), dtype = np.float)\n",
    "    Y_whole = np.zeros(shape = (hourly_bike_cluster.shape[0] - frequency, sn), dtype = np.float)\n",
    "\n",
    "    while i < hourly_bike_cluster.shape[0]:\n",
    "        X_whole[i - frequency, ] = hourly_bike_cluster.iloc[(i - frequency):i, 0:sn].values.flatten() # flatten by row 0, 1, 2...7\n",
    "        Y_whole[i - frequency, ] = hourly_bike_cluster.iloc[i, 0:sn]\n",
    "        i = i + 1\n",
    "        #print (i)\n",
    "    \n",
    "    skip = skip1 + freq_max - frequency # to make sure the testing datasets are the same although the frequency could be different\n",
    "\n",
    "    X_training = X_whole[skip: skip+ training, :]\n",
    "    Y_training = Y_whole[skip: skip+ training, :]\n",
    "    #A_training = A_whole[0: 0+ training, :]\n",
    "\n",
    "    X_val = X_whole[skip+training:skip+training+validation, :]\n",
    "    Y_val = Y_whole[skip+training:skip+training+validation, :]\n",
    "    #A_val = A_whole[0+training:0+training+validation, :]\n",
    "\n",
    "    X_test = X_whole[skip+training+validation:skip+training+validation+test, :]\n",
    "    Y_test = Y_whole[skip+training+validation:skip+training+validation+test, :]\n",
    "    #A_test = A_whole[0+training+validation:0+training+validation+test, :]\n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "    n_hidden_vec1 = int(n_hidden_vec1)\n",
    "    n_hidden_vec2 = int(n_hidden_vec2)\n",
    "    n_hidden_vec3 = int(n_hidden_vec3)\n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    # Network Parameters\n",
    "    n_input = sn # station number\n",
    "    n_input_vec = n_input * frequency # 317 * frequency\n",
    "    n_A_vec = n_input * n_input\n",
    "    n_output_vec = n_input * 1 # each row represent a result\n",
    "\n",
    "    #n_classes = 2 # MNIST total classes (0-9 digits) # n_classes is for classification only\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None, n_input_vec]) # X is the input signal\n",
    "    #X_weather = tf.placeholder(tf.float32, [None, 9 * frequency2]) # X_weather weather and holiday information (9 is the feature number)\n",
    "    A = tf.placeholder(tf.float32, [None, n_A_vec]) # A is the normalized adj matrix\n",
    "    oldA = tf.placeholder(tf.float32, [n_input, n_input])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_output_vec]) # y is the regression output\n",
    "\n",
    "    #Xtem = tf.placeholder(tf.float32, [n_input, frequency]) # for each row of X, A, Y, it can be reshaped to Xtem, Atem, Ytem\n",
    "    #Atem = tf.placeholder(tf.float32, [n_input, n_input]) # \n",
    "    #Ytem = tf.placeholder(tf.float32, [n_input, 1]) #\n",
    "\n",
    "    #Ypre = tf.placeholder(tf.float32, [None, n_output_vec])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([frequency, n_hidden_vec1])),\n",
    "        #'h2': tf.Variable(tf.random_normal([n_hidden_vec1, n_hidden_vec2])),\n",
    "        #'h3': tf.Variable(tf.random_normal([n_hidden_vec2, n_hidden_vec3])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_vec1, 1])), # dont forget to change n_hidden_vec1 when add/delete layers\n",
    "        #'f1': tf.Variable(tf.random_normal([272*n_hidden_vec3, 100])),\n",
    "        #'f2': tf.Variable(tf.random_normal([50, 10])),\n",
    "        #'f3': tf.Variable(tf.random_normal([100, 272])),\n",
    "        'A1': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'A2': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'A3': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'h1_wea': tf.Variable(tf.random_normal([9*frequency2, n_hidden_weather1])),\n",
    "        #'out_wea': tf.Variable(tf.random_normal([n_hidden_weather1, n_input]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_input,1])),# n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        #'b2': tf.Variable(tf.random_normal([n_input, 1])), #n_hidden_vec2])),\n",
    "        #'b3': tf.Variable(tf.random_normal([n_input, 1])),#n_hidden_vec3])),\n",
    "        #'b1': tf.Variable(tf.random_normal([n_input,n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        #'b2': tf.Variable(tf.random_normal([n_input,n_hidden_vec2])),\n",
    "        #'b3': tf.Variable(tf.random_normal([n_input,n_hidden_vec3])),\n",
    "        #'bf1': tf.Variable(tf.random_normal([1, 100])), \n",
    "        #'bf2': tf.Variable(tf.random_normal([1, 10])), \n",
    "        #'bf3': tf.Variable(tf.random_normal([1, 272])), \n",
    "        'bout': tf.Variable(tf.random_normal([n_input, 1])), \n",
    "        #'b1_wea': tf.Variable(tf.random_normal([1, n_hidden_weather1])), \n",
    "        #'bout_wea': tf.Variable(tf.random_normal([1, n_input])), \n",
    "    }\n",
    "\n",
    "    #gcn_no_A\n",
    "    #gcn\n",
    "    # Construct model\n",
    "    pred= gcn(X, weights, biases, batch_size,n_input, frequency, 1)\n",
    "    \n",
    "    #set  negative in pred to 0s\n",
    "    #Computes rectified linear: max(features, 0).\n",
    "    #pred = tf.nn.relu(pred)\n",
    "    # Define loss and optimizer\n",
    "    # RMS for regression \n",
    "    # L2 regularization\n",
    "    #cost = tf.reduce_mean(tf.pow(pred-Y, 2)) + reg1*tf.nn.l2_loss(weights['A1']) + reg2*tf.nn.l2_loss(weights['A2'])# + tf.nn.l2_loss(weights['A2']) + tf.nn.l2_loss(weights['A3'])))\n",
    "\n",
    "    # L1 regular tf.reduce_sum(tf.abs(parameters))\n",
    "    cost = tf.reduce_mean(tf.pow(pred-Y, 2)) + reg1*tf.reduce_sum(tf.abs(weights['A1']))# + reg2*tf.reduce_sum(tf.abs(weights['A2']))\n",
    "    pred_val= gcn(X, weights, biases, batch_size,n_input,frequency, 2)\n",
    "    #pred_val = tf.nn.relu(pred_val)\n",
    "    cost_val = tf.reduce_mean(tf.pow(pred_val-Y, 2))\n",
    "\n",
    "    pred_tes= gcn(X, weights, biases, batch_size,n_input,frequency, 3)\n",
    "    #pred_tes = tf.nn.relu(pred_tes)\n",
    "    cost_tes = tf.reduce_mean(tf.pow(pred_tes-Y, 2))\n",
    "    # cross-entropy for classification\n",
    "    # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y_train))\n",
    "    # ratio = tf.abs(tf.reduce_sum(pred)-tf.reduce_sum(Y))/tf.reduce_sum(Y)\n",
    "    #zero = 0\n",
    "    #ratio = tf.reduce_mean(tf.divide(tf.where(tf.not_equal(Y, zero), np.abs(pred-Y), tf.zeros(Y.get_shape(), tf.float32)), tf.where(tf.not_equal(Y, zero), Y, tf.ones(Y.get_shape(), tf.float32))))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay).minimize(cost)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #total_val_cost = []\n",
    "    #total_val_ratio = []\n",
    "\n",
    "    # learning start from \n",
    "\n",
    "    #index = daily_bike[(daily_bike['year'] == 2016) & (daily_bike['monthofyear'] == 1) & (daily_bike['dayofmonth'] == 1)].index.tolist()[0]\n",
    "    #A_hat = normalize_adj(corr_matrix_trips)\n",
    "    #print(A_hat)\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(training/batch_size)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                      Y: Y_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                              keep_prob: keep})\n",
    "                #print (\"Epoch:\", '%04d' % (epoch+1), \"batch: \", i, \"batch cost=\", \\\n",
    "                #    \"{:.9f}\".format(c))\n",
    "                #print (c)\n",
    "                avg_cost += c / total_batch \n",
    "                #Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Epoch:\", '%04d' % (epoch+1), \"Training RMSE=\", \\\n",
    "                    \"{:.9f}\".format((np.sqrt(avg_cost))))\n",
    "            # validation\n",
    "            c_val = sess.run([cost_val], feed_dict={X: X_val, Y: Y_val,  keep_prob:1})\n",
    "            print(\"Validation RMSE: \", (np.sqrt(c_val[0])))\n",
    "\n",
    "            c_tes, pred_tes1, A1= sess.run([cost_tes, pred_tes, weights['A1']], feed_dict={X: X_test,Y: Y_test, keep_prob: 1})\n",
    "            print(\"Test RMSE: \", (np.sqrt(c_tes)))\n",
    "\n",
    "            if c_val[0] < best_val:\n",
    "                best_val = c_val[0]\n",
    "                #saver.save(sess, './bikesharing_graph_2_th_point1')\n",
    "                test_error = c_tes\n",
    "                traing_error = np.sqrt(avg_cost)\n",
    "                early_stop_k = 0 # reset to 0\n",
    "                #print (pred_tes1)\n",
    "                predic_res = pred_tes1\n",
    "\n",
    "            # early stopping\n",
    "            if c_val[0] >= best_val:\n",
    "                early_stop_k += 1\n",
    "\n",
    "            # threshold\n",
    "            if early_stop_k == early_stop_th:\n",
    "              #  print (\"early stopping...\")\n",
    "                break\n",
    "            \n",
    "\n",
    "        print(\"epoch is \", epoch)\n",
    "        print(\"training error is \", traing_error)\n",
    "        print(\"Optimization Finished! the lowest validation RMSE is \", (np.sqrt(best_val)))\n",
    "        print(\"The test RMSE is \", (np.sqrt(test_error)))\n",
    "    \n",
    "    test_Y = Y_test\n",
    "    test_error = np.sqrt(test_error)\n",
    "    return -np.sqrt(best_val), predic_res,test_Y,test_error, A1#, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-612732e1d8a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mval_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredic_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgcn_corr_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;31m#val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"finished A running: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-90c89f8ac439>\u001b[0m in \u001b[0;36mgcn_corr_final\u001b[0;34m(frequency, learning_rate, decay, batch_size, n_hidden_vec1, n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mhourly_bike_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mX_whole\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhourly_bike_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flatten by row 0, 1, 2...7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mY_whole\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhourly_bike_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#print (i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1492\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   2143\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2145\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2146\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1015\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                 \u001b[0;31m# This is an elided recursive call to iloc/loc/etc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'not applicable'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_slice_axis\u001b[0;34m(self, slice_obj, axis)\u001b[0m\n\u001b[1;32m   2178\u001b[0m         \u001b[0mslice_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_slice_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iloc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_slice\u001b[0;34m(self, obj, axis, kind)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_slice\u001b[0;34m(self, slobj, axis, kind)\u001b[0m\n\u001b[1;32m    861\u001b[0m         slobj = self.index._convert_slice_indexer(slobj,\n\u001b[1;32m    862\u001b[0m                                                   kind=kind or 'getitem')\n\u001b[0;32m--> 863\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_values\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             return self._constructor(self._data.get_slice(indexer),\n\u001b[0;32m--> 978\u001b[0;31m                                      fastpath=True).__finalize__(self)\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5087\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5088\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5089\u001b[0;31m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5090\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5091\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mname\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set size\n",
    "#from bayes_opt import BayesianOptimization\n",
    "import datetime\n",
    "\n",
    "All_pred = np.empty([2000, 272])\n",
    "All_Y = np.empty([2000, 272])\n",
    "\n",
    "training = 20000\n",
    "validation = 2000\n",
    "test = 2000 #24*90\n",
    "skip1 = 20000 + 2000 - training - validation # totally to make the testing dataset the same\n",
    "freq_max = 24\n",
    "#gcn_corr_eval(7, 0.01, 0.5, 100, 0.4, 10, 5, 5, 0.2, 50, 500)\n",
    "\n",
    "step = 0\n",
    "gap = 2000\n",
    "\n",
    "total_sn = 0\n",
    "num_iter = 50\n",
    "init_points = 200\n",
    "rep = 1000\n",
    "\n",
    "# stdbscan\n",
    "spatial_threshold = 300\n",
    "temporal_threshold = 300\n",
    "min_neighbors = 1 # number of neighbor\n",
    "\n",
    "frequency2 = skip1 + freq_max + training\n",
    "\n",
    "while step < 2000:\n",
    "    \n",
    "    #hourly_bike_gps_tem_index = hourly_bike_gps['index1']\n",
    "    #hourly_bike_gps_tem = hourly_bike_gps.iloc[:, np.r_[skip1:frequency2, 26304:26306]] # past frequency2 steps and GPS locations\n",
    "    # 0 index; 401:403 gps locations; ((i - frequency) + 1):(i + 1), temporal features\n",
    "    #print (daily_bike_gps_tem)\n",
    "    #df, cluster_num, station_in_cluster = ST_DBSCAN(hourly_bike_gps_tem, spatial_threshold, temporal_threshold, min_neighbors)\n",
    "\n",
    "    #for c in range(cluster_num):\n",
    "    #c1 = df[df['cluster'] == (c+1)].index.tolist()\n",
    "    #sn = len(c1)\n",
    "    #print (\"this cluster size is: \", sn)\n",
    "    #hourly_bike_cluster = hourly_bike[c1]\n",
    "    #hourly_bike_cluster.shape\n",
    "\n",
    "    sn = 272\n",
    "    hourly_bike_cluster = hourly_bike\n",
    "    '''\n",
    "    gcnBO = BayesianOptimization(gcn_corr_eval, \n",
    "                                 {'frequency': (24, 24),\n",
    "                                  'learning_rate': (0.01, 0.05),\n",
    "                                    'decay': (0.3, 0.7),\n",
    "                                    'batch_size': (80, 120),\n",
    "                                    'n_hidden_vec1': (3, 10),\n",
    "                                    'n_hidden_vec2': (3, 10),\n",
    "                                    'n_hidden_vec3': (5, 5),\n",
    "                                    'keep': (0.8, 1),\n",
    "                                    'early_stop_th': (30, 70),\n",
    "                                    'training_epochs': (500, 500),\n",
    "                                    'reg': (0, 10)\n",
    "                                    })\n",
    "    gcnBO.maximize(init_points=init_points, n_iter=num_iter)\n",
    "    a=gcnBO.res['max']['max_params']\n",
    "    '''\n",
    "    best = -10000\n",
    "    pre_best = []\n",
    "    test_Y_best = []\n",
    "    test_error_best = 1000\n",
    "    A1_best = []\n",
    "   # A2_best = []\n",
    "    for i in range(rep):\n",
    "        a = datetime.datetime.now()\n",
    "        val_error, predic_res, test_Y,test_error, A1=gcn_corr_final(24, 0.005, 0.9, 100, 40, 40, 5, 1, 20, 500, 0, 0)\n",
    "        #val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\n",
    "        print (\"finished A running: \", i)\n",
    "        b = datetime.datetime.now()\n",
    "        print(b-a)\n",
    "        if val_error > best:\n",
    "            best = val_error\n",
    "            pre_best = predic_res\n",
    "            test_Y_best = test_Y\n",
    "            test_error_best = test_error\n",
    "            A1_best = A1\n",
    "           # A2_best = A2\n",
    "\n",
    "    #val_error, predic_res, test_Y,test_error=gcn_corr_final(24, 0.02, 0.2, 100, 5, 10, 10, 0.8, 50, 500)\n",
    "    All_pred[step:(step+gap), total_sn:(total_sn+sn)] = pre_best\n",
    "    All_Y[step:(step+gap), total_sn:(total_sn+sn)]  = test_Y_best\n",
    "\n",
    "    total_sn = total_sn + sn\n",
    "\n",
    "    total_error = np.sqrt(np.mean((All_pred[0:(step+gap),0:total_sn] - All_Y[0:(step+gap),0:total_sn])**2))\n",
    "\n",
    "    #print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the cluster now is: \", c)\n",
    "    print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the val error of this cluster now is: \", best)\n",
    "    print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the test error by this cluster now is: \", total_error)\n",
    "    '''\n",
    "    c1 = df[df['cluster'] == -999999].index.tolist()\n",
    "    sn = len(c1)\n",
    "    print (\"this cluster size is: \", sn)\n",
    "    hourly_bike_cluster = hourly_bike[c1]\n",
    "    #hourly_bike_cluster.shape\n",
    "\n",
    "    #sn = 272\n",
    "    #hourly_bike_cluster = hourly_bike\n",
    "    \n",
    "    if sn != 0:\n",
    "        \n",
    "        gcnBO = BayesianOptimization(gcn_corr_eval, \n",
    "                                     {'frequency': (10, 24),\n",
    "                                      'learning_rate': (0.01, 0.05),\n",
    "                                        'decay': (0.1, 1),\n",
    "                                        'batch_size': (50, 200),\n",
    "                                        'n_hidden_vec1': (3, 20),\n",
    "                                        'n_hidden_vec2': (5, 5),\n",
    "                                        'n_hidden_vec3': (5, 5),\n",
    "                                        'keep': (0.001, 1),\n",
    "                                        'early_stop_th': (50, 500),\n",
    "                                        'training_epochs': (200, 1000),\n",
    "                                        'reg': (0, 100)\n",
    "                                        })\n",
    "        gcnBO.maximize(init_points=init_points, n_iter=num_iter)\n",
    "        a=gcnBO.res['max']['max_params']\n",
    "        \n",
    "        best = -10000\n",
    "        pre_best = []\n",
    "        test_Y_best = []\n",
    "        test_error_best = 1000\n",
    "        for i in range(rep):\n",
    "            #val_error, predic_res, test_Y,test_error=gcn_corr_final(24, 0.05, 0.5, 100, 5, 5, 5, 0.9, 50, 500, 0.5)\n",
    "            val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\n",
    "            print (\"finished A running: \", i)\n",
    "            if val_error > best:\n",
    "                best = val_error\n",
    "                pre_best = predic_res\n",
    "                test_Y_best = test_Y\n",
    "                test_error_best = test_error\n",
    "\n",
    "        #val_error, predic_res, test_Y,test_error=gcn_corr_final(24, 0.02, 0.2, 100, 5, 10, 10, 0.8, 50, 500)\n",
    "        All_pred[step:(step+gap), total_sn:(total_sn+sn)] = pre_best\n",
    "        All_Y[step:(step+gap), total_sn:(total_sn+sn)]  = test_Y_best\n",
    "\n",
    "        total_sn = total_sn + sn\n",
    "\n",
    "        total_error = np.sqrt(np.mean((All_pred[0:(step+gap),0:total_sn] - All_Y[0:(step+gap),0:total_sn])**2))\n",
    "\n",
    "        print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the cluster now is: \", -999999)\n",
    "        print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the val error of this cluster now is: \", best)\n",
    "        print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the test error by this cluster now is: \", total_error)\n",
    "    '''\n",
    "    step = step + gap\n",
    "    skip1 = skip1 + gap\n",
    "    \n",
    "    #np.savetxt(\"prediction_300_300.csv\", All_pred, delimiter = ',')\n",
    "    #np.savetxt(\"prediction_Y_300_300.csv\", All_Y, delimiter = ',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = int(frequency) #\n",
    "\n",
    "i = frequency\n",
    "X_whole = np.zeros(shape = (hourly_bike_cluster.shape[0] - frequency, sn*frequency), dtype = np.float)\n",
    "Y_whole = np.zeros(shape = (hourly_bike_cluster.shape[0] - frequency, sn), dtype = np.float)\n",
    "\n",
    "while i < hourly_bike_cluster.shape[0]:\n",
    "    X_whole[i - frequency, ] = hourly_bike_cluster.iloc[(i - frequency):i, 0:sn].values.flatten() # flatten by row 0, 1, 2...7\n",
    "    Y_whole[i - frequency, ] = hourly_bike_cluster.iloc[i, 0:sn]\n",
    "    i = i + 1\n",
    "    #print (i)\n",
    "\n",
    "skip = skip1 + freq_max - frequency # to make sure the testing datasets are the same although the frequency could be different\n",
    "\n",
    "X_training = X_whole[skip: skip+ training, :]\n",
    "Y_training = Y_whole[skip: skip+ training, :]\n",
    "#A_training = A_whole[0: 0+ training, :]\n",
    "\n",
    "X_val = X_whole[skip+training:skip+training+validation, :]\n",
    "Y_val = Y_whole[skip+training:skip+training+validation, :]\n",
    "#A_val = A_whole[0+training:0+training+validation, :]\n",
    "\n",
    "X_test = X_whole[skip+training+validation:skip+training+validation+test, :]\n",
    "Y_test = Y_whole[skip+training+validation:skip+training+validation+test, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
