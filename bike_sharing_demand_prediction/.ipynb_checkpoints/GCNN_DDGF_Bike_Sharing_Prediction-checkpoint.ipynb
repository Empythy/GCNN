{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "import datetime\n",
    "\n",
    "from utils import normalize_adj, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def gcn(signal_in, weights_hidden, weights_A, biases, hidden_num):\n",
    "    \n",
    "    signal_in = tf.transpose(signal_in, [1, 0, 2]) # node_num, ?batch, feature_in\n",
    "    feature_len = signal_in.shape[2] # feature vector length at the node of the input graph\n",
    "    \n",
    "    i = 0\n",
    "    while i < hidden_num:\n",
    "        \n",
    "        signal_in = tf.reshape(signal_in, [node_num, -1]) # node_num, batch*feature_in\n",
    "        \n",
    "        Adj = 0.5*(weights_A['A'+str(i)] + tf.transpose(weights_A['A'+str(i)]))\n",
    "        Adj = normalize_adj(Adj)\n",
    "        Z = tf.matmul(Adj, signal_in) # node_num, batch*feature_in \n",
    "        \n",
    "        Z = tf.reshape(Z, [-1, int(feature_len)]) # node_num * batch, feature_in\n",
    "        signal_output = tf.add(tf.matmul(Z, weights_hidden['h'+str(i)]), biases['b'+str(i)])\n",
    "        signal_output = tf.nn.relu(signal_output) # node_num * batch, hidden_vec\n",
    "        \n",
    "        i += 1\n",
    "        signal_in = signal_output # the sinal for next layer \n",
    "        feature_len = signal_in.shape[1] # feature vector length at hidden layers\n",
    "    \n",
    "    final_output = tf.add(tf.matmul(signal_output, weights_hidden['out']), biases['bout'])  # node_num * batch, horizon\n",
    "    final_output = tf.reshape(final_output, [node_num, -1, horizon]) # node_num, batch, horizon\n",
    "    final_output = tf.transpose(final_output, [1, 0, 2]) # batch, node_num, horizon\n",
    "    final_output = tf.reshape(final_output, [-1, node_num*horizon]) # batch, node_num*horizon\n",
    " \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_corr_final(feature_num_layer, horizon, learning_rate, decay, batch_size, keep, early_stop_th, training_epochs):\n",
    "   \n",
    "    feature_in = feature_num_layer[0] \n",
    "    \n",
    "    early_stop_k = 0 # early stop patience\n",
    "    display_step = 1 # frequency of printing results\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    predic_res = []\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    batch_size = batch_size\n",
    "    early_stop_th = early_stop_th\n",
    "    training_epochs = training_epochs\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None, node_num, feature_in]) # X is the input signal\n",
    "    Y = tf.placeholder(tf.float32, [None, n_output_vec]) # y is the regression output\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "    # define dictionaries to store layers weight & bias\n",
    "    i = 0\n",
    "    weights_hidden = {}\n",
    "    weights_A = {}\n",
    "    biases = {}\n",
    "    while i < len(feature_num_layer) - 1:\n",
    "        weights_hidden['h'+str(i)] = tf.Variable(tf.random_normal([feature_num_layer[i], feature_num_layer[i + 1]]))\n",
    "        biases['b'+str(i)] = tf.Variable(tf.random_normal([1, feature_num_layer[i + 1]]))\n",
    "        weights_A['A'+str(i)] = tf.Variable(tf.random_normal([node_num, node_num]))\n",
    "        i += 1\n",
    "    \n",
    "    weights_hidden['out'] = tf.Variable(tf.random_normal([feature_num_layer[-1], horizon]))\n",
    "    biases['bout'] = tf.Variable(tf.random_normal([1, horizon]))\n",
    " \n",
    "    # Construct model\n",
    "    hidden_num = len(feature_num_layer) - 1\n",
    "    pred= gcn(X, weights_hidden, weights_A, biases, hidden_num)\n",
    "    pred = scaler.inverse_transform(pred)\n",
    "    Y_true_tr = scaler.inverse_transform(Y)\n",
    "    cost = tf.reduce_mean(tf.pow(pred - Y_true_tr, 2)) \n",
    "\n",
    "    pred_val= gcn(X, weights_hidden, weights_A, biases, hidden_num)\n",
    "    pred_val = scaler.inverse_transform(pred_val)\n",
    "    Y_true_val = scaler.inverse_transform(Y)\n",
    "    cost_val =  tf.reduce_mean(tf.pow(pred_val - Y_true_val, 2)) \n",
    "\n",
    "    pred_tes= gcn(X, weights_hidden, weights_A, biases, hidden_num)\n",
    "    pred_tes = scaler.inverse_transform(pred_tes)\n",
    "    Y_true_tes = scaler.inverse_transform(Y)\n",
    "    cost_tes = tf.reduce_mean(tf.pow(pred_tes - Y_true_tes, 2)) \n",
    "                                         \n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay).minimize(cost)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(num_train/batch_size)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                \n",
    "                _, c = sess.run([optimizer, cost], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                      Y: Y_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                              keep_prob: keep})\n",
    "\n",
    "                avg_cost += c * batch_size #/ total_batch \n",
    "                \n",
    "            # rest part of training dataset\n",
    "            if total_batch * batch_size != num_train:\n",
    "                _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[total_batch*batch_size:num_train,], \n",
    "                                          Y: Y_training[total_batch*batch_size:num_train,],\n",
    "                                                  keep_prob: keep})\n",
    "                avg_cost += c * (num_train - total_batch*batch_size)\n",
    "            \n",
    "            avg_cost = np.sqrt(avg_cost / num_train)\n",
    "            #Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Epoch:\", '%04d' % (epoch+1), \"Training RMSE=\", \\\n",
    "                    \"{:.9f}\".format(avg_cost))\n",
    "            # validation\n",
    "            c_val = sess.run([cost_val], feed_dict={X: X_val, Y: Y_val,  keep_prob:1})\n",
    "            c_val = np.sqrt(c_val[0])\n",
    "            print(\"Validation RMSE: \", c_val)\n",
    "            # testing\n",
    "            c_tes, preds, Y_true = sess.run([cost_tes, pred_tes, Y_true_tes], feed_dict={X: X_test,Y: Y_test, keep_prob: 1})\n",
    "            c_tes = np.sqrt(c_tes)\n",
    "\n",
    "            if c_val < best_val:\n",
    "                best_val = c_val\n",
    "                # save model\n",
    "                #saver.save(sess, './bikesharing_gcnn_ddgf')\n",
    "                test_error = c_tes\n",
    "                traing_error = avg_cost\n",
    "                predic_res = preds\n",
    "                early_stop_k = 0 # reset to 0\n",
    "\n",
    "            # update early stopping patience\n",
    "            if c_val >= best_val:\n",
    "                early_stop_k += 1\n",
    "\n",
    "            # threshold\n",
    "            if early_stop_k == early_stop_th:\n",
    "                break\n",
    "            \n",
    "\n",
    "        print(\"epoch is \", epoch)\n",
    "        print(\"training RMSE is \", traing_error)\n",
    "        print(\"Optimization Finished! the lowest validation RMSE is \", best_val)\n",
    "        print(\"The test RMSE is \", test_error)\n",
    "    \n",
    "    #test_Y = Y_test\n",
    "    #test_error = np.sqrt(test_error)\n",
    "    return best_val, predic_res,Y_true,test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"data/NYCBikeHourly272.pickle\"\n",
    "fileObject = open(file_Name,'rb') \n",
    "hourly_bike = pickle.load(fileObject) \n",
    "hourly_bike = pd.DataFrame(hourly_bike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Training, Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_num = 272 # station number \n",
    "feature_in = 24 # number of features at each node, e.g., bike sharing demand from past 24 hours\n",
    "horizon = 1 # the length to predict, e.g., predict the future one hour bike sharing demand\n",
    "\n",
    "X_whole = []\n",
    "Y_whole = []\n",
    "\n",
    "x_offsets = np.sort(\n",
    "    np.concatenate((np.arange(-feature_in+1, 1, 1),))\n",
    ")\n",
    "\n",
    "y_offsets = np.sort(np.arange(1, 1+ horizon, 1))\n",
    "\n",
    "min_t = abs(min(x_offsets))\n",
    "max_t = abs(hourly_bike.shape[0] - abs(max(y_offsets)))  # Exclusive\n",
    "for t in range(min_t, max_t):\n",
    "    x_t = hourly_bike.iloc[t + x_offsets, 0:node_num].values.flatten('F')\n",
    "    y_t = hourly_bike.iloc[t + y_offsets, 0:node_num].values.flatten('F')\n",
    "    X_whole.append(x_t)\n",
    "    Y_whole.append(y_t)\n",
    "\n",
    "X_whole = np.stack(X_whole, axis=0)\n",
    "Y_whole = np.stack(Y_whole, axis=0)\n",
    "\n",
    "n_input_vec = X_whole.shape[1] # e.g., 272 * 24\n",
    "n_output_vec = Y_whole.shape[1] # each row represent a result\n",
    "\n",
    "\n",
    "X_whole = np.reshape(X_whole, [X_whole.shape[0], node_num, feature_in])\n",
    "num_samples = X_whole.shape[0]\n",
    "num_train = 20000 # Note here actually we use the first 20000 to train the model. The paper mentioned \"22304\" need to be corrected.\n",
    "num_val = 2000\n",
    "num_test = 2000\n",
    "\n",
    "X_training = X_whole[:num_train, :]\n",
    "Y_training = Y_whole[:num_train, :]\n",
    "\n",
    "# shuffle the training dataset\n",
    "perm = np.arange(X_training.shape[0])\n",
    "np.random.shuffle(perm)\n",
    "X_training = X_training[perm]\n",
    "Y_training = Y_training[perm]\n",
    "\n",
    "X_val = X_whole[num_train:num_train+num_val, :]\n",
    "Y_val = Y_whole[num_train:num_train+num_val, :]\n",
    "\n",
    "X_test = X_whole[num_train+num_val:num_train+num_val+num_test, :]\n",
    "Y_test = Y_whole[num_train+num_val:num_train+num_val+num_test, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(mean=X_training.mean(), std=X_training.std())\n",
    "\n",
    "X_training = scaler.transform(X_training)\n",
    "Y_training = scaler.transform(Y_training)\n",
    "\n",
    "X_val = scaler.transform(X_val)\n",
    "Y_val = scaler.transform(Y_val)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "Y_test = scaler.transform(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # learning rate\n",
    "decay = 0.9\n",
    "batchsize = 100 # batch size \n",
    "\n",
    "feature_num_layer = [24, 10, 10, 5] # determine the number of hidden layers and the vector length at each node of each hidden layer\n",
    "\n",
    "keep = 1 # drop out probability\n",
    "\n",
    "early_stop_th = 20 # early stopping threshold, if validation RMSE not dropping in continuous 20 steps, break\n",
    "training_epochs = 200 # total training epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training RMSE= 48.557281030\n",
      "Validation RMSE:  5.630722\n",
      "Epoch: 0002 Training RMSE= 4.829149038\n",
      "Validation RMSE:  4.81646\n",
      "Epoch: 0003 Training RMSE= 4.005378480\n",
      "Validation RMSE:  3.9481895\n",
      "Epoch: 0004 Training RMSE= 3.678251639\n",
      "Validation RMSE:  3.9025466\n",
      "Epoch: 0005 Training RMSE= 3.601050654\n",
      "Validation RMSE:  3.7120907\n",
      "Epoch: 0006 Training RMSE= 3.546212844\n",
      "Validation RMSE:  3.6575234\n",
      "Epoch: 0007 Training RMSE= 3.480819403\n",
      "Validation RMSE:  3.5907094\n",
      "Epoch: 0008 Training RMSE= 3.392607091\n",
      "Validation RMSE:  3.5773451\n",
      "Epoch: 0009 Training RMSE= 3.323543121\n",
      "Validation RMSE:  3.5701938\n",
      "Epoch: 0010 Training RMSE= 3.274765211\n",
      "Validation RMSE:  3.5582807\n",
      "Epoch: 0011 Training RMSE= 3.242607740\n",
      "Validation RMSE:  3.5466013\n",
      "Epoch: 0012 Training RMSE= 3.215577654\n",
      "Validation RMSE:  3.5389512\n",
      "Epoch: 0013 Training RMSE= 3.192793006\n",
      "Validation RMSE:  3.5263317\n",
      "Epoch: 0014 Training RMSE= 3.172752166\n",
      "Validation RMSE:  3.5018847\n",
      "Epoch: 0015 Training RMSE= 3.155356977\n",
      "Validation RMSE:  3.4651306\n",
      "Epoch: 0016 Training RMSE= 3.138456535\n",
      "Validation RMSE:  3.4581347\n",
      "Epoch: 0017 Training RMSE= 3.123011367\n",
      "Validation RMSE:  3.4507926\n",
      "Epoch: 0018 Training RMSE= 3.108264607\n",
      "Validation RMSE:  3.4710083\n",
      "Epoch: 0019 Training RMSE= 3.096460292\n",
      "Validation RMSE:  3.449633\n",
      "Epoch: 0020 Training RMSE= 3.083978829\n",
      "Validation RMSE:  3.4408383\n",
      "Epoch: 0021 Training RMSE= 3.072635441\n",
      "Validation RMSE:  3.4262435\n",
      "Epoch: 0022 Training RMSE= 3.061888814\n",
      "Validation RMSE:  3.3895946\n",
      "Epoch: 0023 Training RMSE= 3.052766753\n",
      "Validation RMSE:  3.3896427\n",
      "Epoch: 0024 Training RMSE= 3.042247844\n",
      "Validation RMSE:  3.358781\n",
      "Epoch: 0025 Training RMSE= 3.031637114\n",
      "Validation RMSE:  3.3476112\n",
      "Epoch: 0026 Training RMSE= 3.020915873\n",
      "Validation RMSE:  3.3387737\n",
      "Epoch: 0027 Training RMSE= 3.013526303\n",
      "Validation RMSE:  3.3358598\n",
      "Epoch: 0028 Training RMSE= 3.006003840\n",
      "Validation RMSE:  3.324596\n",
      "Epoch: 0029 Training RMSE= 2.998324543\n",
      "Validation RMSE:  3.318261\n",
      "Epoch: 0030 Training RMSE= 2.991055325\n",
      "Validation RMSE:  3.3141413\n",
      "Epoch: 0031 Training RMSE= 2.984302505\n",
      "Validation RMSE:  3.313014\n",
      "Epoch: 0032 Training RMSE= 2.977721984\n",
      "Validation RMSE:  3.3080132\n",
      "Epoch: 0033 Training RMSE= 2.971963914\n",
      "Validation RMSE:  3.2967243\n",
      "Epoch: 0034 Training RMSE= 2.965223827\n",
      "Validation RMSE:  3.292479\n",
      "Epoch: 0035 Training RMSE= 2.958406910\n",
      "Validation RMSE:  3.2898712\n",
      "Epoch: 0036 Training RMSE= 2.951447912\n",
      "Validation RMSE:  3.2844453\n",
      "Epoch: 0037 Training RMSE= 2.945010012\n",
      "Validation RMSE:  3.2776854\n",
      "Epoch: 0038 Training RMSE= 2.938354605\n",
      "Validation RMSE:  3.2727826\n",
      "Epoch: 0039 Training RMSE= 2.932512646\n",
      "Validation RMSE:  3.2679765\n",
      "Epoch: 0040 Training RMSE= 2.926908742\n",
      "Validation RMSE:  3.264231\n",
      "Epoch: 0041 Training RMSE= 2.920755212\n",
      "Validation RMSE:  3.2568066\n",
      "Epoch: 0042 Training RMSE= 2.915251135\n",
      "Validation RMSE:  3.2514937\n",
      "Epoch: 0043 Training RMSE= 2.909834442\n",
      "Validation RMSE:  3.246007\n",
      "Epoch: 0044 Training RMSE= 2.904356895\n",
      "Validation RMSE:  3.2408354\n",
      "Epoch: 0045 Training RMSE= 2.898831426\n",
      "Validation RMSE:  3.235977\n",
      "Epoch: 0046 Training RMSE= 2.892810522\n",
      "Validation RMSE:  3.2304232\n",
      "Epoch: 0047 Training RMSE= 2.888906574\n",
      "Validation RMSE:  3.2236006\n",
      "Epoch: 0048 Training RMSE= 2.884849158\n",
      "Validation RMSE:  3.219523\n",
      "Epoch: 0049 Training RMSE= 2.880744509\n",
      "Validation RMSE:  3.2165246\n",
      "Epoch: 0050 Training RMSE= 2.876492491\n",
      "Validation RMSE:  3.2154646\n",
      "Epoch: 0051 Training RMSE= 2.872414860\n",
      "Validation RMSE:  3.2124977\n",
      "Epoch: 0052 Training RMSE= 2.868640940\n",
      "Validation RMSE:  3.2108793\n",
      "Epoch: 0053 Training RMSE= 2.865147890\n",
      "Validation RMSE:  3.208256\n",
      "Epoch: 0054 Training RMSE= 2.861900369\n",
      "Validation RMSE:  3.207149\n",
      "Epoch: 0055 Training RMSE= 2.858547960\n",
      "Validation RMSE:  3.2045505\n",
      "Epoch: 0056 Training RMSE= 2.855328239\n",
      "Validation RMSE:  3.202697\n",
      "Epoch: 0057 Training RMSE= 2.852484980\n",
      "Validation RMSE:  3.1991532\n",
      "Epoch: 0058 Training RMSE= 2.849621147\n",
      "Validation RMSE:  3.197242\n",
      "Epoch: 0059 Training RMSE= 2.847003065\n",
      "Validation RMSE:  3.1941\n",
      "Epoch: 0060 Training RMSE= 2.844308181\n",
      "Validation RMSE:  3.1912892\n",
      "Epoch: 0061 Training RMSE= 2.841676730\n",
      "Validation RMSE:  3.187931\n",
      "Epoch: 0062 Training RMSE= 2.838976975\n",
      "Validation RMSE:  3.1852574\n",
      "Epoch: 0063 Training RMSE= 2.836581921\n",
      "Validation RMSE:  3.1834433\n",
      "Epoch: 0064 Training RMSE= 2.834168569\n",
      "Validation RMSE:  3.1814246\n",
      "Epoch: 0065 Training RMSE= 2.831853863\n",
      "Validation RMSE:  3.1789875\n",
      "Epoch: 0066 Training RMSE= 2.829649109\n",
      "Validation RMSE:  3.1770086\n",
      "Epoch: 0067 Training RMSE= 2.827398730\n",
      "Validation RMSE:  3.1750836\n",
      "Epoch: 0068 Training RMSE= 2.825167805\n",
      "Validation RMSE:  3.172544\n",
      "Epoch: 0069 Training RMSE= 2.822961196\n",
      "Validation RMSE:  3.1706066\n",
      "Epoch: 0070 Training RMSE= 2.821003037\n",
      "Validation RMSE:  3.169193\n",
      "Epoch: 0071 Training RMSE= 2.819007263\n",
      "Validation RMSE:  3.1676888\n",
      "Epoch: 0072 Training RMSE= 2.817005805\n",
      "Validation RMSE:  3.1658432\n",
      "Epoch: 0073 Training RMSE= 2.814984229\n",
      "Validation RMSE:  3.1645272\n",
      "Epoch: 0074 Training RMSE= 2.813080396\n",
      "Validation RMSE:  3.1626475\n",
      "Epoch: 0075 Training RMSE= 2.811222915\n",
      "Validation RMSE:  3.1611104\n",
      "Epoch: 0076 Training RMSE= 2.809334797\n",
      "Validation RMSE:  3.1595414\n",
      "Epoch: 0077 Training RMSE= 2.807471874\n",
      "Validation RMSE:  3.1577818\n",
      "Epoch: 0078 Training RMSE= 2.805585894\n",
      "Validation RMSE:  3.1569247\n",
      "Epoch: 0079 Training RMSE= 2.803794280\n",
      "Validation RMSE:  3.1544962\n",
      "Epoch: 0080 Training RMSE= 2.801918077\n",
      "Validation RMSE:  3.1523902\n",
      "Epoch: 0081 Training RMSE= 2.799952270\n",
      "Validation RMSE:  3.1492345\n",
      "Epoch: 0082 Training RMSE= 2.798167190\n",
      "Validation RMSE:  3.147442\n",
      "Epoch: 0083 Training RMSE= 2.796323740\n",
      "Validation RMSE:  3.1452315\n",
      "Epoch: 0084 Training RMSE= 2.794636507\n",
      "Validation RMSE:  3.1434903\n",
      "Epoch: 0085 Training RMSE= 2.792942145\n",
      "Validation RMSE:  3.1414897\n",
      "Epoch: 0086 Training RMSE= 2.791153117\n",
      "Validation RMSE:  3.1391938\n",
      "Epoch: 0087 Training RMSE= 2.789674048\n",
      "Validation RMSE:  3.138059\n",
      "Epoch: 0088 Training RMSE= 2.788035139\n",
      "Validation RMSE:  3.1350877\n",
      "Epoch: 0089 Training RMSE= 2.786500862\n",
      "Validation RMSE:  3.1324444\n",
      "Epoch: 0090 Training RMSE= 2.785006420\n",
      "Validation RMSE:  3.1305327\n",
      "Epoch: 0091 Training RMSE= 2.783462113\n",
      "Validation RMSE:  3.1291418\n",
      "Epoch: 0092 Training RMSE= 2.782095291\n",
      "Validation RMSE:  3.126779\n",
      "Epoch: 0093 Training RMSE= 2.780635961\n",
      "Validation RMSE:  3.1236625\n",
      "Epoch: 0094 Training RMSE= 2.779213448\n",
      "Validation RMSE:  3.121162\n",
      "Epoch: 0095 Training RMSE= 2.777767803\n",
      "Validation RMSE:  3.120041\n",
      "Epoch: 0096 Training RMSE= 2.776423544\n",
      "Validation RMSE:  3.1166434\n",
      "Epoch: 0097 Training RMSE= 2.774989385\n",
      "Validation RMSE:  3.115066\n",
      "Epoch: 0098 Training RMSE= 2.773565643\n",
      "Validation RMSE:  3.1142173\n",
      "Epoch: 0099 Training RMSE= 2.772379463\n",
      "Validation RMSE:  3.1119661\n",
      "Epoch: 0100 Training RMSE= 2.771053140\n",
      "Validation RMSE:  3.109952\n",
      "Epoch: 0101 Training RMSE= 2.769742985\n",
      "Validation RMSE:  3.1087627\n",
      "Epoch: 0102 Training RMSE= 2.768501853\n",
      "Validation RMSE:  3.1077914\n",
      "Epoch: 0103 Training RMSE= 2.767235666\n",
      "Validation RMSE:  3.1055253\n",
      "Epoch: 0104 Training RMSE= 2.765942754\n",
      "Validation RMSE:  3.1037698\n",
      "Epoch: 0105 Training RMSE= 2.764601896\n",
      "Validation RMSE:  3.1039603\n",
      "Epoch: 0106 Training RMSE= 2.763208749\n",
      "Validation RMSE:  3.1017287\n",
      "Epoch: 0107 Training RMSE= 2.761719787\n",
      "Validation RMSE:  3.1055832\n",
      "Epoch: 0108 Training RMSE= 2.760189172\n",
      "Validation RMSE:  3.105175\n",
      "Epoch: 0109 Training RMSE= 2.759889535\n",
      "Validation RMSE:  3.0994895\n",
      "Epoch: 0110 Training RMSE= 2.758534119\n",
      "Validation RMSE:  3.1088443\n",
      "Epoch: 0111 Training RMSE= 2.757754685\n",
      "Validation RMSE:  3.0960388\n",
      "Epoch: 0112 Training RMSE= 2.756305686\n",
      "Validation RMSE:  3.108768\n",
      "Epoch: 0113 Training RMSE= 2.755658134\n",
      "Validation RMSE:  3.0974245\n",
      "Epoch: 0114 Training RMSE= 2.754296750\n",
      "Validation RMSE:  3.0962772\n",
      "Epoch: 0115 Training RMSE= 2.753123648\n",
      "Validation RMSE:  3.102221\n",
      "Epoch: 0116 Training RMSE= 2.752058519\n",
      "Validation RMSE:  3.112586\n",
      "Epoch: 0117 Training RMSE= 2.751152767\n",
      "Validation RMSE:  3.1256373\n",
      "Epoch: 0118 Training RMSE= 2.750420489\n",
      "Validation RMSE:  3.131724\n",
      "Epoch: 0119 Training RMSE= 2.749029400\n",
      "Validation RMSE:  3.1526675\n",
      "Epoch: 0120 Training RMSE= 2.748217906\n",
      "Validation RMSE:  3.1018052\n",
      "Epoch: 0121 Training RMSE= 2.748051019\n",
      "Validation RMSE:  3.1734953\n",
      "Epoch: 0122 Training RMSE= 2.747336606\n",
      "Validation RMSE:  3.0908473\n",
      "Epoch: 0123 Training RMSE= 2.744973800\n",
      "Validation RMSE:  3.1388721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0124 Training RMSE= 2.745292222\n",
      "Validation RMSE:  3.0970426\n",
      "Epoch: 0125 Training RMSE= 2.743428698\n",
      "Validation RMSE:  3.122601\n",
      "Epoch: 0126 Training RMSE= 2.742251469\n",
      "Validation RMSE:  3.1342633\n",
      "Epoch: 0127 Training RMSE= 2.742409456\n",
      "Validation RMSE:  3.2147627\n",
      "Epoch: 0128 Training RMSE= 2.740608287\n",
      "Validation RMSE:  3.1226804\n",
      "Epoch: 0129 Training RMSE= 2.740537530\n",
      "Validation RMSE:  3.1276197\n",
      "Epoch: 0130 Training RMSE= 2.739202962\n",
      "Validation RMSE:  3.1025262\n",
      "Epoch: 0131 Training RMSE= 2.738527566\n",
      "Validation RMSE:  3.1168916\n",
      "Epoch: 0132 Training RMSE= 2.736965254\n",
      "Validation RMSE:  3.093938\n",
      "Epoch: 0133 Training RMSE= 2.736964103\n",
      "Validation RMSE:  3.152928\n",
      "Epoch: 0134 Training RMSE= 2.735164161\n",
      "Validation RMSE:  3.082569\n",
      "Epoch: 0135 Training RMSE= 2.735844716\n",
      "Validation RMSE:  3.1985042\n",
      "Epoch: 0136 Training RMSE= 2.733857686\n",
      "Validation RMSE:  3.1891146\n",
      "Epoch: 0137 Training RMSE= 2.732535932\n",
      "Validation RMSE:  3.1956096\n",
      "Epoch: 0138 Training RMSE= 2.731338305\n",
      "Validation RMSE:  3.1240535\n",
      "Epoch: 0139 Training RMSE= 2.730652948\n",
      "Validation RMSE:  3.1086993\n",
      "Epoch: 0140 Training RMSE= 2.730130028\n",
      "Validation RMSE:  3.1253705\n",
      "Epoch: 0141 Training RMSE= 2.729503387\n",
      "Validation RMSE:  3.1145947\n",
      "Epoch: 0142 Training RMSE= 2.728547908\n",
      "Validation RMSE:  3.0771775\n",
      "Epoch: 0143 Training RMSE= 2.729363852\n",
      "Validation RMSE:  3.087316\n",
      "Epoch: 0144 Training RMSE= 2.727306906\n",
      "Validation RMSE:  3.0770314\n",
      "Epoch: 0145 Training RMSE= 2.725892364\n",
      "Validation RMSE:  3.1028516\n",
      "Epoch: 0146 Training RMSE= 2.726006584\n",
      "Validation RMSE:  3.0749795\n",
      "Epoch: 0147 Training RMSE= 2.724139295\n",
      "Validation RMSE:  3.1104212\n",
      "Epoch: 0148 Training RMSE= 2.724466171\n",
      "Validation RMSE:  3.0832312\n",
      "Epoch: 0149 Training RMSE= 2.723254693\n",
      "Validation RMSE:  3.0729065\n",
      "Epoch: 0150 Training RMSE= 2.722550958\n",
      "Validation RMSE:  3.0557213\n",
      "Epoch: 0151 Training RMSE= 2.721723851\n",
      "Validation RMSE:  3.049203\n",
      "Epoch: 0152 Training RMSE= 2.720891055\n",
      "Validation RMSE:  3.0523667\n",
      "Epoch: 0153 Training RMSE= 2.719585268\n",
      "Validation RMSE:  3.0700502\n",
      "Epoch: 0154 Training RMSE= 2.718726124\n",
      "Validation RMSE:  3.0999787\n",
      "Epoch: 0155 Training RMSE= 2.718381673\n",
      "Validation RMSE:  3.097309\n",
      "Epoch: 0156 Training RMSE= 2.717531163\n",
      "Validation RMSE:  3.0640085\n",
      "Epoch: 0157 Training RMSE= 2.717244398\n",
      "Validation RMSE:  3.0929508\n",
      "Epoch: 0158 Training RMSE= 2.716078961\n",
      "Validation RMSE:  3.0948632\n",
      "Epoch: 0159 Training RMSE= 2.715155141\n",
      "Validation RMSE:  3.091041\n",
      "Epoch: 0160 Training RMSE= 2.715770466\n",
      "Validation RMSE:  3.0531657\n",
      "Epoch: 0161 Training RMSE= 2.713735030\n",
      "Validation RMSE:  3.0721421\n",
      "Epoch: 0162 Training RMSE= 2.712529397\n",
      "Validation RMSE:  3.0626092\n",
      "Epoch: 0163 Training RMSE= 2.712902797\n",
      "Validation RMSE:  3.0670056\n",
      "Epoch: 0164 Training RMSE= 2.712161677\n",
      "Validation RMSE:  3.0624578\n",
      "Epoch: 0165 Training RMSE= 2.711985092\n",
      "Validation RMSE:  3.0874941\n",
      "Epoch: 0166 Training RMSE= 2.710078085\n",
      "Validation RMSE:  3.058143\n",
      "Epoch: 0167 Training RMSE= 2.710884507\n",
      "Validation RMSE:  3.0472054\n",
      "Epoch: 0168 Training RMSE= 2.709333451\n",
      "Validation RMSE:  3.038636\n",
      "Epoch: 0169 Training RMSE= 2.707435954\n",
      "Validation RMSE:  3.1724632\n",
      "Epoch: 0170 Training RMSE= 2.707686686\n",
      "Validation RMSE:  3.0841734\n",
      "Epoch: 0171 Training RMSE= 2.707728486\n",
      "Validation RMSE:  3.0634072\n",
      "Epoch: 0172 Training RMSE= 2.705782352\n",
      "Validation RMSE:  3.0670936\n",
      "Epoch: 0173 Training RMSE= 2.705563032\n",
      "Validation RMSE:  3.0405009\n",
      "Epoch: 0174 Training RMSE= 2.704547906\n",
      "Validation RMSE:  3.0715234\n",
      "Epoch: 0175 Training RMSE= 2.705248111\n",
      "Validation RMSE:  3.112306\n",
      "Epoch: 0176 Training RMSE= 2.703174127\n",
      "Validation RMSE:  3.1579602\n",
      "Epoch: 0177 Training RMSE= 2.703332953\n",
      "Validation RMSE:  3.1407351\n",
      "Epoch: 0178 Training RMSE= 2.703591055\n",
      "Validation RMSE:  3.0318325\n",
      "Epoch: 0179 Training RMSE= 2.700498134\n",
      "Validation RMSE:  3.0696788\n",
      "Epoch: 0180 Training RMSE= 2.703303477\n",
      "Validation RMSE:  3.0410414\n",
      "Epoch: 0181 Training RMSE= 2.700334478\n",
      "Validation RMSE:  3.04459\n",
      "Epoch: 0182 Training RMSE= 2.700542253\n",
      "Validation RMSE:  3.0694532\n",
      "Epoch: 0183 Training RMSE= 2.699841335\n",
      "Validation RMSE:  3.044781\n",
      "Epoch: 0184 Training RMSE= 2.698322872\n",
      "Validation RMSE:  3.0495193\n",
      "Epoch: 0185 Training RMSE= 2.699200740\n",
      "Validation RMSE:  3.0636468\n",
      "Epoch: 0186 Training RMSE= 2.698320505\n",
      "Validation RMSE:  3.0453367\n",
      "Epoch: 0187 Training RMSE= 2.698742155\n",
      "Validation RMSE:  3.0419695\n",
      "Epoch: 0188 Training RMSE= 2.697117468\n",
      "Validation RMSE:  3.055547\n",
      "Epoch: 0189 Training RMSE= 2.695995641\n",
      "Validation RMSE:  3.0377266\n",
      "Epoch: 0190 Training RMSE= 2.697067500\n",
      "Validation RMSE:  3.063816\n",
      "Epoch: 0191 Training RMSE= 2.695329575\n",
      "Validation RMSE:  3.0406919\n",
      "Epoch: 0192 Training RMSE= 2.695073663\n",
      "Validation RMSE:  3.043615\n",
      "Epoch: 0193 Training RMSE= 2.694251365\n",
      "Validation RMSE:  3.0572174\n",
      "Epoch: 0194 Training RMSE= 2.693838491\n",
      "Validation RMSE:  3.0477052\n",
      "Epoch: 0195 Training RMSE= 2.694733971\n",
      "Validation RMSE:  3.1017578\n",
      "Epoch: 0196 Training RMSE= 2.693249101\n",
      "Validation RMSE:  3.102575\n",
      "Epoch: 0197 Training RMSE= 2.693456705\n",
      "Validation RMSE:  3.040456\n",
      "epoch is  196\n",
      "training RMSE is  2.703591055410974\n",
      "Optimization Finished! the lowest validation RMSE is  3.0318325\n",
      "The test RMSE is  2.3509636\n",
      "Total training time:  0:04:24.178456\n"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now()\n",
    "\n",
    "val_error, predic_res, test_Y, test_error = gcn_corr_final(feature_num_layer, horizon, learning_rate, decay, batchsize, keep, early_stop_th, training_epochs)\n",
    "\n",
    "\n",
    "b = datetime.datetime.now()\n",
    "\n",
    "print('Total training time: ', b-a)\n",
    "\n",
    "#np.savetxt(\"prediction.csv\", predic_res, delimiter = ',')\n",
    "#np.savetxt(\"prediction_Y.csv\", test_Y, delimiter = ',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
