{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"data/NYCBikeHourly272.pickle\"\n",
    "fileObject = open(file_Name,'rb') \n",
    "hourly_bike = pickle.load(fileObject) \n",
    "hourly_bike = pd.DataFrame(hourly_bike)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    #adj[np.isnan(adj)] = 0.\n",
    "    adj = tf.abs(adj)\n",
    "    rowsum = tf.reduce_sum(adj, 1)# sum by row\n",
    "\n",
    "    d_inv_sqrt = tf.pow(rowsum, -0.5)\n",
    "   \n",
    "    #d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    \n",
    "    d_mat_inv_sqrt = tf.diag(d_inv_sqrt)\n",
    "\n",
    "    return tf.matmul(tf.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def gcn(x, weights, biases, batch_size, n_input, frequency,flag):\n",
    "    # Hidden layer with RELU activation\n",
    "    \n",
    "    output_list = tf.Variable(tf.zeros([sn,1]),dtype=tf.float32) #'Tensor' object does not support item assignment, cant build Ypre\n",
    "    \n",
    "    if flag == 1:\n",
    "        rownum = batch_size\n",
    "    elif flag == 2:\n",
    "        rownum = validation\n",
    "    elif flag == 3:\n",
    "        rownum = test\n",
    "    \n",
    "    for i in range(rownum):\n",
    "        Xtem = tf.reshape(x[i,:], [frequency, n_input])\n",
    "        Xtem = tf.transpose(Xtem)\n",
    "        #Atem = tf.convert_to_tensor(A_whole_final, dtype=np.float32)\n",
    "        #Atem1 = tf.diag(tf.ones([n_input])) # Atem not palying any roles\n",
    "        #Ytem = tf.reshape(Y[i,:], [n_input, 1])\n",
    "        Atem = tf.diag(tf.ones([n_input]))\n",
    "        Atem1 = 0.5*(weights['A1'] + tf.transpose(weights['A1']))#+ Atem \n",
    "        #Atem1 = normalize_adj(Atem1)\n",
    "        #th = tf.constant(0.01, dtype=tf.float32)\n",
    "        #where = tf.subtract(Atem1, th)\n",
    "        #Atem1 = tf.nn.relu(where)\n",
    "        \n",
    "        Z1 = tf.matmul(Atem1, Xtem)\n",
    "        #layer_1 = tf.matmul(Z1, weights['h1']) \n",
    "        layer_1 = tf.add(tf.matmul(Z1, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        \n",
    "        #Atem2 = 0.5*(weights['A2'] + tf.transpose(weights['A2']))#+ Atem \n",
    "        #Atem2 = normalize_adj(Atem2)\n",
    "        \n",
    "        #Z2 = tf.matmul(Atem2, layer_1)\n",
    "        #layer_2 = tf.add(tf.matmul(Z2, weights['h2']), biases['b2'])\n",
    "        #layer_2 = tf.nn.relu(layer_2)\n",
    "        \n",
    "        #Atem3 = weights['A3']+ Atem \n",
    "        #Z3 = tf.matmul(Atem3, layer_2)\n",
    "        #layer_3 = tf.add(tf.matmul(Z3, weights['h3']), biases['b3'])\n",
    "        #layer_3 = tf.nn.relu(layer_3)\n",
    "        \n",
    "        # flattern\n",
    "        #layer_3 = tf.reshape(layer_3, [1, 272*n_hidden_vec3])\n",
    "        \n",
    "        #F1 = tf.add(tf.matmul(layer_3, weights['f1']), biases['bf1'])\n",
    "        #F1 = tf.nn.relu(F1)\n",
    "        \n",
    "        #F2 = tf.add(tf.matmul(F1, weights['f2']), biases['bf2'])\n",
    "        #F2 = tf.nn.relu(F2)\n",
    "        \n",
    "        #F3 = tf.add(tf.matmul(F1, weights['f3']), biases['bf3'])\n",
    "        #out_layer = tf.reshape(F3, [272, 1])\n",
    "\n",
    "        # Output layer with linear activation\n",
    "        Z4 = layer_1#tf.matmul(Atem1, layer_1)#tf.matmul(Atem, layer_3)\n",
    "        out_layer = tf.add(tf.matmul(Z4, weights['out']), biases['bout'])\n",
    "        \n",
    "        # weather layer 1\n",
    "        #x_wea_tem = tf.reshape(x_wea[i,:], [1, 9*frequency2]) # 1 by 126\n",
    "        #layer_1_wea = tf.add(tf.matmul(x_wea_tem, weights['h1_wea']), biases['b1_wea'])\n",
    "        #layer_1_wea = tf.nn.relu(layer_1_wea)\n",
    "        \n",
    "        #out_layer_wea = tf.add(tf.matmul(layer_1_wea, weights['out_wea']), biases['bout_wea'])\n",
    "        #out_layer = tf.add(out_layer, tf.reshape(out_layer_wea, [272, 1]))\n",
    "        \n",
    "        #print (out_layer.get_shape())\n",
    "        if i ==0:\n",
    "            output_list = out_layer\n",
    "        else:\n",
    "            output_list = tf.concat([output_list, out_layer], 1)\n",
    "        \n",
    "        #print (tf.reduce_mean(tf.pow(output_list-out_layer, 2)))\n",
    "    \n",
    "    #print (output_list.get_shape())\n",
    "    \n",
    "    output_list = tf.transpose(output_list)\n",
    "    \n",
    "    #print (output_list.get_shape())\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_corr_final(frequency, learning_rate, decay,batch_size, n_hidden_vec1,n_hidden_vec2,n_hidden_vec3,keep, early_stop_th,training_epochs, reg1, reg2):\n",
    "    # set size\n",
    "    #sn = 3 # station number\n",
    "\n",
    "    frequency = int(frequency) #\n",
    "\n",
    "    i = frequency\n",
    "    X_whole = np.zeros(shape = (hourly_bike_cluster.shape[0] - frequency, sn*frequency), dtype = np.float)\n",
    "    Y_whole = np.zeros(shape = (hourly_bike_cluster.shape[0] - frequency, sn), dtype = np.float)\n",
    "\n",
    "    while i < hourly_bike_cluster.shape[0]:\n",
    "        X_whole[i - frequency, ] = hourly_bike_cluster.iloc[(i - frequency):i, 0:sn].values.flatten() # flatten by row 0, 1, 2...7\n",
    "        Y_whole[i - frequency, ] = hourly_bike_cluster.iloc[i, 0:sn]\n",
    "        i = i + 1\n",
    "        #print (i)\n",
    "    \n",
    "    skip = skip1 + freq_max - frequency # to make sure the testing datasets are the same although the frequency could be different\n",
    "\n",
    "    X_training = X_whole[skip: skip+ training, :]\n",
    "    Y_training = Y_whole[skip: skip+ training, :]\n",
    "    #A_training = A_whole[0: 0+ training, :]\n",
    "\n",
    "    X_val = X_whole[skip+training:skip+training+validation, :]\n",
    "    Y_val = Y_whole[skip+training:skip+training+validation, :]\n",
    "    #A_val = A_whole[0+training:0+training+validation, :]\n",
    "\n",
    "    X_test = X_whole[skip+training+validation:skip+training+validation+test, :]\n",
    "    Y_test = Y_whole[skip+training+validation:skip+training+validation+test, :]\n",
    "    #A_test = A_whole[0+training+validation:0+training+validation+test, :]\n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "    n_hidden_vec1 = int(n_hidden_vec1)\n",
    "    n_hidden_vec2 = int(n_hidden_vec2)\n",
    "    n_hidden_vec3 = int(n_hidden_vec3)\n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    # Network Parameters\n",
    "    n_input = sn # station number\n",
    "    n_input_vec = n_input * frequency # 317 * frequency\n",
    "    n_A_vec = n_input * n_input\n",
    "    n_output_vec = n_input * 1 # each row represent a result\n",
    "\n",
    "    #n_classes = 2 # MNIST total classes (0-9 digits) # n_classes is for classification only\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None, n_input_vec]) # X is the input signal\n",
    "    #X_weather = tf.placeholder(tf.float32, [None, 9 * frequency2]) # X_weather weather and holiday information (9 is the feature number)\n",
    "    A = tf.placeholder(tf.float32, [None, n_A_vec]) # A is the normalized adj matrix\n",
    "    oldA = tf.placeholder(tf.float32, [n_input, n_input])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_output_vec]) # y is the regression output\n",
    "\n",
    "    #Xtem = tf.placeholder(tf.float32, [n_input, frequency]) # for each row of X, A, Y, it can be reshaped to Xtem, Atem, Ytem\n",
    "    #Atem = tf.placeholder(tf.float32, [n_input, n_input]) # \n",
    "    #Ytem = tf.placeholder(tf.float32, [n_input, 1]) #\n",
    "\n",
    "    #Ypre = tf.placeholder(tf.float32, [None, n_output_vec])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([frequency, n_hidden_vec1])),\n",
    "        #'h2': tf.Variable(tf.random_normal([n_hidden_vec1, n_hidden_vec2])),\n",
    "        #'h3': tf.Variable(tf.random_normal([n_hidden_vec2, n_hidden_vec3])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_vec1, 1])), # dont forget to change n_hidden_vec1 when add/delete layers\n",
    "        #'f1': tf.Variable(tf.random_normal([272*n_hidden_vec3, 100])),\n",
    "        #'f2': tf.Variable(tf.random_normal([50, 10])),\n",
    "        #'f3': tf.Variable(tf.random_normal([100, 272])),\n",
    "        'A1': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'A2': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'A3': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'h1_wea': tf.Variable(tf.random_normal([9*frequency2, n_hidden_weather1])),\n",
    "        #'out_wea': tf.Variable(tf.random_normal([n_hidden_weather1, n_input]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_input,1])),# n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        #'b2': tf.Variable(tf.random_normal([n_input, 1])), #n_hidden_vec2])),\n",
    "        #'b3': tf.Variable(tf.random_normal([n_input, 1])),#n_hidden_vec3])),\n",
    "        #'b1': tf.Variable(tf.random_normal([n_input,n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        #'b2': tf.Variable(tf.random_normal([n_input,n_hidden_vec2])),\n",
    "        #'b3': tf.Variable(tf.random_normal([n_input,n_hidden_vec3])),\n",
    "        #'bf1': tf.Variable(tf.random_normal([1, 100])), \n",
    "        #'bf2': tf.Variable(tf.random_normal([1, 10])), \n",
    "        #'bf3': tf.Variable(tf.random_normal([1, 272])), \n",
    "        'bout': tf.Variable(tf.random_normal([n_input, 1])), \n",
    "        #'b1_wea': tf.Variable(tf.random_normal([1, n_hidden_weather1])), \n",
    "        #'bout_wea': tf.Variable(tf.random_normal([1, n_input])), \n",
    "    }\n",
    "\n",
    "    #gcn_no_A\n",
    "    #gcn\n",
    "    # Construct model\n",
    "    pred= gcn(X, weights, biases, batch_size,n_input, frequency, 1)\n",
    "    \n",
    "    #set  negative in pred to 0s\n",
    "    #Computes rectified linear: max(features, 0).\n",
    "    #pred = tf.nn.relu(pred)\n",
    "    # Define loss and optimizer\n",
    "    # RMS for regression \n",
    "    # L2 regularization\n",
    "    #cost = tf.reduce_mean(tf.pow(pred-Y, 2)) + reg1*tf.nn.l2_loss(weights['A1']) + reg2*tf.nn.l2_loss(weights['A2'])# + tf.nn.l2_loss(weights['A2']) + tf.nn.l2_loss(weights['A3'])))\n",
    "\n",
    "    # L1 regular tf.reduce_sum(tf.abs(parameters))\n",
    "    cost = tf.reduce_mean(tf.pow(pred-Y, 2)) + reg1*tf.reduce_sum(tf.abs(weights['A1']))# + reg2*tf.reduce_sum(tf.abs(weights['A2']))\n",
    "    pred_val= gcn(X, weights, biases, batch_size,n_input,frequency, 2)\n",
    "    #pred_val = tf.nn.relu(pred_val)\n",
    "    cost_val = tf.reduce_mean(tf.pow(pred_val-Y, 2))\n",
    "\n",
    "    pred_tes= gcn(X, weights, biases, batch_size,n_input,frequency, 3)\n",
    "    #pred_tes = tf.nn.relu(pred_tes)\n",
    "    cost_tes = tf.reduce_mean(tf.pow(pred_tes-Y, 2))\n",
    "    # cross-entropy for classification\n",
    "    # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y_train))\n",
    "    # ratio = tf.abs(tf.reduce_sum(pred)-tf.reduce_sum(Y))/tf.reduce_sum(Y)\n",
    "    #zero = 0\n",
    "    #ratio = tf.reduce_mean(tf.divide(tf.where(tf.not_equal(Y, zero), np.abs(pred-Y), tf.zeros(Y.get_shape(), tf.float32)), tf.where(tf.not_equal(Y, zero), Y, tf.ones(Y.get_shape(), tf.float32))))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay).minimize(cost)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #total_val_cost = []\n",
    "    #total_val_ratio = []\n",
    "\n",
    "    # learning start from \n",
    "\n",
    "    #index = daily_bike[(daily_bike['year'] == 2016) & (daily_bike['monthofyear'] == 1) & (daily_bike['dayofmonth'] == 1)].index.tolist()[0]\n",
    "    #A_hat = normalize_adj(corr_matrix_trips)\n",
    "    #print(A_hat)\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(training/batch_size)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                      Y: Y_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                              keep_prob: keep})\n",
    "                #print (\"Epoch:\", '%04d' % (epoch+1), \"batch: \", i, \"batch cost=\", \\\n",
    "                #    \"{:.9f}\".format(c))\n",
    "                #print (c)\n",
    "                avg_cost += c / total_batch \n",
    "                #Display logs per epoch step\n",
    "            #if epoch % display_step == 0:\n",
    "            #    print (\"Epoch:\", '%04d' % (epoch+1), \"Training RMSE=\", \\\n",
    "            #        \"{:.9f}\".format((np.sqrt(avg_cost))))\n",
    "            # validation\n",
    "            c_val = sess.run([cost_val], feed_dict={X: X_val, Y: Y_val,  keep_prob:1})\n",
    "            #print(\"Validation RMSE: \", (np.sqrt(c_val[0])))\n",
    "\n",
    "            c_tes, pred_tes1, A1= sess.run([cost_tes, pred_tes, weights['A1']], feed_dict={X: X_test,Y: Y_test, keep_prob: 1})\n",
    "            #print(\"Test RMSE: \", (np.sqrt(c_tes)))\n",
    "\n",
    "            if c_val[0] < best_val:\n",
    "                best_val = c_val[0]\n",
    "                #saver.save(sess, './bikesharing_graph_2_th_point1')\n",
    "                test_error = c_tes\n",
    "                traing_error = np.sqrt(avg_cost)\n",
    "                early_stop_k = 0 # reset to 0\n",
    "                #print (pred_tes1)\n",
    "                predic_res = pred_tes1\n",
    "\n",
    "            # early stopping\n",
    "            if c_val[0] >= best_val:\n",
    "                early_stop_k += 1\n",
    "\n",
    "            # threshold\n",
    "            if early_stop_k == early_stop_th:\n",
    "              #  print (\"early stopping...\")\n",
    "                break\n",
    "            \n",
    "\n",
    "        print(\"epoch is \", epoch)\n",
    "        print(\"training error is \", traing_error)\n",
    "        print(\"Optimization Finished! the lowest validation RMSE is \", (np.sqrt(best_val)))\n",
    "        print(\"The test RMSE is \", (np.sqrt(test_error)))\n",
    "    \n",
    "    test_Y = Y_test\n",
    "    test_error = np.sqrt(test_error)\n",
    "    return -np.sqrt(best_val), predic_res,test_Y,test_error, A1#, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_pred = np.empty([2000, 272])\n",
    "All_Y = np.empty([2000, 272])\n",
    "\n",
    "training = 20000\n",
    "validation = 2000\n",
    "test = 2000 #24*90\n",
    "skip1 = 20000 + 2000 - training - validation # totally to make the testing dataset the same\n",
    "freq_max = 24\n",
    "#gcn_corr_eval(7, 0.01, 0.5, 100, 0.4, 10, 5, 5, 0.2, 50, 500)\n",
    "\n",
    "step = 0\n",
    "gap = 2000\n",
    "\n",
    "total_sn = 0\n",
    "num_iter = 50\n",
    "init_points = 200\n",
    "rep = 1000\n",
    "\n",
    "sn = 272\n",
    "\n",
    "# stdbscan\n",
    "spatial_threshold = 300\n",
    "temporal_threshold = 300\n",
    "min_neighbors = 1 # number of neighbor\n",
    "\n",
    "frequency2 = skip1 + freq_max + training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hourly_bike_cluster = hourly_bike\n",
    "\n",
    "best = -10000\n",
    "pre_best = []\n",
    "test_Y_best = []\n",
    "test_error_best = 1000\n",
    "A1_best = []\n",
    "# A2_best = []\n",
    "for i in range(rep):\n",
    "    a = datetime.datetime.now()\n",
    "    val_error, predic_res, test_Y,test_error, A1=gcn_corr_final(24, 0.005, 0.9, 100, 40, 40, 5, 1, 20, 500, 0, 0)\n",
    "    #val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\n",
    "    print (\"finished A running: \", i)\n",
    "    b = datetime.datetime.now()\n",
    "    print(b-a)\n",
    "    if val_error > best:\n",
    "        best = val_error\n",
    "        pre_best = predic_res\n",
    "        test_Y_best = test_Y\n",
    "        test_error_best = test_error\n",
    "        A1_best = A1\n",
    "       # A2_best = A2\n",
    "\n",
    "#val_error, predic_res, test_Y,test_error=gcn_corr_final(24, 0.02, 0.2, 100, 5, 10, 10, 0.8, 50, 500)\n",
    "All_pred[step:(step+gap), total_sn:(total_sn+sn)] = pre_best\n",
    "All_Y[step:(step+gap), total_sn:(total_sn+sn)]  = test_Y_best\n",
    "\n",
    "total_sn = total_sn + sn\n",
    "\n",
    "total_error = np.sqrt(np.mean((All_pred[0:(step+gap),0:total_sn] - All_Y[0:(step+gap),0:total_sn])**2))\n",
    "\n",
    "#print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the cluster now is: \", c)\n",
    "print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the val error of this cluster now is: \", best)\n",
    "print (\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!the test error by this cluster now is: \", total_error)\n",
    "\n",
    "step = step + gap\n",
    "skip1 = skip1 + gap\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
