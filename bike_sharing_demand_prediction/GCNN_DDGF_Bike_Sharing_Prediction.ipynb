{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    #adj[np.isnan(adj)] = 0.\n",
    "    adj = tf.abs(adj)\n",
    "    rowsum = tf.reduce_sum(adj, 1)# sum by row\n",
    "\n",
    "    d_inv_sqrt = tf.pow(rowsum, -0.5)\n",
    "   \n",
    "    #d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    \n",
    "    d_mat_inv_sqrt = tf.diag(d_inv_sqrt)\n",
    "\n",
    "    return tf.matmul(tf.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def gcn(x, weights, biases, batch_size, flag, n_output_vec, num = None):\n",
    "    # Hidden layer with RELU activation\n",
    "    \n",
    "    output_list = tf.Variable(tf.zeros([n_output_vec,1]),dtype=tf.float32) #'Tensor' object does not support item assignment, cant build Ypre\n",
    "    \n",
    "    if flag == 1:\n",
    "        rownum = batch_size\n",
    "    elif flag == 2:\n",
    "        rownum = num\n",
    "    elif flag == 3:\n",
    "        rownum = num\n",
    "    \n",
    "    for i in range(rownum):\n",
    "        Xtem = x[i,:]\n",
    "\n",
    "        Atem1 = 0.5*(weights['A1'] + tf.transpose(weights['A1']))#+ Atem \n",
    "        Atem1 = normalize_adj(Atem1)      \n",
    "        Z1 = tf.matmul(Atem1, Xtem) \n",
    "        layer_1 = tf.add(tf.matmul(Z1, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        \n",
    "        Atem2 = 0.5*(weights['A2'] + tf.transpose(weights['A2']))#+ Atem \n",
    "        Atem2 = normalize_adj(Atem2)\n",
    "        Z2 = tf.matmul(Atem2, layer_1)\n",
    "        layer_2 = tf.add(tf.matmul(Z2, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        \n",
    "        Atem3 = 0.5*(weights['A3'] + tf.transpose(weights['A3']))#+ Atem \n",
    "        Atem3 = normalize_adj(Atem3)\n",
    "        Z3 = tf.matmul(Atem3, layer_2)\n",
    "        layer_3 = tf.add(tf.matmul(Z3, weights['h3']), biases['b3'])\n",
    "        layer_3 = tf.nn.relu(layer_3)\n",
    "        \n",
    "        Z4 = layer_3\n",
    "        out_layer = tf.add(tf.matmul(Z4, weights['out']), biases['bout'])\n",
    "\n",
    "        if i ==0:\n",
    "            #print (out_layer.shape)\n",
    "            tem = tf.reshape(out_layer, [1, n_output_vec])\n",
    "            output_list = tem\n",
    "        else:\n",
    "            tem = tf.reshape(out_layer, [1, n_output_vec])\n",
    "            output_list = tf.concat([output_list, tem], 0)\n",
    "\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_corr_final(feature_in, horizon, learning_rate, decay, batch_size, n_hidden_vec1, n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs):\n",
    "   \n",
    "    feature_in = int(feature_in) \n",
    "    \n",
    "    early_stop_k = 0 # early stop patience\n",
    "    display_step = 1 # frequency of printing results\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    predic_res = []\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    batch_size = int(batch_size)\n",
    "    n_hidden_vec1 = int(n_hidden_vec1)\n",
    "    n_hidden_vec2 = int(n_hidden_vec2)\n",
    "    n_hidden_vec3 = int(n_hidden_vec3)\n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None, sn, feature_in]) # X is the input signal\n",
    "    Y = tf.placeholder(tf.float32, [None, n_output_vec]) # y is the regression output\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([feature_in, n_hidden_vec1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_vec1, n_hidden_vec2])),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_vec2, n_hidden_vec3])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_vec3, 1])),\n",
    "        'A1': tf.Variable(tf.random_normal([sn, sn])),\n",
    "        'A2': tf.Variable(tf.random_normal([sn, sn])),\n",
    "        'A3': tf.Variable(tf.random_normal([sn, sn])),\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([sn, 1])),\n",
    "        'b2': tf.Variable(tf.random_normal([sn, n_hidden_vec2])), \n",
    "        'b3': tf.Variable(tf.random_normal([sn, n_hidden_vec3])),\n",
    "        'bout': tf.Variable(tf.random_normal([sn, 1])), \n",
    "    }\n",
    "\n",
    "    # Construct model\n",
    "    pred= gcn(X, weights, biases, batch_size, 1, n_output_vec)\n",
    "    \n",
    "    pred = scaler.inverse_transform(pred)\n",
    "    Y_true_tr = scaler.inverse_transform(Y)\n",
    "    cost = tf.reduce_mean(tf.pow(pred - Y_true_tr, 2)) \n",
    "\n",
    "    pred_val= gcn(X, weights, biases, batch_size, 2, n_output_vec, num_val)\n",
    "    pred_val = scaler.inverse_transform(pred_val)\n",
    "    Y_true_val = scaler.inverse_transform(Y)\n",
    "    cost_val =  tf.reduce_mean(tf.pow(pred_val - Y_true_val, 2)) \n",
    "\n",
    "    pred_tes= gcn(X, weights, biases, batch_size, 3, n_output_vec, num_test)\n",
    "    pred_tes = scaler.inverse_transform(pred_tes)\n",
    "    Y_true_tes = scaler.inverse_transform(Y)\n",
    "    cost_tes = tf.reduce_mean(tf.pow(pred_tes - Y_true_tes, 2)) \n",
    "                                         \n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay).minimize(cost)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(num_train/batch_size)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                \n",
    "                _, c = sess.run([optimizer, cost], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                      Y: Y_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                              keep_prob: keep})\n",
    "\n",
    "                avg_cost += c / total_batch \n",
    "            \n",
    "            avg_cost = np.sqrt(avg_cost)\n",
    "             #Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Epoch:\", '%04d' % (epoch+1), \"Training RMSE=\", \\\n",
    "                    \"{:.9f}\".format(avg_cost))\n",
    "            # validation\n",
    "            c_val = sess.run([cost_val], feed_dict={X: X_val, Y: Y_val,  keep_prob:1})\n",
    "            c_val = np.sqrt(c_val[0])\n",
    "            print(\"Validation RMSE: \", c_val)\n",
    "            # testing\n",
    "            c_tes, preds, A1, Y_true = sess.run([cost_tes, pred_tes, weights['A1'], Y_true_tes], feed_dict={X: X_test,Y: Y_test, keep_prob: 1})\n",
    "            c_tes = np.sqrt(c_tes)\n",
    "\n",
    "            if c_val < best_val:\n",
    "                best_val = c_val\n",
    "                # save model\n",
    "                #saver.save(sess, './bikesharing_gcnn_ddgf')\n",
    "                test_error = c_tes\n",
    "                traing_error = avg_cost\n",
    "                predic_res = preds\n",
    "                early_stop_k = 0 # reset to 0\n",
    "\n",
    "            # update early stopping patience\n",
    "            if c_val >= best_val:\n",
    "                early_stop_k += 1\n",
    "\n",
    "            # threshold\n",
    "            if early_stop_k == early_stop_th:\n",
    "                break\n",
    "            \n",
    "\n",
    "        print(\"epoch is \", epoch)\n",
    "        print(\"training RMSE is \", traing_error)\n",
    "        print(\"Optimization Finished! the lowest validation RMSE is \", best_val)\n",
    "        print(\"The test RMSE is \", test_error)\n",
    "    \n",
    "    #test_Y = Y_test\n",
    "    #test_error = np.sqrt(test_error)\n",
    "    return best_val, predic_res,Y_true,test_error, A1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"data/NYCBikeHourly272.pickle\"\n",
    "fileObject = open(file_Name,'rb') \n",
    "hourly_bike = pickle.load(fileObject) \n",
    "hourly_bike = pd.DataFrame(hourly_bike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Training, Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    Standard the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = 272 # station number \n",
    "feature_in = 24 # number of features at each node, e.g., bike sharing demand from past 24 hours\n",
    "horizon = 1 # the length to predict, e.g., predict the future one hour bike sharing demand\n",
    "\n",
    "X_whole = []\n",
    "Y_whole = []\n",
    "\n",
    "x_offsets = np.sort(\n",
    "    np.concatenate((np.arange(-feature_in+1, 1, 1),))\n",
    ")\n",
    "\n",
    "y_offsets = np.sort(np.arange(1, 1+ horizon, 1))\n",
    "\n",
    "min_t = abs(min(x_offsets))\n",
    "max_t = abs(hourly_bike.shape[0] - abs(max(y_offsets)))  # Exclusive\n",
    "for t in range(min_t, max_t):\n",
    "    x_t = hourly_bike.iloc[t + x_offsets, 0:sn].values.flatten('F')\n",
    "    y_t = hourly_bike.iloc[t + y_offsets, 0:sn].values.flatten('F')\n",
    "    X_whole.append(x_t)\n",
    "    Y_whole.append(y_t)\n",
    "\n",
    "X_whole = np.stack(X_whole, axis=0)\n",
    "Y_whole = np.stack(Y_whole, axis=0)\n",
    "\n",
    "n_input_vec = X_whole.shape[1] # e.g., 272 * 24\n",
    "n_output_vec = Y_whole.shape[1] # each row represent a result\n",
    "\n",
    "\n",
    "X_whole = np.reshape(X_whole, [X_whole.shape[0], sn, feature_in])\n",
    "num_samples = X_whole.shape[0]\n",
    "num_train = 20000 # Note here actually we use the first 20000 to train the model. The paper mentioned \"22304\" need to be corrected.\n",
    "num_val = 2000\n",
    "num_test = 2000\n",
    "\n",
    "X_training = X_whole[:num_train, :]\n",
    "Y_training = Y_whole[:num_train, :]\n",
    "\n",
    "# shuffle the training dataset\n",
    "perm = np.arange(X_training.shape[0])\n",
    "np.random.shuffle(perm)\n",
    "X_training = X_training[perm]\n",
    "Y_training = Y_training[perm]\n",
    "\n",
    "X_val = X_whole[num_train:num_train+num_val, :]\n",
    "Y_val = Y_whole[num_train:num_train+num_val, :]\n",
    "\n",
    "X_test = X_whole[num_train+num_val:num_train+num_val+num_test, :]\n",
    "Y_test = Y_whole[num_train+num_val:num_train+num_val+num_test, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(mean=X_training.mean(), std=X_training.std())\n",
    "\n",
    "X_training = scaler.transform(X_training)\n",
    "Y_training = scaler.transform(Y_training)\n",
    "\n",
    "X_val = scaler.transform(X_val)\n",
    "Y_val = scaler.transform(Y_val)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "Y_test = scaler.transform(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = 272 # station number \n",
    "feature_in = 24 # number of features at each node, e.g., bike sharing demand from past 24 hours\n",
    "horizon = 1 # the length to predict, e.g., predict the future one hour bike sharing demand\n",
    "\n",
    "learning_rate = 0.005 # learning rate\n",
    "decay = 0.9\n",
    "batchsize = 100 # batch size \n",
    "\n",
    "n_hidden_vec1 = 10 # feature length of the first hidden layer after the graph convolution\n",
    "n_hidden_vec2 = 10 # feature length of the second ...\n",
    "n_hidden_vec3 = 5 # feature length of the third ...\n",
    "\n",
    "keep = 1 # drop out probability\n",
    "\n",
    "early_stop_th = 20 # early stopping threshold, if validation RMSE not dropping in continuous 20 steps, break\n",
    "training_epochs = 100 # total training epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training RMSE= 45.114036799\n",
      "Validation RMSE:  5.108541\n",
      "Epoch: 0002 Training RMSE= 4.017966649\n",
      "Validation RMSE:  3.7551003\n",
      "Epoch: 0003 Training RMSE= 3.415668886\n",
      "Validation RMSE:  3.5079467\n",
      "Epoch: 0004 Training RMSE= 3.221278121\n",
      "Validation RMSE:  3.4558823\n",
      "Epoch: 0005 Training RMSE= 3.121586691\n",
      "Validation RMSE:  3.4163816\n",
      "Epoch: 0006 Training RMSE= 3.057729643\n",
      "Validation RMSE:  3.3816373\n",
      "Epoch: 0007 Training RMSE= 3.008908463\n",
      "Validation RMSE:  3.3330045\n",
      "Epoch: 0008 Training RMSE= 2.973043703\n",
      "Validation RMSE:  3.3097322\n",
      "Epoch: 0009 Training RMSE= 2.945620377\n",
      "Validation RMSE:  3.2862923\n",
      "Epoch: 0010 Training RMSE= 2.923042358\n",
      "Validation RMSE:  3.2650893\n",
      "Epoch: 0011 Training RMSE= 2.903385853\n",
      "Validation RMSE:  3.2435055\n",
      "Epoch: 0012 Training RMSE= 2.886005596\n",
      "Validation RMSE:  3.2216015\n",
      "Epoch: 0013 Training RMSE= 2.870490554\n",
      "Validation RMSE:  3.202019\n",
      "Epoch: 0014 Training RMSE= 2.857524086\n",
      "Validation RMSE:  3.1865935\n",
      "Epoch: 0015 Training RMSE= 2.845514433\n",
      "Validation RMSE:  3.1726682\n",
      "Epoch: 0016 Training RMSE= 2.835377153\n",
      "Validation RMSE:  3.160563\n",
      "Epoch: 0017 Training RMSE= 2.826331159\n",
      "Validation RMSE:  3.1447668\n",
      "Epoch: 0018 Training RMSE= 2.818359777\n",
      "Validation RMSE:  3.136792\n",
      "Epoch: 0019 Training RMSE= 2.811671040\n",
      "Validation RMSE:  3.140788\n",
      "Epoch: 0020 Training RMSE= 2.805303480\n",
      "Validation RMSE:  3.1629117\n",
      "Epoch: 0021 Training RMSE= 2.799409699\n",
      "Validation RMSE:  3.1551943\n",
      "Epoch: 0022 Training RMSE= 2.794102867\n",
      "Validation RMSE:  3.1439526\n",
      "Epoch: 0023 Training RMSE= 2.789256977\n",
      "Validation RMSE:  3.1247468\n",
      "Epoch: 0024 Training RMSE= 2.784657827\n",
      "Validation RMSE:  3.1178882\n",
      "Epoch: 0025 Training RMSE= 2.780722336\n",
      "Validation RMSE:  3.0964909\n",
      "Epoch: 0026 Training RMSE= 2.776588039\n",
      "Validation RMSE:  3.095286\n",
      "Epoch: 0027 Training RMSE= 2.773008395\n",
      "Validation RMSE:  3.1081097\n",
      "Epoch: 0028 Training RMSE= 2.769347012\n",
      "Validation RMSE:  3.1093366\n",
      "Epoch: 0029 Training RMSE= 2.766132661\n",
      "Validation RMSE:  3.1071312\n",
      "Epoch: 0030 Training RMSE= 2.763298093\n",
      "Validation RMSE:  3.0959158\n",
      "Epoch: 0031 Training RMSE= 2.759865216\n",
      "Validation RMSE:  3.0840778\n",
      "Epoch: 0032 Training RMSE= 2.756155941\n",
      "Validation RMSE:  3.0963237\n",
      "Epoch: 0033 Training RMSE= 2.752927715\n",
      "Validation RMSE:  3.1138809\n",
      "Epoch: 0034 Training RMSE= 2.750244047\n",
      "Validation RMSE:  3.103752\n",
      "Epoch: 0035 Training RMSE= 2.747138765\n",
      "Validation RMSE:  3.099662\n",
      "Epoch: 0036 Training RMSE= 2.744595898\n",
      "Validation RMSE:  3.0887601\n",
      "Epoch: 0037 Training RMSE= 2.741382646\n",
      "Validation RMSE:  3.079729\n",
      "Epoch: 0038 Training RMSE= 2.738428507\n",
      "Validation RMSE:  3.0819483\n",
      "Epoch: 0039 Training RMSE= 2.735497799\n",
      "Validation RMSE:  3.0769303\n",
      "Epoch: 0040 Training RMSE= 2.732583201\n",
      "Validation RMSE:  3.0688782\n",
      "Epoch: 0041 Training RMSE= 2.729711670\n",
      "Validation RMSE:  3.0644\n",
      "Epoch: 0042 Training RMSE= 2.726778475\n",
      "Validation RMSE:  3.0657022\n",
      "Epoch: 0043 Training RMSE= 2.723965011\n",
      "Validation RMSE:  3.0555391\n",
      "Epoch: 0044 Training RMSE= 2.721376279\n",
      "Validation RMSE:  3.0553608\n",
      "Epoch: 0045 Training RMSE= 2.718530758\n",
      "Validation RMSE:  3.0517867\n",
      "Epoch: 0046 Training RMSE= 2.715846455\n",
      "Validation RMSE:  3.0527537\n",
      "Epoch: 0047 Training RMSE= 2.712950844\n",
      "Validation RMSE:  3.052985\n",
      "Epoch: 0048 Training RMSE= 2.710321803\n",
      "Validation RMSE:  3.0482626\n",
      "Epoch: 0049 Training RMSE= 2.707693613\n",
      "Validation RMSE:  3.0417163\n",
      "Epoch: 0050 Training RMSE= 2.705495434\n",
      "Validation RMSE:  3.0385187\n",
      "Epoch: 0051 Training RMSE= 2.703289547\n",
      "Validation RMSE:  3.0393436\n",
      "Epoch: 0052 Training RMSE= 2.701147808\n",
      "Validation RMSE:  3.040642\n",
      "Epoch: 0053 Training RMSE= 2.698893607\n",
      "Validation RMSE:  3.0439599\n",
      "Epoch: 0054 Training RMSE= 2.696891418\n",
      "Validation RMSE:  3.0460865\n",
      "Epoch: 0055 Training RMSE= 2.695153843\n",
      "Validation RMSE:  3.0332472\n",
      "Epoch: 0056 Training RMSE= 2.693255229\n",
      "Validation RMSE:  3.0307555\n",
      "Epoch: 0057 Training RMSE= 2.691703039\n",
      "Validation RMSE:  3.022927\n",
      "Epoch: 0058 Training RMSE= 2.690177770\n",
      "Validation RMSE:  3.0363207\n",
      "Epoch: 0059 Training RMSE= 2.688101192\n",
      "Validation RMSE:  3.029696\n",
      "Epoch: 0060 Training RMSE= 2.686882907\n",
      "Validation RMSE:  3.0288806\n",
      "Epoch: 0061 Training RMSE= 2.684957319\n",
      "Validation RMSE:  3.0297928\n",
      "Epoch: 0062 Training RMSE= 2.683503105\n",
      "Validation RMSE:  3.0287187\n",
      "Epoch: 0063 Training RMSE= 2.681950740\n",
      "Validation RMSE:  3.0284777\n",
      "Epoch: 0064 Training RMSE= 2.680157908\n",
      "Validation RMSE:  3.029457\n",
      "Epoch: 0065 Training RMSE= 2.678209163\n",
      "Validation RMSE:  3.0291214\n",
      "Epoch: 0066 Training RMSE= 2.677164730\n",
      "Validation RMSE:  3.028327\n",
      "Epoch: 0067 Training RMSE= 2.675501989\n",
      "Validation RMSE:  3.0232904\n",
      "Epoch: 0068 Training RMSE= 2.673627776\n",
      "Validation RMSE:  3.0171804\n",
      "Epoch: 0069 Training RMSE= 2.672221253\n",
      "Validation RMSE:  3.019869\n",
      "Epoch: 0070 Training RMSE= 2.671224863\n",
      "Validation RMSE:  3.0200899\n",
      "Epoch: 0071 Training RMSE= 2.669473187\n",
      "Validation RMSE:  3.0228834\n",
      "Epoch: 0072 Training RMSE= 2.667833512\n",
      "Validation RMSE:  3.0207636\n",
      "Epoch: 0073 Training RMSE= 2.666299223\n",
      "Validation RMSE:  3.0239213\n",
      "Epoch: 0074 Training RMSE= 2.664728707\n",
      "Validation RMSE:  3.01608\n",
      "Epoch: 0075 Training RMSE= 2.663265628\n",
      "Validation RMSE:  3.0146782\n",
      "Epoch: 0076 Training RMSE= 2.661872115\n",
      "Validation RMSE:  3.0140011\n",
      "Epoch: 0077 Training RMSE= 2.660462858\n",
      "Validation RMSE:  3.0133848\n",
      "Epoch: 0078 Training RMSE= 2.659343716\n",
      "Validation RMSE:  3.0127587\n",
      "Epoch: 0079 Training RMSE= 2.657771637\n",
      "Validation RMSE:  3.011024\n",
      "Epoch: 0080 Training RMSE= 2.656699500\n",
      "Validation RMSE:  3.0066767\n",
      "Epoch: 0081 Training RMSE= 2.655559490\n",
      "Validation RMSE:  3.0100224\n",
      "Epoch: 0082 Training RMSE= 2.654167425\n",
      "Validation RMSE:  3.0040576\n",
      "Epoch: 0083 Training RMSE= 2.653138261\n",
      "Validation RMSE:  3.00282\n",
      "Epoch: 0084 Training RMSE= 2.652338762\n",
      "Validation RMSE:  3.0118306\n",
      "Epoch: 0085 Training RMSE= 2.650585858\n",
      "Validation RMSE:  3.0140681\n",
      "Epoch: 0086 Training RMSE= 2.649946259\n",
      "Validation RMSE:  3.0115361\n",
      "Epoch: 0087 Training RMSE= 2.649329035\n",
      "Validation RMSE:  3.0081546\n",
      "Epoch: 0088 Training RMSE= 2.648458292\n",
      "Validation RMSE:  3.002122\n",
      "Epoch: 0089 Training RMSE= 2.647531712\n",
      "Validation RMSE:  3.007334\n",
      "Epoch: 0090 Training RMSE= 2.646392476\n",
      "Validation RMSE:  3.0022163\n",
      "Epoch: 0091 Training RMSE= 2.645417577\n",
      "Validation RMSE:  3.010571\n",
      "Epoch: 0092 Training RMSE= 2.644325254\n",
      "Validation RMSE:  3.013472\n",
      "Epoch: 0093 Training RMSE= 2.643647862\n",
      "Validation RMSE:  3.0032794\n",
      "Epoch: 0094 Training RMSE= 2.642875592\n",
      "Validation RMSE:  2.9971764\n",
      "Epoch: 0095 Training RMSE= 2.641991453\n",
      "Validation RMSE:  2.9958289\n",
      "Epoch: 0096 Training RMSE= 2.641166121\n",
      "Validation RMSE:  2.9978106\n",
      "Epoch: 0097 Training RMSE= 2.640342090\n",
      "Validation RMSE:  2.9937165\n",
      "Epoch: 0098 Training RMSE= 2.639273893\n",
      "Validation RMSE:  2.9895809\n",
      "Epoch: 0099 Training RMSE= 2.637945726\n",
      "Validation RMSE:  2.9860694\n",
      "Epoch: 0100 Training RMSE= 2.636980924\n",
      "Validation RMSE:  2.9919033\n",
      "epoch is  99\n",
      "training RMSE is  2.6379457255319347\n",
      "Optimization Finished! the lowest validation RMSE is  2.9860694\n",
      "The test RMSE is  2.345093\n",
      "Total training time:  1:01:38.503072\n"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now()\n",
    "\n",
    "val_error, predic_res, test_Y, test_error, A1 = gcn_corr_final(feature_in, horizon, learning_rate, decay, batchsize, n_hidden_vec1,\n",
    "                                                            n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs)\n",
    "\n",
    "\n",
    "b = datetime.datetime.now()\n",
    "\n",
    "print('Total training time: ', b-a)\n",
    "\n",
    "#np.savetxt(\"prediction.csv\", predic_res, delimiter = ',')\n",
    "#np.savetxt(\"prediction_Y.csv\", test_Y, delimiter = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use three layers: 2.34"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
