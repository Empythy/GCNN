{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    #adj[np.isnan(adj)] = 0.\n",
    "    adj = tf.abs(adj)\n",
    "    rowsum = tf.reduce_sum(adj, 1)# sum by row\n",
    "\n",
    "    d_inv_sqrt = tf.pow(rowsum, -0.5)\n",
    "   \n",
    "    #d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    \n",
    "    d_mat_inv_sqrt = tf.diag(d_inv_sqrt)\n",
    "\n",
    "    return tf.matmul(tf.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def gcn(x, weights, biases, batch_size, n_input, frequency,flag, n_output_vec, num = None):\n",
    "    # Hidden layer with RELU activation\n",
    "    \n",
    "    output_list = tf.Variable(tf.zeros([n_output_vec,1]),dtype=tf.float32) #'Tensor' object does not support item assignment, cant build Ypre\n",
    "    \n",
    "    if flag == 1:\n",
    "        rownum = batch_size\n",
    "    elif flag == 2:\n",
    "        rownum = num\n",
    "    elif flag == 3:\n",
    "        rownum = num\n",
    "    \n",
    "    for i in range(rownum):\n",
    "        Xtem = x[i,:]\n",
    "        #Xtem = tf.reshape(x[i,:], [n_input, frequency])\n",
    "        #Xtem = tf.transpose(Xtem)\n",
    "        #Atem = tf.convert_to_tensor(A_whole_final, dtype=np.float32)\n",
    "        #Atem1 = tf.diag(tf.ones([n_input])) # Atem not palying any roles\n",
    "        #Ytem = tf.reshape(Y[i,:], [n_input, 1])\n",
    "        #Atem = tf.diag(tf.ones([n_input]))\n",
    "        Atem1 = 0.5*(weights['A1'] + tf.transpose(weights['A1']))#+ Atem \n",
    "        Atem1 = normalize_adj(Atem1)\n",
    "        #th = tf.constant(0.01, dtype=tf.float32)\n",
    "        #where = tf.subtract(Atem1, th)\n",
    "        #Atem1 = tf.nn.relu(where)\n",
    "        \n",
    "        Z1 = tf.matmul(Atem1, Xtem) #+ tf.matmul( tf.matmul(weights['A1'], weights['A1']), Xtem)\n",
    "        #layer_1 = tf.matmul(Z1, weights['h1']) \n",
    "        layer_1 = tf.add(tf.matmul(Z1, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        \n",
    "        Atem2 = 0.5*(weights['A2'] + tf.transpose(weights['A2']))#+ Atem \n",
    "        Atem2 = normalize_adj(Atem2)\n",
    "        \n",
    "        Z2 = tf.matmul(Atem2, layer_1)\n",
    "        layer_2 = tf.add(tf.matmul(Z2, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        \n",
    "        Atem3 = 0.5*(weights['A3'] + tf.transpose(weights['A3']))#+ Atem \n",
    "        Atem3 = normalize_adj(Atem3)\n",
    "        Z3 = tf.matmul(Atem3, layer_2)\n",
    "        layer_3 = tf.add(tf.matmul(Z3, weights['h3']), biases['b3'])\n",
    "        layer_3 = tf.nn.relu(layer_3)\n",
    "        \n",
    "        #Atem4 = 0.5*(weights['A4'] + tf.transpose(weights['A4']))#+ Atem \n",
    "        #Atem4 = normalize_adj(Atem4)\n",
    "        #Z4 = tf.matmul(Atem4, layer_3)\n",
    "        #layer_4 = tf.add(tf.matmul(Z4, weights['h4']), biases['b4'])\n",
    "        #layer_4 = tf.nn.relu(layer_4)\n",
    "        \n",
    "        # flattern\n",
    "        #layer_3 = tf.reshape(layer_3, [1, 272*n_hidden_vec3])\n",
    "        \n",
    "        #F1 = tf.add(tf.matmul(layer_3, weights['f1']), biases['bf1'])\n",
    "        #F1 = tf.nn.relu(F1)\n",
    "        \n",
    "        #F2 = tf.add(tf.matmul(F1, weights['f2']), biases['bf2'])\n",
    "        #F2 = tf.nn.relu(F2)\n",
    "        \n",
    "        #F3 = tf.add(tf.matmul(F1, weights['f3']), biases['bf3'])\n",
    "        #out_layer = tf.reshape(F3, [272, 1])\n",
    "\n",
    "        # Output layer with linear activation\n",
    "        Z4 = layer_3#tf.matmul(Atem1, layer_1)#tf.matmul(Atem, layer_3)\n",
    "        out_layer = tf.add(tf.matmul(Z4, weights['out']), biases['bout'])\n",
    "        #out_layer = tf.nn.relu(out_layer)\n",
    "        # weather layer 1\n",
    "        #x_wea_tem = tf.reshape(x_wea[i,:], [1, 9*frequency2]) # 1 by 126\n",
    "        #layer_1_wea = tf.add(tf.matmul(x_wea_tem, weights['h1_wea']), biases['b1_wea'])\n",
    "        #layer_1_wea = tf.nn.relu(layer_1_wea)\n",
    "        \n",
    "        #out_layer_wea = tf.add(tf.matmul(layer_1_wea, weights['out_wea']), biases['bout_wea'])\n",
    "        #out_layer = tf.add(out_layer, tf.reshape(out_layer_wea, [272, 1]))\n",
    "        \n",
    "        #print (out_layer.get_shape())\n",
    "        if i ==0:\n",
    "            #print (out_layer.shape)\n",
    "            tem = tf.reshape(out_layer, [1, n_output_vec])\n",
    "            output_list = tem\n",
    "        else:\n",
    "            tem = tf.reshape(out_layer, [1, n_output_vec])\n",
    "            output_list = tf.concat([output_list, tem], 0)\n",
    "        \n",
    "        #print (tf.reduce_mean(tf.pow(output_list-out_layer, 2)))\n",
    "    \n",
    "    #print (output_list.get_shape())\n",
    "    #print ('here!!!!!!!!!!!!!!!!')\n",
    "    #output_list = tf.transpose(output_list)\n",
    "    \n",
    "    #print (output_list.shape)\n",
    "    \n",
    "    #print (output_list.get_shape())\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_corr_final(feature_in, horizon, learning_rate, decay, batch_size, n_hidden_vec1, n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs):\n",
    "   \n",
    "    early_stop_k = 0 # early stop patience\n",
    "    display_step = 1 # frequency of printing results\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    batch_size = int(batch_size)\n",
    "    n_hidden_vec1 = int(n_hidden_vec1)\n",
    "    n_hidden_vec2 = int(n_hidden_vec2)\n",
    "    n_hidden_vec3 = int(n_hidden_vec3)\n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None, sn, feature_in]) # X is the input signal\n",
    "    Y = tf.placeholder(tf.float32, [None, n_output_vec]) # y is the regression output\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([feature_in, n_hidden_vec1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_vec1, n_hidden_vec2])),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_vec2, n_hidden_vec3])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_vec3, horizon])),\n",
    "        'A1': tf.Variable(tf.random_normal([sn, sn])),\n",
    "        'A2': tf.Variable(tf.random_normal([sn, sn])),\n",
    "        'A3': tf.Variable(tf.random_normal([sn, sn])),\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([sn, n_hidden_vec1])),\n",
    "        'b2': tf.Variable(tf.random_normal([sn, n_hidden_vec2])), \n",
    "        'b3': tf.Variable(tf.random_normal([sn, n_hidden_vec3])),\n",
    "        'bout': tf.Variable(tf.random_normal([sn, horizon])), \n",
    "    }\n",
    "\n",
    "    # Construct model\n",
    "    pred= gcn(X, weights, biases, batch_size,n_input, feature_in, 1, n_output_vec)\n",
    "    \n",
    "    pred = scaler.inverse_transform(pred)\n",
    "    Y_true_tr = scaler.inverse_transform(Y)\n",
    "    cost = tf.reduce_mean(tf.pow(pred - Y_true_tr, 2)) \n",
    "\n",
    "    pred_val= gcn(X, weights, biases, batch_size,n_input,feature_in, 2, n_output_vec, num_val)\n",
    "    pred_val = scaler.inverse_transform(pred_val)\n",
    "    Y_true_val = scaler.inverse_transform(Y)\n",
    "    cost_val =  tf.reduce_mean(tf.pow(pred_val - Y_true_val, 2)) \n",
    "\n",
    "    pred_tes= gcn(X, weights, biases, batch_size,n_input,feature_in, 3, n_output_vec, num_test)\n",
    "    pred_tes = scaler.inverse_transform(pred_tes)\n",
    "    Y_true_tes = scaler.inverse_transform(Y)\n",
    "    cost_tes = tf.reduce_mean(tf.pow(pred_tes - Y_true_tes, 2)) \n",
    "                                         \n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay).minimize(cost)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(num_train/batch_size)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                \n",
    "                _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                      Y: Y_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                              keep_prob: keep})\n",
    "\n",
    "                avg_cost += c / total_batch \n",
    "            \n",
    "             #Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Epoch:\", '%04d' % (epoch+1), \"Training RMSE=\", \\\n",
    "                    \"{:.9f}\".format(np.sqrt(avg_cost)))\n",
    "            # validation\n",
    "            c_val = sess.run([cost_val], feed_dict={X: X_val, Y: Y_val,  keep_prob:1})\n",
    "            print(\"Validation RMSE: \", np.sqrt(c_val[0]))\n",
    "            # testing\n",
    "            c_tes, pred_tes1, A1, Y_true = sess.run([cost_tes, pred_tes, weights['A1'], Y_true_tes], feed_dict={X: X_test,Y: Y_test, keep_prob: 1})\n",
    "            print(\"Test RMSE: \", np.sqrt(c_tes))\n",
    "\n",
    "            if c_val[0] < best_val:\n",
    "                best_val = np.sqrt(c_val[0])\n",
    "                # save model\n",
    "                #saver.save(sess, './bikesharing_gcnn_ddgf')\n",
    "                test_error = np.sqrt(c_tes)\n",
    "                traing_error = np.sqrt(avg_cost)#np.sqrt(avg_cost)\n",
    "                early_stop_k = 0 # reset to 0\n",
    "\n",
    "            # update early stopping patience\n",
    "            if c_val[0] >= best_val:\n",
    "                early_stop_k += 1\n",
    "\n",
    "            # threshold\n",
    "            if early_stop_k == early_stop_th:\n",
    "                break\n",
    "            \n",
    "\n",
    "        print(\"epoch is \", epoch)\n",
    "        print(\"training error is \", traing_error)\n",
    "        print(\"Optimization Finished! the lowest validation RMSE is \", best_val)\n",
    "        print(\"The test RMSE is \", test_error)\n",
    "    \n",
    "    test_Y = Y_test\n",
    "    test_error = np.sqrt(test_error)\n",
    "    return best_val, predic_res,Y_true,test_error, A1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"data/NYCBikeHourly272.pickle\"\n",
    "fileObject = open(file_Name,'rb') \n",
    "hourly_bike = pickle.load(fileObject) \n",
    "hourly_bike = pd.DataFrame(hourly_bike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split into Training, Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    Standard the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = 272 # station number \n",
    "feature_in = 24 # number of features at each node, e.g., bike sharing demand from past 24 hours\n",
    "horizon = 1 # the length to predict, e.g., predict the future one hour bike sharing demand\n",
    "\n",
    "X_whole = []\n",
    "Y_whole = []\n",
    "\n",
    "x_offsets = np.sort(\n",
    "    np.concatenate((np.arange(-feature_in+1, 1, 1),))\n",
    ")\n",
    "\n",
    "y_offsets = np.sort(np.arange(1, 1+ horizon, 1))\n",
    "\n",
    "min_t = abs(min(x_offsets))\n",
    "max_t = abs(hourly_bike.shape[0] - abs(max(y_offsets)))  # Exclusive\n",
    "for t in range(min_t, max_t):\n",
    "    x_t = hourly_bike.iloc[t + x_offsets, 0:sn].values.flatten('F')\n",
    "    y_t = hourly_bike.iloc[t + y_offsets, 0:sn].values.flatten('F')\n",
    "    X_whole.append(x_t)\n",
    "    Y_whole.append(y_t)\n",
    "\n",
    "X_whole = np.stack(X_whole, axis=0)\n",
    "Y_whole = np.stack(Y_whole, axis=0)\n",
    "\n",
    "n_input_vec = X_whole.shape[1] # e.g., 272 * 24\n",
    "n_output_vec = Y_whole.shape[1] # each row represent a result\n",
    "\n",
    "\n",
    "X_whole = np.reshape(X_whole, [X_whole.shape[0], sn, feature_in])\n",
    "num_samples = X_whole.shape[0]\n",
    "num_test = 2000\n",
    "num_val = 2000\n",
    "num_train = num_samples - num_test - num_val\n",
    "\n",
    "X_training = X_whole[:num_train, :]\n",
    "Y_training = Y_whole[:num_train, :]\n",
    "\n",
    "# shuffle the training dataset\n",
    "perm = np.arange(X_training.shape[0])\n",
    "np.random.shuffle(perm)\n",
    "X_training = X_training[perm]\n",
    "Y_training = Y_training[perm]\n",
    "\n",
    "X_val = X_whole[num_train:num_train+num_val, :]\n",
    "Y_val = Y_whole[num_train:num_train+num_val, :]\n",
    "\n",
    "X_test = X_whole[-num_test:, :]\n",
    "Y_test = Y_whole[-num_test:, :]\n",
    "\n",
    "scaler = StandardScaler(mean=X_training.mean(), std=X_training.std())\n",
    "\n",
    "X_training = scaler.transform(X_training)\n",
    "Y_training = scaler.transform(Y_training)\n",
    "\n",
    "X_val = scaler.transform(X_val)\n",
    "Y_val = scaler.transform(Y_val)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "Y_test = scaler.transform(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # learning rate\n",
    "decay = 0.9\n",
    "\n",
    "n_hidden_vec1 = 100 # feature length of the first hidden layer after the graph convolution\n",
    "n_hidden_vec2 = 40 # feature length of the second ...\n",
    "n_hidden_vec3 = 40 # feature length of the third ...\n",
    "\n",
    "keep = 1 # drop out probability\n",
    "batchsize = 100 # batch size \n",
    "early_stop_th = 50 # early stopping threshold, if validation RMSE not dropping in continuous 50 steps, break\n",
    "training_epochs = 500 # total training epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-0dd3131fdc3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m val_error, predic_res, test_Y,test_error, A1 = gcn_corr_final(feature_in, horizon, learning_rate, decay, batchsize, n_hidden_vec1,\n\u001b[0;32m----> 4\u001b[0;31m                                                             n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-e66ffb7ee13a>\u001b[0m in \u001b[0;36mgcn_corr_final\u001b[0;34m(feature_in, horizon, learning_rate, decay, batch_size, n_hidden_vec1, n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1015\u001b[0m                 run_metadata):\n\u001b[1;32m   1016\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1066\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now()\n",
    "\n",
    "val_error, predic_res, test_Y,test_error, A1 = gcn_corr_final(feature_in, horizon, learning_rate, decay, batchsize, n_hidden_vec1,\n",
    "                                                            n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs)\n",
    "\n",
    "\n",
    "b = datetime.datetime.now()\n",
    "\n",
    "print('Total training time: ', b-a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
