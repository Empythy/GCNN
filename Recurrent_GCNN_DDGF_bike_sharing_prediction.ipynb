{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr\n",
    "from tensorflow.python.ops.rnn_cell_impl import _RNNCell as RNNCell\n",
    "import collections\n",
    "from tensorflow.contrib import rnn\n",
    "import h5py\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from utils import normalize_adj, StandardScaler, masked_mae_tf, masked_mae_tf_by_horizon\n",
    "from gcn import gcn, gcnn_ddgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"data/NYCBikeHourly272.pickle\"\n",
    "fileObject = open(file_Name,'rb') \n",
    "hourly_bike = pickle.load(fileObject) \n",
    "hourly_bike = pd.DataFrame(hourly_bike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_num = 272 # node number \n",
    "feature_in = 24 # number of features at each node, e.g., bike sharing demand from past 24 hours\n",
    "horizon = 1 # the length to predict, e.g., predict the future one hour bike sharing demand\n",
    "lstm_steps = 1 # number of cells in the LSTM part\n",
    "\n",
    "X_whole = []\n",
    "Y_whole = []\n",
    "\n",
    "x_offsets = np.sort(\n",
    "    np.concatenate((np.arange(-feature_in+1, 1, 1),))\n",
    ")\n",
    "# Predict the next one hour\n",
    "y_offsets = np.sort(np.arange(1, 1+ horizon, 1))\n",
    "\n",
    "min_t = abs(min(x_offsets))\n",
    "max_t = abs(hourly_bike.shape[0] - abs(max(y_offsets)))  # Exclusive\n",
    "for t in range(min_t, max_t):\n",
    "    x_t = hourly_bike.iloc[t + x_offsets, 0:sn].values.flatten('F')\n",
    "    y_t = hourly_bike.iloc[t + y_offsets, 0:sn].values.flatten('F')\n",
    "    X_whole.append(x_t)\n",
    "    Y_whole.append(y_t)\n",
    "\n",
    "X_whole = np.stack(X_whole, axis=0)\n",
    "Y_whole = np.stack(Y_whole, axis=0)\n",
    "\n",
    "X_whole = np.reshape(X_whole, [X_whole.shape[0], node_num, feature_in])\n",
    "\n",
    "i = lstm_steps\n",
    "X_whole_lstm = []\n",
    "Y_whole_lstm = []\n",
    "\n",
    "while i <= X_whole.shape[0]:\n",
    "    X_whole_lstm.append(X_whole[i-lstm_steps:i,:]) # features from i-lstm_steps + 1 to i\n",
    "    Y_whole_lstm.append(Y_whole[i-1])\n",
    "    i = i + 1\n",
    "\n",
    "X_whole_lstm = np.stack(X_whole_lstm, axis = 0) # (34239, 10, 207, 12)\n",
    "Y_whole_lstm = np.stack(Y_whole_lstm, axis = 0) # (34239, 2484)\n",
    "\n",
    "num_train = 20000 # Note here actually we use the first 20000 to train the model. The paper mentioned \"22304\" need to be corrected.\n",
    "num_val = 2000\n",
    "num_test = 2000\n",
    "\n",
    "X_training = X_whole_lstm[:num_train, :]\n",
    "Y_training = Y_whole_lstm[:num_train, :]\n",
    "\n",
    "# shuffle the training dataset\n",
    "perm = np.arange(X_training.shape[0])\n",
    "np.random.shuffle(perm)\n",
    "X_training = X_training[perm]\n",
    "Y_training = Y_training[perm]\n",
    "\n",
    "X_val = X_whole_lstm[num_train:num_train+num_val, :]\n",
    "Y_val = Y_whole_lstm[num_train:num_train+num_val, :]\n",
    "\n",
    "X_test = X_whole_lstm[num_train+num_val:num_train+num_val+num_test, :]\n",
    "Y_test = Y_whole_lstm[num_train+num_val:num_train+num_val+num_test, :]\n",
    "\n",
    "scaler = StandardScaler(mean=X_training.mean(), std=X_training.std())\n",
    "\n",
    "X_training = scaler.transform(X_training)\n",
    "Y_training = scaler.transform(Y_training)\n",
    "\n",
    "X_val = scaler.transform(X_val)\n",
    "Y_val = scaler.transform(Y_val)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "Y_test = scaler.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def gcn(x, weights, biases, batch_size, n_input, frequency,flag, n_output_vec):\n",
    "    # Hidden layer with RELU activation\n",
    "    \n",
    "   # output_list = tf.Variable(tf.zeros([n_output_vec,1]),dtype=tf.float32) #'Tensor' object does not support item assignment, cant build Ypre\n",
    "\n",
    "    #Xtem = tf.reshape(x[i,:], [n_input, frequency])\n",
    "    #Xtem = tf.transpose(Xtem)\n",
    "    #Atem = tf.convert_to_tensor(A_whole_final, dtype=np.float32)\n",
    "    #Atem1 = tf.diag(tf.ones([n_input])) # Atem not palying any roles\n",
    "    #Ytem = tf.reshape(Y[i,:], [n_input, 1])\n",
    "    #Atem = tf.diag(tf.ones([n_input]))\n",
    "    #x = tf.reshape(x, [-1, sn, 1]) # 100, 207, 1\n",
    "    \n",
    "    # x (?, 207, 12)\n",
    "    x = tf.transpose(x, [1, 0, 2]) # 207, ?, 12\n",
    "    x = tf.reshape(x, [sn, -1]) # 207, batch*feature_num\n",
    "    Atem1 = 0.5*(weights['A1'] + tf.transpose(weights['A1']))#+ Atem \n",
    "    Atem1 = normalize_adj(Atem1)\n",
    "    #th = tf.constant(0.01, dtype=tf.float32)\n",
    "    #where = tf.subtract(Atem1, th)\n",
    "    #Atem1 = tf.nn.relu(where)\n",
    "\n",
    "    Z1 = tf.matmul(Atem1, x) # 207, batch*feature_num  #+ tf.matmul( tf.matmul(weights['A1'], weights['A1']), Xtem)\n",
    "    Z1 = tf.reshape(Z1, [-1, frequency]) # 207* 100, frequency\n",
    "    #layer_1 = tf.matmul(Z1, weights['h1']) \n",
    "    layer_1 = tf.add(tf.matmul(Z1, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1) # 207*100, hidden1\n",
    "\n",
    "    \n",
    "    Atem2 = 0.5*(weights['A2'] + tf.transpose(weights['A2']))#+ Atem \n",
    "    Atem2 = normalize_adj(Atem2)\n",
    "    \n",
    "    layer_1 = tf.reshape(layer_1, [sn, -1])  # 207, batchsize*hidden1\n",
    "    Z2 = tf.matmul(Atem2, layer_1)\n",
    "    Z2 = tf.reshape(Z2, [-1, n_hidden_vec1]) # 207*batchsize, n_hidden_vec1\n",
    "    layer_2 = tf.add(tf.matmul(Z2, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2) # 207*batchsize, hidden2\n",
    "\n",
    "    Atem3 = 0.5*(weights['A3'] + tf.transpose(weights['A3']))#+ Atem \n",
    "    Atem3 = normalize_adj(Atem3) \n",
    "    \n",
    "    layer_2 = tf.reshape(layer_2, [sn, -1])  # 207, batchsize*hidden2\n",
    "    Z3 = tf.matmul(Atem3, layer_2)\n",
    "    Z3 = tf.reshape(Z3, [-1, n_hidden_vec2]) # 207*batchsize, hidden2\n",
    "    layer_3 = tf.add(tf.matmul(Z3, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3) # 207*batchsize, hidden3\n",
    "    \n",
    "    #Atem4 = 0.5*(weights['A4'] + tf.transpose(weights['A4']))#+ Atem \n",
    "    #Atem4 = normalize_adj(Atem4)\n",
    "    #Z4 = tf.matmul(Atem4, layer_3)\n",
    "    #layer_4 = tf.add(tf.matmul(Z4, weights['h4']), biases['b4'])\n",
    "    #layer_4 = tf.nn.relu(layer_4)\n",
    "\n",
    "    # flattern\n",
    "    #layer_3 = tf.reshape(layer_3, [1, 272*n_hidden_vec3])\n",
    "\n",
    "    #F1 = tf.add(tf.matmul(layer_3, weights['f1']), biases['bf1'])\n",
    "    #F1 = tf.nn.relu(F1)\n",
    "\n",
    "    #F2 = tf.add(tf.matmul(F1, weights['f2']), biases['bf2'])\n",
    "    #F2 = tf.nn.relu(F2)\n",
    "\n",
    "    #F3 = tf.add(tf.matmul(F1, weights['f3']), biases['bf3'])\n",
    "    #out_layer = tf.reshape(F3, [272, 1])\n",
    "\n",
    "    # Output layer with linear activation\n",
    "    layer_3 = tf.reshape(layer_3, [sn, -1, n_hidden_vec3])\n",
    "    layer_3 = tf.transpose(layer_3, [1, 0, 2]) # batchsize, sn, hidden3\n",
    "    layer_3 = tf.reshape(layer_3, [-1, sn*n_hidden_vec3]) # batchsize, sn*hidden3\n",
    "    Z4 = layer_3\n",
    "    #out_layer = tf.add(tf.matmul(Z4, weights['outg']), biases['boutg'])\n",
    "    #tf.matmul(Atem1, layer_1)#tf.matmul(Atem, layer_3)\n",
    "    #out_layer = tf.add(tf.matmul(Z4, weights['out']), biases['bout'])\n",
    "    #out_layer = tf.nn.relu(out_layer)\n",
    "    # weather layer 1\n",
    "    #x_wea_tem = tf.reshape(x_wea[i,:], [1, 9*frequency2]) # 1 by 126\n",
    "    #layer_1_wea = tf.add(tf.matmul(x_wea_tem, weights['h1_wea']), biases['b1_wea'])\n",
    "    #layer_1_wea = tf.nn.relu(layer_1_wea)\n",
    "\n",
    "    #out_layer_wea = tf.add(tf.matmul(layer_1_wea, weights['out_wea']), biases['bout_wea'])\n",
    "    #out_layer = tf.add(out_layer, tf.reshape(out_layer_wea, [272, 1]))\n",
    "\n",
    "\n",
    "    \n",
    "    return Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_corr_final(frequency, horizon, learning_rate, decay,batch_size, n_hidden_vec1,n_hidden_vec2,n_hidden_vec3,keep, early_stop_th,training_epochs, reg1, reg2):\n",
    "    # set size\n",
    "    #sn = 3 # station number\n",
    "\n",
    "    \n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    predic_res = []\n",
    "    Y_true = []\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    \n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "    n_hidden_vec1 = int(n_hidden_vec1)\n",
    "    n_hidden_vec2 = int(n_hidden_vec2)\n",
    "    n_hidden_vec3 = int(n_hidden_vec3)\n",
    "    early_stop_th = int(early_stop_th)\n",
    "    training_epochs = int(training_epochs)\n",
    "    \n",
    "    early_stop_k=0\n",
    "    display_step = 1\n",
    "    best_val = 10000\n",
    "    traing_error = 0\n",
    "    test_error = 0\n",
    "    # Network Parameters\n",
    "\n",
    "    #n_classes = 2 # MNIST total classes (0-9 digits) # n_classes is for classification only\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None, lstm_steps, sn, feature_in]) # X is the input signal\n",
    "    #X_weather = tf.placeholder(tf.float32, [None, 9 * frequency2]) # X_weather weather and holiday information (9 is the feature number)\n",
    "    A = tf.placeholder(tf.float32, [None, n_A_vec]) # A is the normalized adj matrix\n",
    "    oldA = tf.placeholder(tf.float32, [n_input, n_input])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_output_vec]) # y is the regression output\n",
    "    #num = tf.placeholder(tf.int32,[1, 1] )\n",
    "\n",
    "    #Xtem = tf.placeholder(tf.float32, [n_input, frequency]) # for each row of X, A, Y, it can be reshaped to Xtem, Atem, Ytem\n",
    "    #Atem = tf.placeholder(tf.float32, [n_input, n_input]) # \n",
    "    #Ytem = tf.placeholder(tf.float32, [n_input, 1]) #\n",
    "\n",
    "    #Ypre = tf.placeholder(tf.float32, [None, n_output_vec])\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([frequency, n_hidden_vec1]), dtype=np.float32),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_vec1, n_hidden_vec2]), dtype=np.float32),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_vec2, n_hidden_vec3]), dtype=np.float32),\n",
    "        #'h4': tf.Variable(tf.random_normal([n_hidden_vec3, n_hidden_vec4])),\n",
    "        'outg': tf.Variable(tf.random_normal([sn*n_hidden_vec3, n_hidden_vec4]), dtype=np.float32), \n",
    "        'out': tf.Variable(tf.random_normal([num_hidden, Y_whole.shape[1]]), dtype=np.float32), # dont forget to change n_hidden_vec1 when add/delete layers\n",
    "        #'f1': tf.Variable(tf.random_normal([272*n_hidden_vec3, 100])),\n",
    "        #'f2': tf.Variable(tf.random_normal([50, 10])),\n",
    "        #'f3': tf.Variable(tf.random_normal([100, 272])),\n",
    "        'A1': tf.Variable(tf.random_normal([n_input,n_input]), dtype=np.float32),\n",
    "        'A2': tf.Variable(tf.random_normal([n_input,n_input]), dtype=np.float32),\n",
    "        'A3': tf.Variable(tf.random_normal([n_input,n_input]), dtype=np.float32),\n",
    "        #'A4': tf.Variable(tf.random_normal([n_input,n_input])),\n",
    "        #'h1_wea': tf.Variable(tf.random_normal([9*frequency2, n_hidden_weather1])),\n",
    "        #'out_wea': tf.Variable(tf.random_normal([n_hidden_weather1, n_input]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([1, n_hidden_vec1]), dtype=np.float32),# n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        'b2': tf.Variable(tf.random_normal([1, n_hidden_vec2]), dtype=np.float32), #n_hidden_vec2])),\n",
    "        'b3': tf.Variable(tf.random_normal([1, n_hidden_vec3]), dtype=np.float32),#n_hidden_vec3])),\n",
    "        #'b4': tf.Variable(tf.random_normal([n_input, n_hidden_vec4])),\n",
    "        #'b1': tf.Variable(tf.random_normal([n_input,n_hidden_vec1])),# bias all the same??? dont forget to test\n",
    "        #'b2': tf.Variable(tf.random_normal([n_input,n_hidden_vec2])),\n",
    "        #'b3': tf.Variable(tf.random_normal([n_input,n_hidden_vec3])),\n",
    "        #'bf1': tf.Variable(tf.random_normal([1, 100])), \n",
    "        #'bf2': tf.Variable(tf.random_normal([1, 10])), \n",
    "        #'bf3': tf.Variable(tf.random_normal([1, 272])), \n",
    "        'boutg': tf.Variable(tf.random_normal([1, n_hidden_vec4]), dtype=np.float32), \n",
    "        'bout': tf.Variable(tf.random_normal([Y_whole.shape[1]]), dtype=np.float32), \n",
    "        #'b1_wea': tf.Variable(tf.random_normal([1, n_hidden_weather1])), \n",
    "        #'bout_wea': tf.Variable(tf.random_normal([1, n_input])), \n",
    "    }\n",
    "    \n",
    "    with tf.variable_scope('lstm'):\n",
    "        lstm = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(num_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "        rnn_input_seq = tf.unstack(X, lstm_steps, 1) # lstm_steps is the 2nd variable\n",
    "\n",
    "        for i in range(lstm_steps):\n",
    "            rnn_input_seq[i] = gcn(rnn_input_seq[i], weights, biases, batch_size,n_input, frequency, 1, n_output_vec)\n",
    "            #print (rnn_input_seq[i].shape)\n",
    "        outputs, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs[-1], [-1, num_hidden])\n",
    "        #print ('123here!!!!!!!!!!!')\n",
    "        pred = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        #print (pred)\n",
    "        #pred = tf.reshape(pred, [-1, Y_whole.shape[1]])\n",
    "        #print ('here!!!!!!!!!!!')\n",
    "        pred = scaler.inverse_transform(pred)\n",
    "        Y_true_tr = scaler.inverse_transform(Y)\n",
    "\n",
    "        cost = tf.reduce_mean(tf.pow(pred - Y_true_tr, 2)) \n",
    "        #print (cost)\n",
    "        \n",
    "    with tf.variable_scope('lstm', reuse=True):\n",
    "        rnn_input_seq_val = tf.unstack(X, lstm_steps, 1)\n",
    "\n",
    "        for i in range(lstm_steps):\n",
    "            rnn_input_seq_val[i] = gcn(rnn_input_seq_val[i], weights, biases, batch_size,n_input,frequency, 2, n_output_vec)\n",
    "        outputs_val, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq_val, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs_val[-1], [-1, num_hidden])\n",
    "\n",
    "        pred_val = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        pred_val = scaler.inverse_transform(pred_val)\n",
    "        Y_true_val = scaler.inverse_transform(Y)\n",
    "        \n",
    "        cost_val =  tf.reduce_mean(tf.pow(pred_val - Y_true_val, 2)) \n",
    "        #print ('234here!!!!!!!!!!!')\n",
    "    with tf.variable_scope('lstm', reuse=True):\n",
    "        rnn_input_seq_test = tf.unstack(X, lstm_steps, 1)\n",
    "        \n",
    "        for i in range(lstm_steps):\n",
    "            rnn_input_seq_test[i] = gcn(rnn_input_seq_test[i], weights, biases, batch_size,n_input,frequency, 3, n_output_vec)\n",
    "        outputs_test, states = tf.contrib.rnn.static_rnn(lstm, rnn_input_seq_test, dtype=tf.float32)\n",
    "        output_reshape = tf.reshape(outputs_test[-1], [-1, num_hidden])\n",
    "\n",
    "        pred_tes = tf.matmul(output_reshape, weights['out']) + biases['bout']\n",
    "        pred_tes = scaler.inverse_transform(pred_tes)\n",
    "        Y_true_tes = scaler.inverse_transform(Y)\n",
    "        \n",
    "        cost_tes = tf.reduce_mean(tf.pow(pred_tes - Y_true_tes, 2)) \n",
    "        \n",
    "        #print ('345here!!!!!!!!!!!')\n",
    "    #rmse\n",
    "    #cost_tes = tf.reduce_mean(tf.pow(pred_tes-Y, 2))\n",
    "    # cross-entropy for classification\n",
    "    # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y_train))\n",
    "    # ratio = tf.abs(tf.reduce_sum(pred)-tf.reduce_sum(Y))/tf.reduce_sum(Y)\n",
    "    #zero = 0\n",
    "    #ratio = tf.reduce_mean(tf.divide(tf.where(tf.not_equal(Y, zero), np.abs(pred-Y), tf.zeros(Y.get_shape(), tf.float32)), tf.where(tf.not_equal(Y, zero), Y, tf.ones(Y.get_shape(), tf.float32))))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay).minimize(cost)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #total_val_cost = []\n",
    "    #total_val_ratio = []\n",
    "\n",
    "    # learning start from \n",
    "\n",
    "    #index = daily_bike[(daily_bike['year'] == 2016) & (daily_bike['monthofyear'] == 1) & (daily_bike['dayofmonth'] == 1)].index.tolist()[0]\n",
    "    #A_hat = normalize_adj(corr_matrix_trips)\n",
    "    #print(A_hat)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(num_train/batch_size) #int(num_train/batch_size)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                #print (Y_training[i*batch_size:(i+1)*batch_size,].size())\n",
    "                #num = batch_size\n",
    "                _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n",
    "                                                      Y: Y_training[i*batch_size:(i+1)*batch_size,],  \n",
    "                                                              keep_prob: keep})\n",
    "                #print (preds)\n",
    "                #print (trueval)\n",
    "                #print (\"Epoch:\", '%04d' % (epoch+1), \"batch: \", i, \"batch cost=\", \\\n",
    "                #    \"{:.9f}\".format(c))\n",
    "                #print ('here!!!!!!!!!!!!!!!!')\n",
    "                avg_cost += c * batch_size #/ total_batch \n",
    "                #Display logs per epoch step\n",
    "                \n",
    "            # rest part of training dataset\n",
    "            #num = num_train - total_batch*batch_size \n",
    "            if total_batch * batch_size != num_train:\n",
    "                _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[total_batch*batch_size:num_train,], \n",
    "                                          Y: Y_training[total_batch*batch_size:num_train,],\n",
    "                                                  keep_prob: keep})\n",
    "                avg_cost += c * (num_train - total_batch*batch_size)\n",
    "            \n",
    "            avg_cost = np.sqrt(avg_cost / num_train)\n",
    "            \n",
    "            if epoch % display_step == 0:\n",
    "                print (\"Epoch:\", '%04d' % (epoch+1), \"Training RMSE=\", \\\n",
    "                    \"{:.9f}\".format(avg_cost)) #np.sqrt(avg_cost)\n",
    "                \n",
    "            # also use batch to save memory\n",
    "            # validation\n",
    "            c_val = 0.\n",
    "            total_bat_val = int(num_val/batch_size)\n",
    "            for i in range(total_bat_val):\n",
    "                #num = batch_size\n",
    "                c_val_b = sess.run([cost_val], feed_dict={X: X_val[i*batch_size:(i+1)*batch_size,], \n",
    "                                                          Y: Y_val[i*batch_size:(i+1)*batch_size,],   keep_prob:1})\n",
    "                c_val += c_val_b[0]*batch_size\n",
    "            \n",
    "            if total_bat_val * batch_size != num_val:\n",
    "                #num = num_val - total_bat_val*batch_size\n",
    "                c_val_b = sess.run([cost_val], feed_dict={X: X_val[total_bat_val*batch_size:num_val,], \n",
    "                                                          Y: Y_val[total_bat_val*batch_size:num_val,],  keep_prob:1})\n",
    "                c_val += c_val_b[0] * (num_val - total_bat_val*batch_size)\n",
    "                \n",
    "            c_val = np.sqrt(c_val / num_val)\n",
    "            \n",
    "            print(\"Validation RMSE: \", c_val)\n",
    "            \n",
    "            # test\n",
    "            c_tes = 0.\n",
    "            total_bat_test = int(num_test/batch_size)\n",
    "            \n",
    "            pre_test_tem = [] # save the prediction results\n",
    "            Y_tes_true = []\n",
    "            \n",
    "            for i in range(total_bat_test):\n",
    "                #num = batch_size\n",
    "                c_tes_b, pred_tes1, Y_tes_batch = sess.run([cost_tes, pred_tes, Y_true_tes], feed_dict={X: X_test[i*batch_size:(i+1)*batch_size,],\n",
    "                                                                               Y: Y_test[i*batch_size:(i+1)*batch_size,],  keep_prob: 1})\n",
    "                c_tes += c_tes_b*batch_size\n",
    "\n",
    "                #print (cost_h)\n",
    "                #print (pred_tes1.shape)\n",
    "                pre_test_tem.append(pred_tes1)\n",
    "                Y_tes_true.append(Y_tes_batch)\n",
    "                \n",
    "            if total_bat_test * batch_size != num_test:\n",
    "                #num = num_test - total_bat_test*batch_size\n",
    "                c_tes_b, pred_tes1, Y_tes_batch = sess.run([cost_tes, pred_tes, Y_true_tes], feed_dict={X: X_test[total_bat_test*batch_size:num_test,],\n",
    "                                                                               Y: Y_test[total_bat_test*batch_size:num_test,],  keep_prob: 1})\n",
    "                c_tes += c_tes_b * (num_test - total_bat_test*batch_size) \n",
    "                 \n",
    "                \n",
    "                #print (pred_tes1.shape)\n",
    "                pre_test_tem.append(pred_tes1)\n",
    "                Y_tes_true.append(Y_tes_batch)\n",
    "            \n",
    "            #print (c_tes_h.shape)\n",
    "            pre_test_tem = np.concatenate(pre_test_tem, axis = 0)\n",
    "            #print (pre_test_tem.shape)\n",
    "            Y_tes_true = np.concatenate(Y_tes_true, axis = 0)\n",
    "            \n",
    "            c_tes = np.sqrt(c_tes / num_test)\n",
    "            #c_tes_h = c_tes_h / num_test\n",
    "            \n",
    "            print(\"Test RMSE: \", c_tes)\n",
    "            #print(\"predic step: \", cost_by_hor)\n",
    "\n",
    "            if c_val < best_val:\n",
    "                best_val = c_val\n",
    "                #saver.save(sess, './bikesharing_graph_2_th_point1')\n",
    "                test_error = c_tes\n",
    "                \n",
    "                traing_error = avg_cost#np.sqrt(avg_cost)\n",
    "                early_stop_k = 0 # reset to 0\n",
    "                #print (pred_tes1)\n",
    "                predic_res = pre_test_tem\n",
    "                Y_true = Y_tes_true\n",
    "                #predic_step = cost_by_hor\n",
    "\n",
    "            # early stopping\n",
    "            if c_val >= best_val:\n",
    "                early_stop_k += 1\n",
    "\n",
    "            # threshold\n",
    "            if early_stop_k == early_stop_th:\n",
    "              #  print (\"early stopping...\")\n",
    "                break\n",
    "            \n",
    "\n",
    "        print(\"epoch is \", epoch)\n",
    "        print(\"training RMSE is \", traing_error)\n",
    "        print(\"Optimization Finished! the lowest validation RMSE is \", best_val)#(np.sqrt(best_val)))\n",
    "        print(\"The test RMSE is \", test_error)#(np.sqrt(test_error)))\n",
    "    \n",
    "    #test_Y = Y_test\n",
    "    #test_error = np.sqrt(test_error)\n",
    "    return test_error, predic_res, Y_true#, A1#, predic_step#, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training RMSE= 8.214547363\n",
      "Validation RMSE:  7.9315295255085365\n",
      "Test RMSE:  7.029860295607363\n",
      "Epoch: 0002 Training RMSE= 7.511738251\n",
      "Validation RMSE:  7.558921408636187\n",
      "Test RMSE:  6.6482448527556794\n",
      "Epoch: 0003 Training RMSE= 7.252211687\n",
      "Validation RMSE:  7.293514286472102\n",
      "Test RMSE:  6.358865403511125\n",
      "Epoch: 0004 Training RMSE= 6.985178979\n",
      "Validation RMSE:  7.0327229631204\n",
      "Test RMSE:  6.074397103490884\n",
      "Epoch: 0005 Training RMSE= 6.724559781\n",
      "Validation RMSE:  6.7927983127159255\n",
      "Test RMSE:  5.8112658759458435\n",
      "Epoch: 0006 Training RMSE= 6.487968754\n",
      "Validation RMSE:  6.575557252890066\n",
      "Test RMSE:  5.571095715175908\n",
      "Epoch: 0007 Training RMSE= 6.283824685\n",
      "Validation RMSE:  6.451980304562613\n",
      "Test RMSE:  5.37408049059416\n",
      "Epoch: 0008 Training RMSE= 6.096662778\n",
      "Validation RMSE:  6.234002353172115\n",
      "Test RMSE:  5.173329220914353\n",
      "Epoch: 0009 Training RMSE= 5.957270295\n",
      "Validation RMSE:  6.124635023803842\n",
      "Test RMSE:  5.039449135784891\n",
      "Epoch: 0010 Training RMSE= 5.756134758\n",
      "Validation RMSE:  5.889146938108301\n",
      "Test RMSE:  4.829179173415771\n",
      "Epoch: 0011 Training RMSE= 5.534133496\n",
      "Validation RMSE:  5.701425210225546\n",
      "Test RMSE:  4.6702482012970945\n",
      "Epoch: 0012 Training RMSE= 5.422471168\n",
      "Validation RMSE:  5.599180090009509\n",
      "Test RMSE:  4.55360643175707\n",
      "Epoch: 0013 Training RMSE= 5.271487505\n",
      "Validation RMSE:  5.5036254551277\n",
      "Test RMSE:  4.463631319312622\n",
      "Epoch: 0014 Training RMSE= 5.179994443\n",
      "Validation RMSE:  5.416819917516072\n",
      "Test RMSE:  4.368313830081192\n",
      "Epoch: 0015 Training RMSE= 5.145169248\n",
      "Validation RMSE:  5.337672296463468\n",
      "Test RMSE:  4.291177474847199\n",
      "Epoch: 0016 Training RMSE= 5.041869827\n",
      "Validation RMSE:  5.281153661086221\n",
      "Test RMSE:  4.22991708696779\n",
      "Epoch: 0017 Training RMSE= 5.001308546\n",
      "Validation RMSE:  5.227290856853905\n",
      "Test RMSE:  4.190369491127002\n",
      "Epoch: 0018 Training RMSE= 4.980741136\n",
      "Validation RMSE:  5.199738554617622\n",
      "Test RMSE:  4.152973748087149\n",
      "Epoch: 0019 Training RMSE= 4.733734410\n",
      "Validation RMSE:  4.83977172896499\n",
      "Test RMSE:  3.9408913191876325\n",
      "Epoch: 0020 Training RMSE= 4.488073078\n",
      "Validation RMSE:  4.700157166957718\n",
      "Test RMSE:  3.787837571815743\n",
      "Epoch: 0021 Training RMSE= 4.345204490\n",
      "Validation RMSE:  4.643347817996496\n",
      "Test RMSE:  3.7598333496335448\n",
      "Epoch: 0022 Training RMSE= 4.256370393\n",
      "Validation RMSE:  4.558925744056305\n",
      "Test RMSE:  3.669753545338789\n",
      "Epoch: 0023 Training RMSE= 4.178844240\n",
      "Validation RMSE:  4.467801240094504\n",
      "Test RMSE:  3.5602308709580486\n",
      "Epoch: 0024 Training RMSE= 4.109944386\n",
      "Validation RMSE:  4.428195708900875\n",
      "Test RMSE:  3.5354374726282134\n",
      "Epoch: 0025 Training RMSE= 4.036188244\n",
      "Validation RMSE:  4.403597256396106\n",
      "Test RMSE:  3.5154910591889648\n",
      "Epoch: 0026 Training RMSE= 3.996993163\n",
      "Validation RMSE:  4.3175062340104375\n",
      "Test RMSE:  3.414651710767464\n",
      "Epoch: 0027 Training RMSE= 3.963495696\n",
      "Validation RMSE:  4.274052716870361\n",
      "Test RMSE:  3.3672860611300117\n",
      "Epoch: 0028 Training RMSE= 3.928494075\n",
      "Validation RMSE:  4.256328975264084\n",
      "Test RMSE:  3.3919510599510465\n",
      "Epoch: 0029 Training RMSE= 3.889503029\n",
      "Validation RMSE:  4.234712256482444\n",
      "Test RMSE:  3.3136049263192273\n",
      "Epoch: 0030 Training RMSE= 3.853626978\n",
      "Validation RMSE:  4.187097216920438\n",
      "Test RMSE:  3.2938471625978942\n",
      "Epoch: 0031 Training RMSE= 3.809328136\n",
      "Validation RMSE:  4.151117123915542\n",
      "Test RMSE:  3.2782454040350837\n",
      "Epoch: 0032 Training RMSE= 3.787923526\n",
      "Validation RMSE:  4.141391841487002\n",
      "Test RMSE:  3.2522589828950106\n",
      "Epoch: 0033 Training RMSE= 3.741161185\n",
      "Validation RMSE:  4.0976593337579255\n",
      "Test RMSE:  3.26592402937132\n",
      "Epoch: 0034 Training RMSE= 3.757792933\n",
      "Validation RMSE:  4.038923368142105\n",
      "Test RMSE:  3.198301190712892\n",
      "Epoch: 0035 Training RMSE= 3.677863762\n",
      "Validation RMSE:  4.026176276019049\n",
      "Test RMSE:  3.221858107247823\n",
      "Epoch: 0036 Training RMSE= 3.652037341\n",
      "Validation RMSE:  4.019175284799132\n",
      "Test RMSE:  3.2303398872329736\n",
      "Epoch: 0037 Training RMSE= 3.622281785\n",
      "Validation RMSE:  3.999516755954205\n",
      "Test RMSE:  3.203510763617935\n",
      "Epoch: 0038 Training RMSE= 3.597456677\n",
      "Validation RMSE:  3.967517571317226\n",
      "Test RMSE:  3.185994484594328\n",
      "Epoch: 0039 Training RMSE= 3.579765350\n",
      "Validation RMSE:  3.9387225720495667\n",
      "Test RMSE:  3.156860972274944\n",
      "Epoch: 0040 Training RMSE= 3.626166893\n",
      "Validation RMSE:  3.940371541063395\n",
      "Test RMSE:  3.16530281528645\n",
      "Epoch: 0041 Training RMSE= 3.588771709\n",
      "Validation RMSE:  3.906915043485453\n",
      "Test RMSE:  3.138251323334096\n",
      "Epoch: 0042 Training RMSE= 3.538380531\n",
      "Validation RMSE:  3.877392491732379\n",
      "Test RMSE:  3.1321057214444687\n",
      "Epoch: 0043 Training RMSE= 3.546811081\n",
      "Validation RMSE:  3.8937251596898386\n",
      "Test RMSE:  3.1703128820337296\n",
      "Epoch: 0044 Training RMSE= 3.535392013\n",
      "Validation RMSE:  3.8682858535326874\n",
      "Test RMSE:  3.0649986194899967\n",
      "Epoch: 0045 Training RMSE= 3.507251829\n",
      "Validation RMSE:  3.8581025037383085\n",
      "Test RMSE:  3.0535575942870996\n",
      "Epoch: 0046 Training RMSE= 3.496363140\n",
      "Validation RMSE:  3.908668196809996\n",
      "Test RMSE:  3.1066250549716705\n",
      "Epoch: 0047 Training RMSE= 3.497739007\n",
      "Validation RMSE:  3.872526917828859\n",
      "Test RMSE:  3.085458375458699\n",
      "Epoch: 0048 Training RMSE= 3.479287310\n",
      "Validation RMSE:  3.8589442703876933\n",
      "Test RMSE:  3.052818018962656\n",
      "Epoch: 0049 Training RMSE= 3.475467597\n",
      "Validation RMSE:  3.8456876879411794\n",
      "Test RMSE:  3.10489611834288\n",
      "Epoch: 0050 Training RMSE= 3.479900022\n",
      "Validation RMSE:  3.8649921389520805\n",
      "Test RMSE:  3.0852669675487343\n",
      "Epoch: 0051 Training RMSE= 3.460567412\n",
      "Validation RMSE:  3.914861784014009\n",
      "Test RMSE:  3.161705438969038\n",
      "Epoch: 0052 Training RMSE= 3.461306109\n",
      "Validation RMSE:  3.8234086235524223\n",
      "Test RMSE:  3.0520383464809044\n",
      "Epoch: 0053 Training RMSE= 3.452168570\n",
      "Validation RMSE:  3.8371028334274384\n",
      "Test RMSE:  3.07999409891777\n",
      "Epoch: 0054 Training RMSE= 3.453389685\n",
      "Validation RMSE:  3.8495691182176555\n",
      "Test RMSE:  3.1119089201124117\n",
      "Epoch: 0055 Training RMSE= 3.465671204\n",
      "Validation RMSE:  4.47109763628364\n",
      "Test RMSE:  4.014608766679452\n",
      "Epoch: 0056 Training RMSE= 3.509059720\n",
      "Validation RMSE:  3.8129437297807858\n",
      "Test RMSE:  3.0592156139778357\n",
      "Epoch: 0057 Training RMSE= 3.429163253\n",
      "Validation RMSE:  3.843999094761515\n",
      "Test RMSE:  3.048482695713268\n",
      "Epoch: 0058 Training RMSE= 3.446779405\n",
      "Validation RMSE:  3.840239992390803\n",
      "Test RMSE:  3.0694995997916887\n",
      "Epoch: 0059 Training RMSE= 3.423638434\n",
      "Validation RMSE:  3.810514464599802\n",
      "Test RMSE:  3.078416684655083\n",
      "Epoch: 0060 Training RMSE= 3.421866444\n",
      "Validation RMSE:  3.8881855252903184\n",
      "Test RMSE:  3.081935982627841\n",
      "Epoch: 0061 Training RMSE= 3.444943626\n",
      "Validation RMSE:  3.832306240531388\n",
      "Test RMSE:  3.065393248964597\n",
      "Epoch: 0062 Training RMSE= 3.415779761\n",
      "Validation RMSE:  3.802367770211428\n",
      "Test RMSE:  3.024646093615404\n",
      "Epoch: 0063 Training RMSE= 3.444507787\n",
      "Validation RMSE:  3.8250824963019587\n",
      "Test RMSE:  3.076886387752991\n",
      "Epoch: 0064 Training RMSE= 3.406129894\n",
      "Validation RMSE:  3.793295283459149\n",
      "Test RMSE:  3.025958880256682\n",
      "Epoch: 0065 Training RMSE= 3.409300532\n",
      "Validation RMSE:  3.7892629668699804\n",
      "Test RMSE:  2.9927813386405155\n",
      "Epoch: 0066 Training RMSE= 3.409747241\n",
      "Validation RMSE:  3.8175741729825163\n",
      "Test RMSE:  3.060808259456416\n",
      "Epoch: 0067 Training RMSE= 3.398040194\n",
      "Validation RMSE:  3.7623547801637858\n",
      "Test RMSE:  3.015092041673045\n",
      "Epoch: 0068 Training RMSE= 3.401642439\n",
      "Validation RMSE:  3.7648509009476014\n",
      "Test RMSE:  3.0208505608078386\n",
      "Epoch: 0069 Training RMSE= 3.390882505\n",
      "Validation RMSE:  3.7859847005624663\n",
      "Test RMSE:  2.9977659808354775\n",
      "Epoch: 0070 Training RMSE= 3.380343326\n",
      "Validation RMSE:  3.7871111377477846\n",
      "Test RMSE:  3.037572068666342\n",
      "Epoch: 0071 Training RMSE= 3.384726454\n",
      "Validation RMSE:  3.77147059386238\n",
      "Test RMSE:  2.996314327868757\n",
      "Epoch: 0072 Training RMSE= 3.376359341\n",
      "Validation RMSE:  3.782727039731614\n",
      "Test RMSE:  2.9906217696631834\n",
      "Epoch: 0073 Training RMSE= 3.386835426\n",
      "Validation RMSE:  3.8065306411285573\n",
      "Test RMSE:  3.019069897937072\n",
      "Epoch: 0074 Training RMSE= 3.383052643\n",
      "Validation RMSE:  3.8023897160796127\n",
      "Test RMSE:  3.0304079360937553\n",
      "Epoch: 0075 Training RMSE= 3.380230989\n",
      "Validation RMSE:  3.762764441743473\n",
      "Test RMSE:  3.0060371609089436\n",
      "Epoch: 0076 Training RMSE= 3.367737465\n",
      "Validation RMSE:  3.7770077576662384\n",
      "Test RMSE:  3.030925772719267\n",
      "Epoch: 0077 Training RMSE= 3.361293178\n",
      "Validation RMSE:  3.8033280656150112\n",
      "Test RMSE:  3.0179038652816272\n",
      "Epoch: 0078 Training RMSE= 3.366433369\n",
      "Validation RMSE:  3.76962833575996\n",
      "Test RMSE:  3.0000306366309806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0079 Training RMSE= 3.353073178\n",
      "Validation RMSE:  3.863914445569659\n",
      "Test RMSE:  3.113817922555015\n",
      "Epoch: 0080 Training RMSE= 3.342843255\n",
      "Validation RMSE:  3.7160602106158236\n",
      "Test RMSE:  2.949900365213732\n",
      "Epoch: 0081 Training RMSE= 3.368715496\n",
      "Validation RMSE:  3.815346765019847\n",
      "Test RMSE:  3.0543751666695598\n",
      "Epoch: 0082 Training RMSE= 3.337321082\n",
      "Validation RMSE:  3.7081732483217515\n",
      "Test RMSE:  2.9550733244781933\n",
      "Epoch: 0083 Training RMSE= 3.315833922\n",
      "Validation RMSE:  3.7129404545249116\n",
      "Test RMSE:  2.9607145280852705\n",
      "Epoch: 0084 Training RMSE= 3.321336928\n",
      "Validation RMSE:  3.778222377333098\n",
      "Test RMSE:  2.997538430903743\n",
      "Epoch: 0085 Training RMSE= 3.312777180\n",
      "Validation RMSE:  3.6924289800365186\n",
      "Test RMSE:  2.986564112957508\n",
      "Epoch: 0086 Training RMSE= 3.302854689\n",
      "Validation RMSE:  3.7005208190159147\n",
      "Test RMSE:  2.9753136605716346\n",
      "Epoch: 0087 Training RMSE= 3.283417565\n",
      "Validation RMSE:  3.7358225805970404\n",
      "Test RMSE:  3.041561246813096\n",
      "Epoch: 0088 Training RMSE= 3.289168860\n",
      "Validation RMSE:  3.6600770402314877\n",
      "Test RMSE:  2.9249424056553246\n",
      "Epoch: 0089 Training RMSE= 3.280808962\n",
      "Validation RMSE:  3.6484853345198\n",
      "Test RMSE:  2.920982020389708\n",
      "Epoch: 0090 Training RMSE= 3.254615573\n",
      "Validation RMSE:  3.7898327860519294\n",
      "Test RMSE:  3.036379181150309\n",
      "Epoch: 0091 Training RMSE= 3.270074505\n",
      "Validation RMSE:  3.6456938726000603\n",
      "Test RMSE:  2.8624460373502143\n",
      "Epoch: 0092 Training RMSE= 3.239241480\n",
      "Validation RMSE:  3.6026640095064466\n",
      "Test RMSE:  2.821957306701818\n",
      "Epoch: 0093 Training RMSE= 3.241609931\n",
      "Validation RMSE:  3.651503457084606\n",
      "Test RMSE:  2.8877029083430705\n",
      "Epoch: 0094 Training RMSE= 3.216044302\n",
      "Validation RMSE:  3.588619318077205\n",
      "Test RMSE:  2.8241349625986443\n",
      "Epoch: 0095 Training RMSE= 3.211926117\n",
      "Validation RMSE:  3.628091940454025\n",
      "Test RMSE:  2.836451754432209\n",
      "Epoch: 0096 Training RMSE= 3.203491197\n",
      "Validation RMSE:  3.649916877845226\n",
      "Test RMSE:  2.87495555014042\n",
      "Epoch: 0097 Training RMSE= 3.186185873\n",
      "Validation RMSE:  3.606454898295716\n",
      "Test RMSE:  2.8415296920548743\n",
      "Epoch: 0098 Training RMSE= 3.177382753\n",
      "Validation RMSE:  3.6276951997740077\n",
      "Test RMSE:  2.847142553894779\n",
      "Epoch: 0099 Training RMSE= 3.166376838\n",
      "Validation RMSE:  3.5950338309530583\n",
      "Test RMSE:  2.831997028012521\n",
      "Epoch: 0100 Training RMSE= 3.162751962\n",
      "Validation RMSE:  3.5824614654406526\n",
      "Test RMSE:  2.798756824125597\n",
      "Epoch: 0101 Training RMSE= 3.142136608\n",
      "Validation RMSE:  3.528274362678237\n",
      "Test RMSE:  2.762904019392745\n",
      "Epoch: 0102 Training RMSE= 3.140950891\n",
      "Validation RMSE:  3.5492571254958833\n",
      "Test RMSE:  2.797896853840984\n",
      "Epoch: 0103 Training RMSE= 3.109455610\n",
      "Validation RMSE:  3.5062281847596437\n",
      "Test RMSE:  2.696144572651961\n",
      "Epoch: 0104 Training RMSE= 3.104481413\n",
      "Validation RMSE:  3.5177939273058954\n",
      "Test RMSE:  2.729972521395604\n",
      "Epoch: 0105 Training RMSE= 3.076468488\n",
      "Validation RMSE:  3.513120513625403\n",
      "Test RMSE:  2.744812841653428\n",
      "Epoch: 0106 Training RMSE= 3.061263960\n",
      "Validation RMSE:  3.4562959223036676\n",
      "Test RMSE:  2.627723597876875\n",
      "Epoch: 0107 Training RMSE= 3.048912025\n",
      "Validation RMSE:  3.4804003159702073\n",
      "Test RMSE:  2.698462108329771\n",
      "Epoch: 0108 Training RMSE= 3.025609705\n",
      "Validation RMSE:  3.410006525307506\n",
      "Test RMSE:  2.613430047467617\n",
      "Epoch: 0109 Training RMSE= 3.008135161\n",
      "Validation RMSE:  3.4413771525266768\n",
      "Test RMSE:  2.642463134609523\n",
      "Epoch: 0110 Training RMSE= 2.990937827\n",
      "Validation RMSE:  3.4430293608252014\n",
      "Test RMSE:  2.6610215358041867\n",
      "Epoch: 0111 Training RMSE= 2.975740102\n",
      "Validation RMSE:  3.4108466184761075\n",
      "Test RMSE:  2.6319818289949857\n",
      "Epoch: 0112 Training RMSE= 2.970014058\n",
      "Validation RMSE:  3.4112218216696393\n",
      "Test RMSE:  2.636000064815121\n",
      "Epoch: 0113 Training RMSE= 2.956393489\n",
      "Validation RMSE:  3.3431444778488286\n",
      "Test RMSE:  2.5487251348624933\n",
      "Epoch: 0114 Training RMSE= 2.937695079\n",
      "Validation RMSE:  3.3369921908723055\n",
      "Test RMSE:  2.5450312019339054\n",
      "Epoch: 0115 Training RMSE= 2.939319210\n",
      "Validation RMSE:  3.3458327821103246\n",
      "Test RMSE:  2.5674981475496526\n",
      "Epoch: 0116 Training RMSE= 2.924128515\n",
      "Validation RMSE:  3.3449877293109345\n",
      "Test RMSE:  2.5671081512131555\n",
      "Epoch: 0117 Training RMSE= 2.912655807\n",
      "Validation RMSE:  3.3297625887212012\n",
      "Test RMSE:  2.5681437702843786\n",
      "Epoch: 0118 Training RMSE= 2.899208850\n",
      "Validation RMSE:  3.3313114511286357\n",
      "Test RMSE:  2.558852227819324\n",
      "Epoch: 0119 Training RMSE= 2.892255466\n",
      "Validation RMSE:  3.305282358870841\n",
      "Test RMSE:  2.5331233592184548\n",
      "Epoch: 0120 Training RMSE= 2.881726296\n",
      "Validation RMSE:  3.2941834369068173\n",
      "Test RMSE:  2.531613594597178\n",
      "Epoch: 0121 Training RMSE= 2.879227764\n",
      "Validation RMSE:  3.2694789285244754\n",
      "Test RMSE:  2.5056104647908723\n",
      "Epoch: 0122 Training RMSE= 2.867674542\n",
      "Validation RMSE:  3.2679981935302336\n",
      "Test RMSE:  2.5008563005697155\n",
      "Epoch: 0123 Training RMSE= 2.862034721\n",
      "Validation RMSE:  3.256437162266573\n",
      "Test RMSE:  2.4945433671599058\n",
      "Epoch: 0124 Training RMSE= 2.844932537\n",
      "Validation RMSE:  3.2811970297761257\n",
      "Test RMSE:  2.509057802363873\n",
      "Epoch: 0125 Training RMSE= 2.856174166\n",
      "Validation RMSE:  3.2559640154759792\n",
      "Test RMSE:  2.4964404514575027\n",
      "Epoch: 0126 Training RMSE= 2.841050944\n",
      "Validation RMSE:  3.2929291818940785\n",
      "Test RMSE:  2.541718300050004\n",
      "Epoch: 0127 Training RMSE= 2.840018643\n",
      "Validation RMSE:  3.293304752027632\n",
      "Test RMSE:  2.541681013431602\n",
      "Epoch: 0128 Training RMSE= 2.833104445\n",
      "Validation RMSE:  3.289450543272036\n",
      "Test RMSE:  2.5477125510809064\n",
      "Epoch: 0129 Training RMSE= 2.828861857\n",
      "Validation RMSE:  3.2723135828204044\n",
      "Test RMSE:  2.522644485252452\n",
      "Epoch: 0130 Training RMSE= 2.827399120\n",
      "Validation RMSE:  3.258504670525377\n",
      "Test RMSE:  2.5088742584338415\n",
      "Epoch: 0131 Training RMSE= 2.824716501\n",
      "Validation RMSE:  3.2388895197461145\n",
      "Test RMSE:  2.4755990353450352\n",
      "Epoch: 0132 Training RMSE= 2.824289497\n",
      "Validation RMSE:  3.2275366247863335\n",
      "Test RMSE:  2.476575207602453\n",
      "Epoch: 0133 Training RMSE= 2.811731089\n",
      "Validation RMSE:  3.2779943754719305\n",
      "Test RMSE:  2.5099325280410634\n",
      "Epoch: 0134 Training RMSE= 2.803479449\n",
      "Validation RMSE:  3.28692894824057\n",
      "Test RMSE:  2.563611743087526\n",
      "Epoch: 0135 Training RMSE= 2.808566458\n",
      "Validation RMSE:  3.2775826440978726\n",
      "Test RMSE:  2.559023708879148\n",
      "Epoch: 0136 Training RMSE= 2.807616852\n",
      "Validation RMSE:  3.2807911506623713\n",
      "Test RMSE:  2.5529618807620342\n",
      "Epoch: 0137 Training RMSE= 2.798524334\n",
      "Validation RMSE:  3.2864146322900583\n",
      "Test RMSE:  2.5549053134281703\n",
      "Epoch: 0138 Training RMSE= 2.798112576\n",
      "Validation RMSE:  3.2560767070910566\n",
      "Test RMSE:  2.500082443785258\n",
      "Epoch: 0139 Training RMSE= 2.801971315\n",
      "Validation RMSE:  3.210372429571202\n",
      "Test RMSE:  2.4656694241427055\n",
      "Epoch: 0140 Training RMSE= 2.783832714\n",
      "Validation RMSE:  3.22584510937723\n",
      "Test RMSE:  2.481490662095446\n",
      "Epoch: 0141 Training RMSE= 2.789523559\n",
      "Validation RMSE:  3.222251321032737\n",
      "Test RMSE:  2.467895214969323\n",
      "Epoch: 0142 Training RMSE= 2.787892521\n",
      "Validation RMSE:  3.2083422714373118\n",
      "Test RMSE:  2.46519305720889\n",
      "Epoch: 0143 Training RMSE= 2.785786904\n",
      "Validation RMSE:  3.223071706174799\n",
      "Test RMSE:  2.4831485240022673\n",
      "Epoch: 0144 Training RMSE= 2.777421206\n",
      "Validation RMSE:  3.2476183894824886\n",
      "Test RMSE:  2.512187101324007\n",
      "Epoch: 0145 Training RMSE= 2.775898764\n",
      "Validation RMSE:  3.242635305341892\n",
      "Test RMSE:  2.5018757935483897\n",
      "Epoch: 0146 Training RMSE= 2.778899462\n",
      "Validation RMSE:  3.2161383171868883\n",
      "Test RMSE:  2.464976166615743\n",
      "Epoch: 0147 Training RMSE= 2.771594081\n",
      "Validation RMSE:  3.2234266805166745\n",
      "Test RMSE:  2.476445673644946\n",
      "Epoch: 0148 Training RMSE= 2.770226301\n",
      "Validation RMSE:  3.220923199372519\n",
      "Test RMSE:  2.4644197561798715\n",
      "Epoch: 0149 Training RMSE= 2.767264808\n",
      "Validation RMSE:  3.2210669835375842\n",
      "Test RMSE:  2.4772939955503896\n",
      "Epoch: 0150 Training RMSE= 2.766053446\n",
      "Validation RMSE:  3.2053470998501203\n",
      "Test RMSE:  2.4589857324137436\n",
      "Epoch: 0151 Training RMSE= 2.769000318\n",
      "Validation RMSE:  3.19812529605091\n",
      "Test RMSE:  2.4511911824594423\n",
      "Epoch: 0152 Training RMSE= 2.759453991\n",
      "Validation RMSE:  3.192841000727967\n",
      "Test RMSE:  2.4536205204339723\n",
      "Epoch: 0153 Training RMSE= 2.760060578\n",
      "Validation RMSE:  3.2106531393174365\n",
      "Test RMSE:  2.4604447613263356\n",
      "Epoch: 0154 Training RMSE= 2.751487737\n",
      "Validation RMSE:  3.2557113421585893\n",
      "Test RMSE:  2.5054081118757106\n",
      "Epoch: 0155 Training RMSE= 2.753978574\n",
      "Validation RMSE:  3.252116944365204\n",
      "Test RMSE:  2.5143242073286243\n",
      "Epoch: 0156 Training RMSE= 2.755669255\n",
      "Validation RMSE:  3.232123065517561\n",
      "Test RMSE:  2.488653900651594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0157 Training RMSE= 2.753169241\n",
      "Validation RMSE:  3.1846035157513737\n",
      "Test RMSE:  2.446085554783561\n",
      "Epoch: 0158 Training RMSE= 2.757152684\n",
      "Validation RMSE:  3.1809349518606553\n",
      "Test RMSE:  2.448067962083316\n",
      "Epoch: 0159 Training RMSE= 2.740671583\n",
      "Validation RMSE:  3.2055351308094435\n",
      "Test RMSE:  2.4583546481313006\n",
      "Epoch: 0160 Training RMSE= 2.746513259\n",
      "Validation RMSE:  3.2111235330342134\n",
      "Test RMSE:  2.4618692238888333\n",
      "Epoch: 0161 Training RMSE= 2.746782371\n",
      "Validation RMSE:  3.2284068823730516\n",
      "Test RMSE:  2.4865832800941927\n",
      "Epoch: 0162 Training RMSE= 2.744401904\n",
      "Validation RMSE:  3.1833822542862014\n",
      "Test RMSE:  2.441421533155289\n",
      "Epoch: 0163 Training RMSE= 2.744650723\n",
      "Validation RMSE:  3.178240135005325\n",
      "Test RMSE:  2.4433055112468156\n",
      "Epoch: 0164 Training RMSE= 2.736630596\n",
      "Validation RMSE:  3.2336077678530493\n",
      "Test RMSE:  2.499534515914791\n",
      "Epoch: 0165 Training RMSE= 2.735935546\n",
      "Validation RMSE:  3.188999646028611\n",
      "Test RMSE:  2.4453930826285384\n",
      "Epoch: 0166 Training RMSE= 2.740480745\n",
      "Validation RMSE:  3.173624204686981\n",
      "Test RMSE:  2.4414236327506176\n",
      "Epoch: 0167 Training RMSE= 2.731716813\n",
      "Validation RMSE:  3.1811891426305974\n",
      "Test RMSE:  2.441517380283232\n",
      "Epoch: 0168 Training RMSE= 2.732711329\n",
      "Validation RMSE:  3.1821812403989225\n",
      "Test RMSE:  2.4411234211176023\n",
      "Epoch: 0169 Training RMSE= 2.725234583\n",
      "Validation RMSE:  3.1935235497505383\n",
      "Test RMSE:  2.453169220932788\n",
      "Epoch: 0170 Training RMSE= 2.736745241\n",
      "Validation RMSE:  3.2355969834215426\n",
      "Test RMSE:  2.5075522314471677\n",
      "Epoch: 0171 Training RMSE= 2.725731810\n",
      "Validation RMSE:  3.197111150497183\n",
      "Test RMSE:  2.4455109537051296\n",
      "Epoch: 0172 Training RMSE= 2.730276181\n",
      "Validation RMSE:  3.248037699527592\n",
      "Test RMSE:  2.518816942787829\n",
      "Epoch: 0173 Training RMSE= 2.721986344\n",
      "Validation RMSE:  3.2080053227848975\n",
      "Test RMSE:  2.4548451412484225\n",
      "Epoch: 0174 Training RMSE= 2.732577835\n",
      "Validation RMSE:  3.174037064937421\n",
      "Test RMSE:  2.4295206288625595\n",
      "Epoch: 0175 Training RMSE= 2.719708101\n",
      "Validation RMSE:  3.1887439843468353\n",
      "Test RMSE:  2.44714652317362\n",
      "Epoch: 0176 Training RMSE= 2.717906005\n",
      "Validation RMSE:  3.182563849180998\n",
      "Test RMSE:  2.4527487010631597\n",
      "Epoch: 0177 Training RMSE= 2.721694851\n",
      "Validation RMSE:  3.179252051077197\n",
      "Test RMSE:  2.4363973030665367\n",
      "Epoch: 0178 Training RMSE= 2.719959665\n",
      "Validation RMSE:  3.169606793081407\n",
      "Test RMSE:  2.427260090909023\n",
      "Epoch: 0179 Training RMSE= 2.715814202\n",
      "Validation RMSE:  3.1836953737854294\n",
      "Test RMSE:  2.438237152140857\n",
      "Epoch: 0180 Training RMSE= 2.714940131\n",
      "Validation RMSE:  3.2451422039196185\n",
      "Test RMSE:  2.5283275735278607\n",
      "Epoch: 0181 Training RMSE= 2.715190434\n",
      "Validation RMSE:  3.1635335597860856\n",
      "Test RMSE:  2.4274066387730997\n",
      "Epoch: 0182 Training RMSE= 2.717433233\n",
      "Validation RMSE:  3.1941028721031377\n",
      "Test RMSE:  2.437869215169718\n",
      "Epoch: 0183 Training RMSE= 2.708070956\n",
      "Validation RMSE:  3.161246471229596\n",
      "Test RMSE:  2.429365032421928\n",
      "Epoch: 0184 Training RMSE= 2.709415635\n",
      "Validation RMSE:  3.1602802417323006\n",
      "Test RMSE:  2.4272572914836754\n",
      "Epoch: 0185 Training RMSE= 2.710324212\n",
      "Validation RMSE:  3.225949134439842\n",
      "Test RMSE:  2.494610173917974\n",
      "Epoch: 0186 Training RMSE= 2.705927619\n",
      "Validation RMSE:  3.2176465485579895\n",
      "Test RMSE:  2.508905333085067\n",
      "Epoch: 0187 Training RMSE= 2.710995540\n",
      "Validation RMSE:  3.2192386885830357\n",
      "Test RMSE:  2.4855578983295654\n",
      "Epoch: 0188 Training RMSE= 2.708967336\n",
      "Validation RMSE:  3.1992921552981675\n",
      "Test RMSE:  2.4463983629368404\n",
      "Epoch: 0189 Training RMSE= 2.708852119\n",
      "Validation RMSE:  3.1536684715325776\n",
      "Test RMSE:  2.426295178408923\n",
      "Epoch: 0190 Training RMSE= 2.700007834\n",
      "Validation RMSE:  3.22336798929035\n",
      "Test RMSE:  2.498916915883196\n",
      "Epoch: 0191 Training RMSE= 2.704491150\n",
      "Validation RMSE:  3.1821440409098045\n",
      "Test RMSE:  2.435686855377451\n",
      "Epoch: 0192 Training RMSE= 2.702721423\n",
      "Validation RMSE:  3.184426340394942\n",
      "Test RMSE:  2.4361432529979012\n",
      "Epoch: 0193 Training RMSE= 2.694461403\n",
      "Validation RMSE:  3.2040662545558223\n",
      "Test RMSE:  2.449428324273477\n",
      "Epoch: 0194 Training RMSE= 2.700817031\n",
      "Validation RMSE:  3.1827728149157264\n",
      "Test RMSE:  2.4490742374599854\n",
      "Epoch: 0195 Training RMSE= 2.696116134\n",
      "Validation RMSE:  3.174421819471926\n",
      "Test RMSE:  2.4455838281415603\n",
      "Epoch: 0196 Training RMSE= 2.702229200\n",
      "Validation RMSE:  3.169864976034994\n",
      "Test RMSE:  2.420727759558017\n",
      "Epoch: 0197 Training RMSE= 2.694039643\n",
      "Validation RMSE:  3.1452158307125977\n",
      "Test RMSE:  2.425315824569549\n",
      "Epoch: 0198 Training RMSE= 2.694945477\n",
      "Validation RMSE:  3.1604474550045483\n",
      "Test RMSE:  2.4229028797058074\n",
      "Epoch: 0199 Training RMSE= 2.688377494\n",
      "Validation RMSE:  3.210175918143786\n",
      "Test RMSE:  2.5002540459300238\n",
      "Epoch: 0200 Training RMSE= 2.693405560\n",
      "Validation RMSE:  3.172243331795147\n",
      "Test RMSE:  2.431163768661019\n",
      "Epoch: 0201 Training RMSE= 2.697756849\n",
      "Validation RMSE:  3.142151115430661\n",
      "Test RMSE:  2.419166822767455\n",
      "Epoch: 0202 Training RMSE= 2.685641076\n",
      "Validation RMSE:  3.201280425618646\n",
      "Test RMSE:  2.47964141745194\n",
      "Epoch: 0203 Training RMSE= 2.692402551\n",
      "Validation RMSE:  3.153545127077622\n",
      "Test RMSE:  2.4208547596686336\n",
      "Epoch: 0204 Training RMSE= 2.685340351\n",
      "Validation RMSE:  3.1459937823137754\n",
      "Test RMSE:  2.4156521991812188\n",
      "Epoch: 0205 Training RMSE= 2.688689736\n",
      "Validation RMSE:  3.1790883392510825\n",
      "Test RMSE:  2.446715321208642\n",
      "Epoch: 0206 Training RMSE= 2.683347906\n",
      "Validation RMSE:  3.1823165484681972\n",
      "Test RMSE:  2.4448748309620383\n",
      "Epoch: 0207 Training RMSE= 2.690925495\n",
      "Validation RMSE:  3.1352476706009864\n",
      "Test RMSE:  2.4197938406435435\n",
      "Epoch: 0208 Training RMSE= 2.680937597\n",
      "Validation RMSE:  3.2193874727843266\n",
      "Test RMSE:  2.512977249215574\n",
      "Epoch: 0209 Training RMSE= 2.687695634\n",
      "Validation RMSE:  3.153634640174782\n",
      "Test RMSE:  2.411925175232138\n",
      "Epoch: 0210 Training RMSE= 2.675529101\n",
      "Validation RMSE:  3.134435561442222\n",
      "Test RMSE:  2.415960806222394\n",
      "Epoch: 0211 Training RMSE= 2.685222148\n",
      "Validation RMSE:  3.145695971759473\n",
      "Test RMSE:  2.4215529196905727\n",
      "Epoch: 0212 Training RMSE= 2.675599528\n",
      "Validation RMSE:  3.218276905735294\n",
      "Test RMSE:  2.525042988628389\n",
      "Epoch: 0213 Training RMSE= 2.678821052\n",
      "Validation RMSE:  3.1763607308198343\n",
      "Test RMSE:  2.4345441649280275\n",
      "Epoch: 0214 Training RMSE= 2.684445375\n",
      "Validation RMSE:  3.167656484470369\n",
      "Test RMSE:  2.415112066650194\n",
      "Epoch: 0215 Training RMSE= 2.673959283\n",
      "Validation RMSE:  3.191376666115176\n",
      "Test RMSE:  2.4824606279399175\n",
      "Epoch: 0216 Training RMSE= 2.677286412\n",
      "Validation RMSE:  3.134344016746362\n",
      "Test RMSE:  2.415046960283285\n",
      "Epoch: 0217 Training RMSE= 2.680934711\n",
      "Validation RMSE:  3.142387806385005\n",
      "Test RMSE:  2.4168006158296973\n",
      "Epoch: 0218 Training RMSE= 2.667966592\n",
      "Validation RMSE:  3.1685555287413933\n",
      "Test RMSE:  2.457059487088092\n",
      "Epoch: 0219 Training RMSE= 2.677246267\n",
      "Validation RMSE:  3.15387957856715\n",
      "Test RMSE:  2.427083720806129\n",
      "Epoch: 0220 Training RMSE= 2.674126877\n",
      "Validation RMSE:  3.1546403538438765\n",
      "Test RMSE:  2.42332439736665\n",
      "Epoch: 0221 Training RMSE= 2.668972898\n",
      "Validation RMSE:  3.148790666257779\n",
      "Test RMSE:  2.4435320822879345\n",
      "Epoch: 0222 Training RMSE= 2.673553141\n",
      "Validation RMSE:  3.1441526576003995\n",
      "Test RMSE:  2.426071075964409\n",
      "Epoch: 0223 Training RMSE= 2.667560748\n",
      "Validation RMSE:  3.163410562296749\n",
      "Test RMSE:  2.4413911620627533\n",
      "Epoch: 0224 Training RMSE= 2.673102399\n",
      "Validation RMSE:  3.1677626085299138\n",
      "Test RMSE:  2.424431368434845\n",
      "Epoch: 0225 Training RMSE= 2.665238545\n",
      "Validation RMSE:  3.1883915421044104\n",
      "Test RMSE:  2.4797823218981163\n",
      "Epoch: 0226 Training RMSE= 2.669098689\n",
      "Validation RMSE:  3.1756240787065564\n",
      "Test RMSE:  2.4651977961883027\n",
      "Epoch: 0227 Training RMSE= 2.672819040\n",
      "Validation RMSE:  3.12680348807281\n",
      "Test RMSE:  2.415055055484348\n",
      "Epoch: 0228 Training RMSE= 2.660713225\n",
      "Validation RMSE:  3.122498159293494\n",
      "Test RMSE:  2.408373304605147\n",
      "Epoch: 0229 Training RMSE= 2.665397126\n",
      "Validation RMSE:  3.1334164767360417\n",
      "Test RMSE:  2.42616787346464\n",
      "Epoch: 0230 Training RMSE= 2.664541807\n",
      "Validation RMSE:  3.1597381805650384\n",
      "Test RMSE:  2.4283141643035506\n",
      "Epoch: 0231 Training RMSE= 2.665885874\n",
      "Validation RMSE:  3.103214352522759\n",
      "Test RMSE:  2.401409184483057\n",
      "Epoch: 0232 Training RMSE= 2.663044858\n",
      "Validation RMSE:  3.1509377370461795\n",
      "Test RMSE:  2.4465394278381356\n",
      "Epoch: 0233 Training RMSE= 2.660770008\n",
      "Validation RMSE:  3.1290642059117317\n",
      "Test RMSE:  2.4220620852166785\n",
      "Epoch: 0234 Training RMSE= 2.662316201\n",
      "Validation RMSE:  3.136571403263504\n",
      "Test RMSE:  2.4245713022155027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0235 Training RMSE= 2.659233863\n",
      "Validation RMSE:  3.1273177135107\n",
      "Test RMSE:  2.4037667378732053\n",
      "Epoch: 0236 Training RMSE= 2.662679234\n",
      "Validation RMSE:  3.124996757505642\n",
      "Test RMSE:  2.4121406584035396\n",
      "Epoch: 0237 Training RMSE= 2.653686835\n",
      "Validation RMSE:  3.1689016377359183\n",
      "Test RMSE:  2.481613880063192\n",
      "Epoch: 0238 Training RMSE= 2.658660580\n",
      "Validation RMSE:  3.1183040215932434\n",
      "Test RMSE:  2.410980578252157\n",
      "Epoch: 0239 Training RMSE= 2.657996131\n",
      "Validation RMSE:  3.1321454943409535\n",
      "Test RMSE:  2.4079987747848284\n",
      "Epoch: 0240 Training RMSE= 2.659795540\n",
      "Validation RMSE:  3.105268517522755\n",
      "Test RMSE:  2.402752154611176\n",
      "Epoch: 0241 Training RMSE= 2.650005320\n",
      "Validation RMSE:  3.1430016629939543\n",
      "Test RMSE:  2.4427248880331023\n",
      "Epoch: 0242 Training RMSE= 2.658497943\n",
      "Validation RMSE:  3.1149513809671547\n",
      "Test RMSE:  2.406019125427183\n",
      "Epoch: 0243 Training RMSE= 2.655083389\n",
      "Validation RMSE:  3.120813731975311\n",
      "Test RMSE:  2.4154870727957505\n",
      "Epoch: 0244 Training RMSE= 2.648733351\n",
      "Validation RMSE:  3.1263914443716003\n",
      "Test RMSE:  2.4327394017561126\n",
      "Epoch: 0245 Training RMSE= 2.659874496\n",
      "Validation RMSE:  3.1117112474018267\n",
      "Test RMSE:  2.402531462988321\n",
      "Epoch: 0246 Training RMSE= 2.650455244\n",
      "Validation RMSE:  3.114020629540076\n",
      "Test RMSE:  2.410079682801715\n",
      "Epoch: 0247 Training RMSE= 2.648505993\n",
      "Validation RMSE:  3.1813473130127092\n",
      "Test RMSE:  2.5022993957418795\n",
      "Epoch: 0248 Training RMSE= 2.649660167\n",
      "Validation RMSE:  3.1118078251146786\n",
      "Test RMSE:  2.41729559333042\n",
      "Epoch: 0249 Training RMSE= 2.655366854\n",
      "Validation RMSE:  3.1228826216016174\n",
      "Test RMSE:  2.411280291076842\n",
      "Epoch: 0250 Training RMSE= 2.645770465\n",
      "Validation RMSE:  3.1078612510267485\n",
      "Test RMSE:  2.4164621716061663\n",
      "Epoch: 0251 Training RMSE= 2.647792898\n",
      "Validation RMSE:  3.125215713685188\n",
      "Test RMSE:  2.4391074626720077\n",
      "Epoch: 0252 Training RMSE= 2.648271926\n",
      "Validation RMSE:  3.1448441817675237\n",
      "Test RMSE:  2.4585324115211002\n",
      "Epoch: 0253 Training RMSE= 2.649829087\n",
      "Validation RMSE:  3.1341856803273354\n",
      "Test RMSE:  2.4383485246377616\n",
      "Epoch: 0254 Training RMSE= 2.644297787\n",
      "Validation RMSE:  3.1067287360145346\n",
      "Test RMSE:  2.410690619337383\n",
      "Epoch: 0255 Training RMSE= 2.648580177\n",
      "Validation RMSE:  3.1184243638048375\n",
      "Test RMSE:  2.4252958195951635\n",
      "Epoch: 0256 Training RMSE= 2.643804354\n",
      "Validation RMSE:  3.1487465226418077\n",
      "Test RMSE:  2.448310549815902\n",
      "Epoch: 0257 Training RMSE= 2.641379033\n",
      "Validation RMSE:  3.119880832083589\n",
      "Test RMSE:  2.433178028829129\n",
      "Epoch: 0258 Training RMSE= 2.647073229\n",
      "Validation RMSE:  3.108796185624732\n",
      "Test RMSE:  2.3977336037029158\n",
      "Epoch: 0259 Training RMSE= 2.638286734\n",
      "Validation RMSE:  3.1078174849940576\n",
      "Test RMSE:  2.415024599617404\n",
      "Epoch: 0260 Training RMSE= 2.645989318\n",
      "Validation RMSE:  3.1249816893994806\n",
      "Test RMSE:  2.421569608076599\n",
      "Epoch: 0261 Training RMSE= 2.637975024\n",
      "Validation RMSE:  3.1244729551285464\n",
      "Test RMSE:  2.4442987260108264\n",
      "Epoch: 0262 Training RMSE= 2.645485570\n",
      "Validation RMSE:  3.118140244743928\n",
      "Test RMSE:  2.4086691845499635\n",
      "Epoch: 0263 Training RMSE= 2.635889224\n",
      "Validation RMSE:  3.130036682060209\n",
      "Test RMSE:  2.420424586952014\n",
      "Epoch: 0264 Training RMSE= 2.638482424\n",
      "Validation RMSE:  3.090201336680792\n",
      "Test RMSE:  2.3986819263875465\n",
      "Epoch: 0265 Training RMSE= 2.640863252\n",
      "Validation RMSE:  3.0822959950579882\n",
      "Test RMSE:  2.3906711748438445\n",
      "Epoch: 0266 Training RMSE= 2.635550237\n",
      "Validation RMSE:  3.157854710919212\n",
      "Test RMSE:  2.46662280026514\n",
      "Epoch: 0267 Training RMSE= 2.636397704\n",
      "Validation RMSE:  3.1281947780757\n",
      "Test RMSE:  2.416461480955879\n",
      "Epoch: 0268 Training RMSE= 2.641125436\n",
      "Validation RMSE:  3.1105325355759756\n",
      "Test RMSE:  2.4221125332419517\n",
      "Epoch: 0269 Training RMSE= 2.631042030\n",
      "Validation RMSE:  3.104378140476544\n",
      "Test RMSE:  2.434815371130787\n",
      "Epoch: 0270 Training RMSE= 2.643102917\n",
      "Validation RMSE:  3.101573492458283\n",
      "Test RMSE:  2.4018994920778733\n",
      "Epoch: 0271 Training RMSE= 2.628399460\n",
      "Validation RMSE:  3.186184069197162\n",
      "Test RMSE:  2.5101560296427503\n",
      "Epoch: 0272 Training RMSE= 2.632761715\n",
      "Validation RMSE:  3.1070838806743906\n",
      "Test RMSE:  2.394198221816847\n",
      "Epoch: 0273 Training RMSE= 2.635020110\n",
      "Validation RMSE:  3.1363454480202955\n",
      "Test RMSE:  2.440499001350972\n",
      "Epoch: 0274 Training RMSE= 2.633456202\n",
      "Validation RMSE:  3.1093501564451924\n",
      "Test RMSE:  2.4067568554802885\n",
      "Epoch: 0275 Training RMSE= 2.631117083\n",
      "Validation RMSE:  3.1034508632963362\n",
      "Test RMSE:  2.414203133545214\n",
      "Epoch: 0276 Training RMSE= 2.630046000\n",
      "Validation RMSE:  3.144380743559458\n",
      "Test RMSE:  2.4631865381869718\n",
      "Epoch: 0277 Training RMSE= 2.630998148\n",
      "Validation RMSE:  3.128229418068387\n",
      "Test RMSE:  2.4577456165096527\n",
      "Epoch: 0278 Training RMSE= 2.632424323\n",
      "Validation RMSE:  3.094453750868781\n",
      "Test RMSE:  2.4046207708726204\n",
      "Epoch: 0279 Training RMSE= 2.629893546\n",
      "Validation RMSE:  3.0980955873054765\n",
      "Test RMSE:  2.3993672112877307\n",
      "Epoch: 0280 Training RMSE= 2.622327861\n",
      "Validation RMSE:  3.1233934087346293\n",
      "Test RMSE:  2.433178126815619\n",
      "Epoch: 0281 Training RMSE= 2.630856484\n",
      "Validation RMSE:  3.104243121816409\n",
      "Test RMSE:  2.403208804808537\n",
      "Epoch: 0282 Training RMSE= 2.625210554\n",
      "Validation RMSE:  3.152708654070717\n",
      "Test RMSE:  2.488953456178338\n",
      "Epoch: 0283 Training RMSE= 2.628688986\n",
      "Validation RMSE:  3.081146229201704\n",
      "Test RMSE:  2.3971163313625823\n",
      "Epoch: 0284 Training RMSE= 2.624383681\n",
      "Validation RMSE:  3.0794308211429593\n",
      "Test RMSE:  2.387411763023663\n",
      "Epoch: 0285 Training RMSE= 2.628782848\n",
      "Validation RMSE:  3.1240491564909205\n",
      "Test RMSE:  2.4257483242091635\n",
      "Epoch: 0286 Training RMSE= 2.625808868\n",
      "Validation RMSE:  3.120564097125909\n",
      "Test RMSE:  2.441268062495433\n",
      "Epoch: 0287 Training RMSE= 2.619750792\n",
      "Validation RMSE:  3.0808303098394614\n",
      "Test RMSE:  2.401972796324987\n",
      "Epoch: 0288 Training RMSE= 2.623485528\n",
      "Validation RMSE:  3.1167120702431657\n",
      "Test RMSE:  2.4549868861856234\n",
      "Epoch: 0289 Training RMSE= 2.622631489\n",
      "Validation RMSE:  3.080005129648866\n",
      "Test RMSE:  2.3963708081186126\n",
      "Epoch: 0290 Training RMSE= 2.625721451\n",
      "Validation RMSE:  3.106467377944955\n",
      "Test RMSE:  2.434178950944685\n",
      "Epoch: 0291 Training RMSE= 2.618680058\n",
      "Validation RMSE:  3.1427861459565314\n",
      "Test RMSE:  2.4545337495299973\n",
      "Epoch: 0292 Training RMSE= 2.624280808\n",
      "Validation RMSE:  3.0874604149261513\n",
      "Test RMSE:  2.3904193964288702\n",
      "Epoch: 0293 Training RMSE= 2.620109803\n",
      "Validation RMSE:  3.0918838189937925\n",
      "Test RMSE:  2.4161728211858433\n",
      "Epoch: 0294 Training RMSE= 2.621019288\n",
      "Validation RMSE:  3.0981856250968507\n",
      "Test RMSE:  2.4335522104701153\n",
      "Epoch: 0295 Training RMSE= 2.618690619\n",
      "Validation RMSE:  3.098958418816744\n",
      "Test RMSE:  2.429650652934151\n",
      "Epoch: 0296 Training RMSE= 2.618463762\n",
      "Validation RMSE:  3.1296814757054396\n",
      "Test RMSE:  2.473834921452402\n",
      "Epoch: 0297 Training RMSE= 2.618132177\n",
      "Validation RMSE:  3.0819904049099556\n",
      "Test RMSE:  2.406976020573095\n",
      "Epoch: 0298 Training RMSE= 2.621542766\n",
      "Validation RMSE:  3.0725559647928398\n",
      "Test RMSE:  2.3921977336323295\n",
      "Epoch: 0299 Training RMSE= 2.614569322\n",
      "Validation RMSE:  3.1175236723856856\n",
      "Test RMSE:  2.453189824699573\n",
      "Epoch: 0300 Training RMSE= 2.613790219\n",
      "Validation RMSE:  3.100938864018268\n",
      "Test RMSE:  2.4220714366372835\n",
      "Epoch: 0301 Training RMSE= 2.622161975\n",
      "Validation RMSE:  3.1232300514384743\n",
      "Test RMSE:  2.4376441717304007\n",
      "Epoch: 0302 Training RMSE= 2.616589023\n",
      "Validation RMSE:  3.0857612656115165\n",
      "Test RMSE:  2.395909023471432\n",
      "Epoch: 0303 Training RMSE= 2.615958232\n",
      "Validation RMSE:  3.080397062655055\n",
      "Test RMSE:  2.4058528423762584\n",
      "Epoch: 0304 Training RMSE= 2.614694479\n",
      "Validation RMSE:  3.104332174885513\n",
      "Test RMSE:  2.4398621581962217\n",
      "Epoch: 0305 Training RMSE= 2.613501784\n",
      "Validation RMSE:  3.1193729091997273\n",
      "Test RMSE:  2.443634042159491\n",
      "Epoch: 0306 Training RMSE= 2.610989452\n",
      "Validation RMSE:  3.0966703714924124\n",
      "Test RMSE:  2.396510539887473\n",
      "Epoch: 0307 Training RMSE= 2.619968697\n",
      "Validation RMSE:  3.071629676064861\n",
      "Test RMSE:  2.3992463776399218\n",
      "Epoch: 0308 Training RMSE= 2.609512586\n",
      "Validation RMSE:  3.0700204336623313\n",
      "Test RMSE:  2.387110801250648\n",
      "Epoch: 0309 Training RMSE= 2.612077618\n",
      "Validation RMSE:  3.114418042410075\n",
      "Test RMSE:  2.4246740594061915\n",
      "Epoch: 0310 Training RMSE= 2.611643877\n",
      "Validation RMSE:  3.0788319351996267\n",
      "Test RMSE:  2.4193813653230016\n",
      "Epoch: 0311 Training RMSE= 2.611203975\n",
      "Validation RMSE:  3.1108757882772693\n",
      "Test RMSE:  2.44538825652046\n",
      "Epoch: 0312 Training RMSE= 2.611435110\n",
      "Validation RMSE:  3.115511221228619\n",
      "Test RMSE:  2.4375651913263656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0313 Training RMSE= 2.610423091\n",
      "Validation RMSE:  3.0843088222156223\n",
      "Test RMSE:  2.408587868207245\n",
      "Epoch: 0314 Training RMSE= 2.608403442\n",
      "Validation RMSE:  3.091002274832356\n",
      "Test RMSE:  2.438758231911191\n",
      "Epoch: 0315 Training RMSE= 2.614155649\n",
      "Validation RMSE:  3.076813859101582\n",
      "Test RMSE:  2.3959151433715387\n",
      "Epoch: 0316 Training RMSE= 2.607696427\n",
      "Validation RMSE:  3.0625416499828435\n",
      "Test RMSE:  2.4029471779302796\n",
      "Epoch: 0317 Training RMSE= 2.606729804\n",
      "Validation RMSE:  3.1007913548465162\n",
      "Test RMSE:  2.437706131655519\n",
      "Epoch: 0318 Training RMSE= 2.612624095\n",
      "Validation RMSE:  3.0732101472700246\n",
      "Test RMSE:  2.4040333830472114\n",
      "Epoch: 0319 Training RMSE= 2.601661612\n",
      "Validation RMSE:  3.106733801024946\n",
      "Test RMSE:  2.4613764780075087\n",
      "Epoch: 0320 Training RMSE= 2.610328313\n",
      "Validation RMSE:  3.082962610966449\n",
      "Test RMSE:  2.417239619989584\n",
      "Epoch: 0321 Training RMSE= 2.604255474\n",
      "Validation RMSE:  3.0844394572749327\n",
      "Test RMSE:  2.391929519865276\n",
      "Epoch: 0322 Training RMSE= 2.605498718\n",
      "Validation RMSE:  3.0811816302716974\n",
      "Test RMSE:  2.4177631543436404\n",
      "Epoch: 0323 Training RMSE= 2.603625344\n",
      "Validation RMSE:  3.1167553671456125\n",
      "Test RMSE:  2.436915987917008\n",
      "Epoch: 0324 Training RMSE= 2.604647821\n",
      "Validation RMSE:  3.074046109500865\n",
      "Test RMSE:  2.414667640072173\n",
      "Epoch: 0325 Training RMSE= 2.605262920\n",
      "Validation RMSE:  3.059033163765707\n",
      "Test RMSE:  2.3927410459517295\n",
      "Epoch: 0326 Training RMSE= 2.603238305\n",
      "Validation RMSE:  3.0996410269704606\n",
      "Test RMSE:  2.4464904580428177\n",
      "Epoch: 0327 Training RMSE= 2.603143092\n",
      "Validation RMSE:  3.100458981775853\n",
      "Test RMSE:  2.436527694056277\n",
      "Epoch: 0328 Training RMSE= 2.604619395\n",
      "Validation RMSE:  3.0561514075979153\n",
      "Test RMSE:  2.3877657577638525\n",
      "Epoch: 0329 Training RMSE= 2.602329400\n",
      "Validation RMSE:  3.0967637997707276\n",
      "Test RMSE:  2.4422439968922935\n",
      "Epoch: 0330 Training RMSE= 2.602568158\n",
      "Validation RMSE:  3.140572618645498\n",
      "Test RMSE:  2.48306537394319\n",
      "Epoch: 0331 Training RMSE= 2.599796532\n",
      "Validation RMSE:  3.086571930000659\n",
      "Test RMSE:  2.4409292014394475\n",
      "Epoch: 0332 Training RMSE= 2.598483131\n",
      "Validation RMSE:  3.1212416364369076\n",
      "Test RMSE:  2.4287309634276912\n",
      "Epoch: 0333 Training RMSE= 2.605558146\n",
      "Validation RMSE:  3.069295117661768\n",
      "Test RMSE:  2.4165504252222014\n",
      "Epoch: 0334 Training RMSE= 2.600569458\n",
      "Validation RMSE:  3.0922974924009816\n",
      "Test RMSE:  2.4240784988376967\n",
      "Epoch: 0335 Training RMSE= 2.597771810\n",
      "Validation RMSE:  3.085673028743279\n",
      "Test RMSE:  2.435217644679676\n",
      "Epoch: 0336 Training RMSE= 2.597530822\n",
      "Validation RMSE:  3.0776417154365077\n",
      "Test RMSE:  2.414339166978193\n",
      "Epoch: 0337 Training RMSE= 2.597970291\n",
      "Validation RMSE:  3.061695187753711\n",
      "Test RMSE:  2.3887749787705914\n",
      "Epoch: 0338 Training RMSE= 2.598018572\n",
      "Validation RMSE:  3.0450486798458947\n",
      "Test RMSE:  2.3974770477437333\n",
      "Epoch: 0339 Training RMSE= 2.602105303\n",
      "Validation RMSE:  3.0567936185627915\n",
      "Test RMSE:  2.3914436484462365\n",
      "Epoch: 0340 Training RMSE= 2.593169848\n",
      "Validation RMSE:  3.075886488497993\n",
      "Test RMSE:  2.417434460797938\n",
      "Epoch: 0341 Training RMSE= 2.598230697\n",
      "Validation RMSE:  3.1013221551511103\n",
      "Test RMSE:  2.4349663600125364\n",
      "Epoch: 0342 Training RMSE= 2.597309754\n",
      "Validation RMSE:  3.078096805966215\n",
      "Test RMSE:  2.402292638715689\n",
      "Epoch: 0343 Training RMSE= 2.594735825\n",
      "Validation RMSE:  3.052711686863367\n",
      "Test RMSE:  2.3865375855861064\n",
      "Epoch: 0344 Training RMSE= 2.593253375\n",
      "Validation RMSE:  3.0865768349787666\n",
      "Test RMSE:  2.4353865239540786\n",
      "Epoch: 0345 Training RMSE= 2.595007534\n",
      "Validation RMSE:  3.1059637478925373\n",
      "Test RMSE:  2.4470775437596433\n",
      "Epoch: 0346 Training RMSE= 2.591825290\n",
      "Validation RMSE:  3.0525171320978854\n",
      "Test RMSE:  2.3912822842051282\n",
      "Epoch: 0347 Training RMSE= 2.598616683\n",
      "Validation RMSE:  3.0549202364484103\n",
      "Test RMSE:  2.3910978261063724\n",
      "Epoch: 0348 Training RMSE= 2.593507976\n",
      "Validation RMSE:  3.0643147910838047\n",
      "Test RMSE:  2.409828151337535\n",
      "Epoch: 0349 Training RMSE= 2.595368604\n",
      "Validation RMSE:  3.1091518226159445\n",
      "Test RMSE:  2.453596324956957\n",
      "Epoch: 0350 Training RMSE= 2.588208817\n",
      "Validation RMSE:  3.066296190364138\n",
      "Test RMSE:  2.393690649347607\n",
      "Epoch: 0351 Training RMSE= 2.600878454\n",
      "Validation RMSE:  3.054090358584041\n",
      "Test RMSE:  2.400264081731079\n",
      "Epoch: 0352 Training RMSE= 2.586400468\n",
      "Validation RMSE:  3.0530334833772392\n",
      "Test RMSE:  2.3836178778693458\n",
      "Epoch: 0353 Training RMSE= 2.591581495\n",
      "Validation RMSE:  3.084241067593121\n",
      "Test RMSE:  2.4254256278049806\n",
      "Epoch: 0354 Training RMSE= 2.588165232\n",
      "Validation RMSE:  3.1129926789489004\n",
      "Test RMSE:  2.444990191683764\n",
      "Epoch: 0355 Training RMSE= 2.591482371\n",
      "Validation RMSE:  3.096765609025593\n",
      "Test RMSE:  2.44208877178429\n",
      "Epoch: 0356 Training RMSE= 2.594346224\n",
      "Validation RMSE:  3.102988734331497\n",
      "Test RMSE:  2.4271540542799084\n",
      "Epoch: 0357 Training RMSE= 2.584454225\n",
      "Validation RMSE:  3.0595491166876725\n",
      "Test RMSE:  2.4127207857607798\n",
      "Epoch: 0358 Training RMSE= 2.595105311\n",
      "Validation RMSE:  3.0486393050789946\n",
      "Test RMSE:  2.3873123954043685\n",
      "Epoch: 0359 Training RMSE= 2.583028631\n",
      "Validation RMSE:  3.0953871508574236\n",
      "Test RMSE:  2.4331843489496454\n",
      "Epoch: 0360 Training RMSE= 2.589597172\n",
      "Validation RMSE:  3.0618774404085816\n",
      "Test RMSE:  2.4182948075986808\n",
      "Epoch: 0361 Training RMSE= 2.591425932\n",
      "Validation RMSE:  3.0576563698308883\n",
      "Test RMSE:  2.4049425412489556\n",
      "Epoch: 0362 Training RMSE= 2.586206234\n",
      "Validation RMSE:  3.0492732073226083\n",
      "Test RMSE:  2.393475596982265\n",
      "Epoch: 0363 Training RMSE= 2.586681708\n",
      "Validation RMSE:  3.0844045573850316\n",
      "Test RMSE:  2.4337804731862316\n",
      "Epoch: 0364 Training RMSE= 2.584434478\n",
      "Validation RMSE:  3.0614576706826626\n",
      "Test RMSE:  2.4058514054344937\n",
      "Epoch: 0365 Training RMSE= 2.589291355\n",
      "Validation RMSE:  3.0693854565559793\n",
      "Test RMSE:  2.416498479800434\n",
      "Epoch: 0366 Training RMSE= 2.583684759\n",
      "Validation RMSE:  3.0922493811338505\n",
      "Test RMSE:  2.4352574914539566\n",
      "Epoch: 0367 Training RMSE= 2.589602645\n",
      "Validation RMSE:  3.066679690858578\n",
      "Test RMSE:  2.3942659364862813\n",
      "Epoch: 0368 Training RMSE= 2.582772873\n",
      "Validation RMSE:  3.067376322285497\n",
      "Test RMSE:  2.3985420726134197\n",
      "Epoch: 0369 Training RMSE= 2.582963853\n",
      "Validation RMSE:  3.059696860912249\n",
      "Test RMSE:  2.4150574248063554\n",
      "Epoch: 0370 Training RMSE= 2.585981570\n",
      "Validation RMSE:  3.105927631375162\n",
      "Test RMSE:  2.456814948963582\n",
      "Epoch: 0371 Training RMSE= 2.580859534\n",
      "Validation RMSE:  3.0501113527326513\n",
      "Test RMSE:  2.384672745779108\n",
      "Epoch: 0372 Training RMSE= 2.590220428\n",
      "Validation RMSE:  3.0541041370841335\n",
      "Test RMSE:  2.4017900030307495\n",
      "Epoch: 0373 Training RMSE= 2.577427024\n",
      "Validation RMSE:  3.1129180811253874\n",
      "Test RMSE:  2.4573116172852782\n",
      "Epoch: 0374 Training RMSE= 2.583969838\n",
      "Validation RMSE:  3.0654475371687826\n",
      "Test RMSE:  2.429011750775085\n",
      "Epoch: 0375 Training RMSE= 2.585316162\n",
      "Validation RMSE:  3.0750788996428904\n",
      "Test RMSE:  2.427158179923188\n",
      "Epoch: 0376 Training RMSE= 2.585329677\n",
      "Validation RMSE:  3.0430617899743555\n",
      "Test RMSE:  2.3853640043839524\n",
      "Epoch: 0377 Training RMSE= 2.582038789\n",
      "Validation RMSE:  3.0840238794522947\n",
      "Test RMSE:  2.411761573087554\n",
      "Epoch: 0378 Training RMSE= 2.578181207\n",
      "Validation RMSE:  3.1025599646171886\n",
      "Test RMSE:  2.4471919238172677\n",
      "Epoch: 0379 Training RMSE= 2.578321410\n",
      "Validation RMSE:  3.0601390379440394\n",
      "Test RMSE:  2.4194138357701065\n",
      "Epoch: 0380 Training RMSE= 2.581910073\n",
      "Validation RMSE:  3.0558005254014375\n",
      "Test RMSE:  2.3952464891508916\n",
      "Epoch: 0381 Training RMSE= 2.580476805\n",
      "Validation RMSE:  3.065089785083791\n",
      "Test RMSE:  2.4004892029474756\n",
      "Epoch: 0382 Training RMSE= 2.579100707\n",
      "Validation RMSE:  3.096553957530169\n",
      "Test RMSE:  2.443275602689978\n",
      "Epoch: 0383 Training RMSE= 2.582170261\n",
      "Validation RMSE:  3.0545516898558085\n",
      "Test RMSE:  2.4115054714355155\n",
      "Epoch: 0384 Training RMSE= 2.575426639\n",
      "Validation RMSE:  3.0705324827015654\n",
      "Test RMSE:  2.4123584942997467\n",
      "Epoch: 0385 Training RMSE= 2.581577094\n",
      "Validation RMSE:  3.057817382496721\n",
      "Test RMSE:  2.4008286078884447\n",
      "Epoch: 0386 Training RMSE= 2.578332779\n",
      "Validation RMSE:  3.0599653693340123\n",
      "Test RMSE:  2.410918969845222\n",
      "Epoch: 0387 Training RMSE= 2.575144804\n",
      "Validation RMSE:  3.0978259200089537\n",
      "Test RMSE:  2.445767052665967\n",
      "Epoch: 0388 Training RMSE= 2.578911998\n",
      "Validation RMSE:  3.0723043489046757\n",
      "Test RMSE:  2.4067586881299077\n",
      "Epoch: 0389 Training RMSE= 2.576844960\n",
      "Validation RMSE:  3.0791414780332835\n",
      "Test RMSE:  2.435555636245644\n",
      "Epoch: 0390 Training RMSE= 2.578607956\n",
      "Validation RMSE:  3.071857054183557\n",
      "Test RMSE:  2.418394775328701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0391 Training RMSE= 2.575677508\n",
      "Validation RMSE:  3.068260769197532\n",
      "Test RMSE:  2.416226007033578\n",
      "Epoch: 0392 Training RMSE= 2.573025732\n",
      "Validation RMSE:  3.096704247749301\n",
      "Test RMSE:  2.436771479577611\n",
      "Epoch: 0393 Training RMSE= 2.578396638\n",
      "Validation RMSE:  3.0476264222657576\n",
      "Test RMSE:  2.400680685869827\n",
      "Epoch: 0394 Training RMSE= 2.572707686\n",
      "Validation RMSE:  3.0628554468770406\n",
      "Test RMSE:  2.407295247736509\n",
      "Epoch: 0395 Training RMSE= 2.578369286\n",
      "Validation RMSE:  3.061942653153877\n",
      "Test RMSE:  2.4003533282071174\n",
      "Epoch: 0396 Training RMSE= 2.572195042\n",
      "Validation RMSE:  3.065344559871972\n",
      "Test RMSE:  2.40839681596696\n",
      "Epoch: 0397 Training RMSE= 2.578236368\n",
      "Validation RMSE:  3.052538845368078\n",
      "Test RMSE:  2.402449095362375\n",
      "Epoch: 0398 Training RMSE= 2.572339075\n",
      "Validation RMSE:  3.0561996190573826\n",
      "Test RMSE:  2.3911105890708755\n",
      "Epoch: 0399 Training RMSE= 2.569986674\n",
      "Validation RMSE:  3.0505620704292156\n",
      "Test RMSE:  2.4064935338028364\n",
      "Epoch: 0400 Training RMSE= 2.574654438\n",
      "Validation RMSE:  3.081386522901448\n",
      "Test RMSE:  2.423793352908946\n",
      "Epoch: 0401 Training RMSE= 2.573960153\n",
      "Validation RMSE:  3.042211514276788\n",
      "Test RMSE:  2.3891027752791545\n",
      "Epoch: 0402 Training RMSE= 2.568834180\n",
      "Validation RMSE:  3.0834934648395365\n",
      "Test RMSE:  2.426999632302136\n",
      "Epoch: 0403 Training RMSE= 2.575047556\n",
      "Validation RMSE:  3.046775151108321\n",
      "Test RMSE:  2.394043267514768\n",
      "Epoch: 0404 Training RMSE= 2.569376177\n",
      "Validation RMSE:  3.0499168666078416\n",
      "Test RMSE:  2.398379595427212\n",
      "Epoch: 0405 Training RMSE= 2.572385982\n",
      "Validation RMSE:  3.076511715479032\n",
      "Test RMSE:  2.437211679866194\n",
      "Epoch: 0406 Training RMSE= 2.570847541\n",
      "Validation RMSE:  3.0589104070721427\n",
      "Test RMSE:  2.3891589088522442\n",
      "Epoch: 0407 Training RMSE= 2.573020571\n",
      "Validation RMSE:  3.0780545918888755\n",
      "Test RMSE:  2.396059827188188\n",
      "Epoch: 0408 Training RMSE= 2.573677425\n",
      "Validation RMSE:  3.0836931010155033\n",
      "Test RMSE:  2.4244935185178034\n",
      "Epoch: 0409 Training RMSE= 2.568466614\n",
      "Validation RMSE:  3.061743623399359\n",
      "Test RMSE:  2.422299009553519\n",
      "Epoch: 0410 Training RMSE= 2.570744835\n",
      "Validation RMSE:  3.0743066564300943\n",
      "Test RMSE:  2.4097799690600046\n",
      "Epoch: 0411 Training RMSE= 2.569691615\n",
      "Validation RMSE:  3.0956650788770115\n",
      "Test RMSE:  2.4540923817072353\n",
      "Epoch: 0412 Training RMSE= 2.570464907\n",
      "Validation RMSE:  3.055517878926273\n",
      "Test RMSE:  2.4021158746090556\n",
      "Epoch: 0413 Training RMSE= 2.569915129\n",
      "Validation RMSE:  3.0595323625409327\n",
      "Test RMSE:  2.41056066053841\n",
      "Epoch: 0414 Training RMSE= 2.566549646\n",
      "Validation RMSE:  3.0647629531996126\n",
      "Test RMSE:  2.4135334212867936\n",
      "Epoch: 0415 Training RMSE= 2.567725589\n",
      "Validation RMSE:  3.08176648219858\n",
      "Test RMSE:  2.4303919516103054\n",
      "Epoch: 0416 Training RMSE= 2.568984619\n",
      "Validation RMSE:  3.0537591094147785\n",
      "Test RMSE:  2.396634694851342\n",
      "Epoch: 0417 Training RMSE= 2.568226245\n",
      "Validation RMSE:  3.052840042482031\n",
      "Test RMSE:  2.3910927906992927\n",
      "Epoch: 0418 Training RMSE= 2.569133142\n",
      "Validation RMSE:  3.043394830179195\n",
      "Test RMSE:  2.3864370326525335\n",
      "Epoch: 0419 Training RMSE= 2.566232587\n",
      "Validation RMSE:  3.0438308376097396\n",
      "Test RMSE:  2.407372844344688\n",
      "Epoch: 0420 Training RMSE= 2.568072795\n",
      "Validation RMSE:  3.0744844779855605\n",
      "Test RMSE:  2.404814105948843\n",
      "Epoch: 0421 Training RMSE= 2.568380159\n",
      "Validation RMSE:  3.0663620477880014\n",
      "Test RMSE:  2.4222931039522173\n",
      "Epoch: 0422 Training RMSE= 2.565648503\n",
      "Validation RMSE:  3.057892583812941\n",
      "Test RMSE:  2.422474842236156\n",
      "Epoch: 0423 Training RMSE= 2.563336989\n",
      "Validation RMSE:  3.054292033711612\n",
      "Test RMSE:  2.412088568593842\n",
      "Epoch: 0424 Training RMSE= 2.567753533\n",
      "Validation RMSE:  3.0492162072583797\n",
      "Test RMSE:  2.3891748255987424\n",
      "Epoch: 0425 Training RMSE= 2.566515089\n",
      "Validation RMSE:  3.085760995187141\n",
      "Test RMSE:  2.4372703248414074\n",
      "Epoch: 0426 Training RMSE= 2.564186816\n",
      "Validation RMSE:  3.055137542587736\n",
      "Test RMSE:  2.3989473975116318\n",
      "Epoch: 0427 Training RMSE= 2.564310472\n",
      "Validation RMSE:  3.0843000099473272\n",
      "Test RMSE:  2.431813821407872\n",
      "Epoch: 0428 Training RMSE= 2.564128879\n",
      "Validation RMSE:  3.07141815544973\n",
      "Test RMSE:  2.4235356697745707\n",
      "Epoch: 0429 Training RMSE= 2.560675548\n",
      "Validation RMSE:  3.0699359769699925\n",
      "Test RMSE:  2.430590544554745\n",
      "Epoch: 0430 Training RMSE= 2.565440756\n",
      "Validation RMSE:  3.077881314463781\n",
      "Test RMSE:  2.418747389838809\n",
      "Epoch: 0431 Training RMSE= 2.566939342\n",
      "Validation RMSE:  3.042496258898702\n",
      "Test RMSE:  2.390186643614824\n",
      "Epoch: 0432 Training RMSE= 2.563688259\n",
      "Validation RMSE:  3.0869770472037477\n",
      "Test RMSE:  2.4426640802752173\n",
      "Epoch: 0433 Training RMSE= 2.562982993\n",
      "Validation RMSE:  3.0716825345382066\n",
      "Test RMSE:  2.4267048574768038\n",
      "Epoch: 0434 Training RMSE= 2.563904350\n",
      "Validation RMSE:  3.0638520895587837\n",
      "Test RMSE:  2.4123624969985435\n",
      "Epoch: 0435 Training RMSE= 2.564653143\n",
      "Validation RMSE:  3.0555314559178797\n",
      "Test RMSE:  2.4110024325415083\n",
      "Epoch: 0436 Training RMSE= 2.558717532\n",
      "Validation RMSE:  3.0580035306412428\n",
      "Test RMSE:  2.3938436345444893\n",
      "Epoch: 0437 Training RMSE= 2.560450730\n",
      "Validation RMSE:  3.068885141415356\n",
      "Test RMSE:  2.4176476778450273\n",
      "Epoch: 0438 Training RMSE= 2.561394728\n",
      "Validation RMSE:  3.0731165072013615\n",
      "Test RMSE:  2.4209140963276856\n",
      "Epoch: 0439 Training RMSE= 2.559955171\n",
      "Validation RMSE:  3.10201834684661\n",
      "Test RMSE:  2.4481304859901414\n",
      "Epoch: 0440 Training RMSE= 2.561543011\n",
      "Validation RMSE:  3.0474803225450757\n",
      "Test RMSE:  2.3931598065231885\n",
      "Epoch: 0441 Training RMSE= 2.560700878\n",
      "Validation RMSE:  3.072165010408951\n",
      "Test RMSE:  2.4218727850134782\n",
      "Epoch: 0442 Training RMSE= 2.558924581\n",
      "Validation RMSE:  3.0611644090219166\n",
      "Test RMSE:  2.4169464168646\n",
      "Epoch: 0443 Training RMSE= 2.557380372\n",
      "Validation RMSE:  3.0833366540083316\n",
      "Test RMSE:  2.423370883795893\n",
      "Epoch: 0444 Training RMSE= 2.563117125\n",
      "Validation RMSE:  3.0711036419957827\n",
      "Test RMSE:  2.403045750217788\n",
      "Epoch: 0445 Training RMSE= 2.561713144\n",
      "Validation RMSE:  3.0606867813627057\n",
      "Test RMSE:  2.4015434606885733\n",
      "Epoch: 0446 Training RMSE= 2.558404651\n",
      "Validation RMSE:  3.0584712874839166\n",
      "Test RMSE:  2.4060848724381803\n",
      "Epoch: 0447 Training RMSE= 2.560919716\n",
      "Validation RMSE:  3.0568846778817824\n",
      "Test RMSE:  2.4154157580003868\n",
      "Epoch: 0448 Training RMSE= 2.558382261\n",
      "Validation RMSE:  3.066346652672737\n",
      "Test RMSE:  2.402327126546271\n",
      "Epoch: 0449 Training RMSE= 2.560267029\n",
      "Validation RMSE:  3.056282583113714\n",
      "Test RMSE:  2.4137318702621395\n",
      "Epoch: 0450 Training RMSE= 2.556142140\n",
      "Validation RMSE:  3.057822060704087\n",
      "Test RMSE:  2.389656419198971\n",
      "Epoch: 0451 Training RMSE= 2.558634764\n",
      "Validation RMSE:  3.0639186609442763\n",
      "Test RMSE:  2.413366618673323\n",
      "Epoch: 0452 Training RMSE= 2.558200038\n",
      "Validation RMSE:  3.0549822808983333\n",
      "Test RMSE:  2.400825926603654\n",
      "Epoch: 0453 Training RMSE= 2.558261707\n",
      "Validation RMSE:  3.0524444930569943\n",
      "Test RMSE:  2.405995689924126\n",
      "Epoch: 0454 Training RMSE= 2.554569263\n",
      "Validation RMSE:  3.0468010135373\n",
      "Test RMSE:  2.4052114846555988\n",
      "Epoch: 0455 Training RMSE= 2.561249521\n",
      "Validation RMSE:  3.0528585905355596\n",
      "Test RMSE:  2.404098490162119\n",
      "Epoch: 0456 Training RMSE= 2.556021349\n",
      "Validation RMSE:  3.048125924788228\n",
      "Test RMSE:  2.387730460503061\n",
      "Epoch: 0457 Training RMSE= 2.558282494\n",
      "Validation RMSE:  3.0809813283659713\n",
      "Test RMSE:  2.4330043415731692\n",
      "Epoch: 0458 Training RMSE= 2.554216515\n",
      "Validation RMSE:  3.062729768502673\n",
      "Test RMSE:  2.387631705281344\n",
      "Epoch: 0459 Training RMSE= 2.558332399\n",
      "Validation RMSE:  3.0520336203491962\n",
      "Test RMSE:  2.399213634250743\n",
      "Epoch: 0460 Training RMSE= 2.556388932\n",
      "Validation RMSE:  3.0567004115683556\n",
      "Test RMSE:  2.407703555128424\n",
      "Epoch: 0461 Training RMSE= 2.556365644\n",
      "Validation RMSE:  3.066990848869649\n",
      "Test RMSE:  2.4154003102966852\n",
      "Epoch: 0462 Training RMSE= 2.556197618\n",
      "Validation RMSE:  3.0615794296863448\n",
      "Test RMSE:  2.41701551638047\n",
      "Epoch: 0463 Training RMSE= 2.555044078\n",
      "Validation RMSE:  3.0912850703969945\n",
      "Test RMSE:  2.4371036305079357\n",
      "Epoch: 0464 Training RMSE= 2.555190552\n",
      "Validation RMSE:  3.0562529394217557\n",
      "Test RMSE:  2.405596855000913\n",
      "Epoch: 0465 Training RMSE= 2.553051052\n",
      "Validation RMSE:  3.043157451689297\n",
      "Test RMSE:  2.3961003251509476\n",
      "Epoch: 0466 Training RMSE= 2.553003065\n",
      "Validation RMSE:  3.1008281078532094\n",
      "Test RMSE:  2.4319088217276272\n",
      "Epoch: 0467 Training RMSE= 2.555903010\n",
      "Validation RMSE:  3.0654530981554697\n",
      "Test RMSE:  2.3889059730283613\n",
      "Epoch: 0468 Training RMSE= 2.552376792\n",
      "Validation RMSE:  3.075511733172738\n",
      "Test RMSE:  2.4206512312646056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0469 Training RMSE= 2.556291931\n",
      "Validation RMSE:  3.0413218437768275\n",
      "Test RMSE:  2.3844710284453017\n",
      "Epoch: 0470 Training RMSE= 2.553503372\n",
      "Validation RMSE:  3.0818254718811935\n",
      "Test RMSE:  2.4293694978008373\n",
      "Epoch: 0471 Training RMSE= 2.551151318\n",
      "Validation RMSE:  3.0652351232581005\n",
      "Test RMSE:  2.417819953740318\n",
      "Epoch: 0472 Training RMSE= 2.557836000\n",
      "Validation RMSE:  3.0564424590690358\n",
      "Test RMSE:  2.3989399933427986\n",
      "Epoch: 0473 Training RMSE= 2.546742142\n",
      "Validation RMSE:  3.0682890924651796\n",
      "Test RMSE:  2.429248046384275\n",
      "Epoch: 0474 Training RMSE= 2.550932889\n",
      "Validation RMSE:  3.07275774701065\n",
      "Test RMSE:  2.40944335880108\n",
      "Epoch: 0475 Training RMSE= 2.553385874\n",
      "Validation RMSE:  3.09409704092195\n",
      "Test RMSE:  2.4464141509155155\n",
      "Epoch: 0476 Training RMSE= 2.551373063\n",
      "Validation RMSE:  3.07900903080616\n",
      "Test RMSE:  2.4263061840031863\n",
      "Epoch: 0477 Training RMSE= 2.545677475\n",
      "Validation RMSE:  3.088834923410772\n",
      "Test RMSE:  2.435148865851037\n",
      "Epoch: 0478 Training RMSE= 2.561279914\n",
      "Validation RMSE:  3.0401365446495205\n",
      "Test RMSE:  2.389715732364291\n",
      "Epoch: 0479 Training RMSE= 2.546882208\n",
      "Validation RMSE:  3.0553306039213535\n",
      "Test RMSE:  2.4049313387517133\n",
      "Epoch: 0480 Training RMSE= 2.550475355\n",
      "Validation RMSE:  3.0559502452674674\n",
      "Test RMSE:  2.402258001518441\n",
      "Epoch: 0481 Training RMSE= 2.547962883\n",
      "Validation RMSE:  3.0638027923405913\n",
      "Test RMSE:  2.40703456027835\n",
      "Epoch: 0482 Training RMSE= 2.550862113\n",
      "Validation RMSE:  3.055244804725634\n",
      "Test RMSE:  2.397576938782867\n",
      "Epoch: 0483 Training RMSE= 2.551878801\n",
      "Validation RMSE:  3.0464923324901685\n",
      "Test RMSE:  2.395994899664722\n",
      "Epoch: 0484 Training RMSE= 2.546266705\n",
      "Validation RMSE:  3.072325029908334\n",
      "Test RMSE:  2.417505962597221\n",
      "Epoch: 0485 Training RMSE= 2.550056675\n",
      "Validation RMSE:  3.042903953322669\n",
      "Test RMSE:  2.397152584469618\n",
      "Epoch: 0486 Training RMSE= 2.549363335\n",
      "Validation RMSE:  3.044171077419803\n",
      "Test RMSE:  2.3864463738139436\n",
      "Epoch: 0487 Training RMSE= 2.549959138\n",
      "Validation RMSE:  3.07509932939086\n",
      "Test RMSE:  2.4155865151718894\n",
      "Epoch: 0488 Training RMSE= 2.547642912\n",
      "Validation RMSE:  3.066220884126567\n",
      "Test RMSE:  2.399741200560206\n",
      "Epoch: 0489 Training RMSE= 2.547591426\n",
      "Validation RMSE:  3.0413971002001623\n",
      "Test RMSE:  2.38456611517325\n",
      "Epoch: 0490 Training RMSE= 2.548071331\n",
      "Validation RMSE:  3.055484560421073\n",
      "Test RMSE:  2.4055520568873114\n",
      "Epoch: 0491 Training RMSE= 2.546454375\n",
      "Validation RMSE:  3.1017648933540722\n",
      "Test RMSE:  2.429864367950177\n",
      "Epoch: 0492 Training RMSE= 2.551158883\n",
      "Validation RMSE:  3.0511947918139826\n",
      "Test RMSE:  2.39822272417012\n",
      "Epoch: 0493 Training RMSE= 2.549156323\n",
      "Validation RMSE:  3.062296957953524\n",
      "Test RMSE:  2.3951913941108356\n",
      "Epoch: 0494 Training RMSE= 2.543294662\n",
      "Validation RMSE:  3.0470528575142235\n",
      "Test RMSE:  2.3892597463851457\n",
      "Epoch: 0495 Training RMSE= 2.553840184\n",
      "Validation RMSE:  3.0539624458582133\n",
      "Test RMSE:  2.406157851002895\n",
      "Epoch: 0496 Training RMSE= 2.541945460\n",
      "Validation RMSE:  3.0868635888515334\n",
      "Test RMSE:  2.436135521474883\n",
      "Epoch: 0497 Training RMSE= 2.551587600\n",
      "Validation RMSE:  3.053159560883525\n",
      "Test RMSE:  2.3987538380713915\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-540d1ccacc35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     test_error, predic_res, Y_true = gcn_corr_final(frequency, horizon, learning_rate, decay, batch_size, n_hidden_vec1,\n\u001b[0;32m---> 61\u001b[0;31m                                                                 n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;31m#val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m#print (\"finished A running: \", i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-0c7c9fcb1628>\u001b[0m in \u001b[0;36mgcn_corr_final\u001b[0;34m(frequency, horizon, learning_rate, decay, batch_size, n_hidden_vec1, n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 _, c, preds, trueval = sess.run([optimizer, cost, pred, Y_true_tr], feed_dict={X: X_training[i*batch_size:(i+1)*batch_size,], \n\u001b[1;32m    177\u001b[0m                                                       \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                                               keep_prob: keep})\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0;31m#print (preds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;31m#print (trueval)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "#freq_max = 12\n",
    "#time_step = 12\n",
    "learning_rate = 0.005\n",
    "decay = 0.9 \n",
    "batch_size = 1000\n",
    "num_hidden = 50 # number of hiddent units in LSTM Cell\n",
    "early_stop_th = 150\n",
    "training_epochs = 1000\n",
    "keep = 1#0.2\n",
    "#time_step_max = 10\n",
    "\n",
    "sn = 272 # station num\n",
    "\n",
    "n_hidden_vec1 = 10\n",
    "n_hidden_vec2 = 10\n",
    "n_hidden_vec3 = 10\n",
    "#n_hidden_vec4 = 10\n",
    "#num = 0\n",
    "#All_pred = np.empty([2000, 207])\n",
    "#All_Y = np.empty([2000, 207])\n",
    "\n",
    "#24*90\n",
    "#step = 0\n",
    "#gap = 100\n",
    "#training = 0.7\n",
    "#validation = 0.1\n",
    "#test = 0.2\n",
    "\n",
    "#gcn_corr_eval(7, 0.01, 0.5, 100, 0.4, 10, 5, 5, 0.2, 50, 500)\n",
    "\n",
    "\n",
    "\n",
    "rep = 1 # repeating times\n",
    "\n",
    "#total_sn = 0\n",
    "#num_iter = 50\n",
    "#init_points = 200\n",
    "\n",
    "\n",
    "# stdbscan\n",
    "#spatial_threshold = 300\n",
    "#temporal_threshold = 300\n",
    "#min_neighbors = 1 # number of neighbor\n",
    "\n",
    "#frequency2 = skip1 + freq_max + training\n",
    "\n",
    "#while step < 2000:\n",
    "\n",
    "#hourly_bike_cluster = hourly_bike\n",
    "best = -10000\n",
    "pre_best = []\n",
    "test_Y_best = []\n",
    "test_error_best = 1000\n",
    "A1_best = []\n",
    "# A2_best = []\n",
    "for i in range(rep):\n",
    "    a = datetime.datetime.now()\n",
    "    test_error, predic_res, Y_true = gcn_corr_final(frequency, horizon, learning_rate, decay, batch_size, n_hidden_vec1,\n",
    "                                                                n_hidden_vec2, n_hidden_vec3, keep, early_stop_th, training_epochs, reg1, reg2)\n",
    "    #val_error, predic_res, test_Y,test_error=gcn_corr_final(a['frequency'], a['learning_rate'], a['decay'], a['batch_size'], a['n_hidden_vec1'], a['n_hidden_vec2'], a['n_hidden_vec3'], a['keep'], a['early_stop_th'], a['training_epochs'], a['reg'])\n",
    "    #print (\"finished A running: \", i)\n",
    "    b = datetime.datetime.now()\n",
    "    print(b-a)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm cell num 10, hidden num [10, 10, 5], batchsize 100, hidden state 50: 4.14, not decreasing\n",
    "#lstm cell num 10, hidden num [10, 10, 5], batchsize 500, hidden state 50: 2.43\n",
    "#lstm cell num 10, hidden num [10, 10, 10], batchsize 500, hidden state 50: 4.13, not dropping\n",
    "#lstm cell num 10, hidden num [10, 10, 10], batchsize 1000, hidden state 50:4.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"lstm_gcnn_prediction_1.95.csv\", predic_res, delimiter = ',')\n",
    "#np.savetxt(\"lstm_gcnn_prediction_1.95_Y.csv\", Y_true, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
